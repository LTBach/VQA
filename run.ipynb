{"cells":[{"cell_type":"markdown","metadata":{"id":"dwhbfOv64SrM"},"source":["# DRIVE\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":25921,"status":"ok","timestamp":1769739083035,"user":{"displayName":"Toàn Bách Lương","userId":"04806593440338345841"},"user_tz":-420},"id":"urXAm-PNdN3f","outputId":"ddb53e6b-928b-41c7-db54-a7aa16c7662f"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":265,"status":"ok","timestamp":1769739083298,"user":{"displayName":"Toàn Bách Lương","userId":"04806593440338345841"},"user_tz":-420},"id":"ppM8Zh6wra9w","outputId":"b22043f2-1b73-4303-95c3-192635bf9f04"},"outputs":[{"output_type":"stream","name":"stdout","text":["/content/drive/MyDrive/VQA\n"]}],"source":["%cd /content/drive/MyDrive/VQA"]},{"cell_type":"markdown","metadata":{"id":"uQSzD0a8CUJh"},"source":["# DATA"]},{"cell_type":"markdown","metadata":{"id":"zXA_eWngv7qu"},"source":["## AOKVQA"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Vpx5rdLdvn2g"},"outputs":[],"source":["!export AOKVQA_DIR=\"/content/drive/MyDrive/VQA/data/aokvqa/\""]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":124,"status":"ok","timestamp":1769603699790,"user":{"displayName":"Toàn Bách Lương","userId":"04806593440338345841"},"user_tz":-420},"id":"AQrz_kIKmX9S","outputId":"7a807fba-1dfb-4b90-aa80-b3e9c99d5b2e"},"outputs":[{"name":"stdout","output_type":"stream","text":["/content/drive/MyDrive/VQA/data/aokvqa/\n"]}],"source":["!echo $AOKVQA_DIR"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"itiphTO0ySRH"},"outputs":[],"source":["!mkdir -p $AOKVQA_DIR"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1619,"status":"ok","timestamp":1769155909355,"user":{"displayName":"Toàn Bách Lương","userId":"04806593440338345841"},"user_tz":-420},"id":"up8YkUZ1xlWq","outputId":"28442855-085c-4e68-ed97-6304323f725d"},"outputs":[{"name":"stdout","output_type":"stream","text":["aokvqa_v1p0_train.json\n","aokvqa_v1p0_val.json\n","aokvqa_v1p0_test.json\n","large_vocab_train.csv\n","specialized_vocab_train.csv\n"]}],"source":["!curl -fsSL https://prior-datasets.s3.us-east-2.amazonaws.com/aokvqa/aokvqa_v1p0.tar.gz | tar xvz -C $AOKVQA_DIR"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":102,"status":"ok","timestamp":1769155909459,"user":{"displayName":"Toàn Bách Lương","userId":"04806593440338345841"},"user_tz":-420},"id":"l3Kzu8BG03kr","outputId":"7f53e89c-3b2f-43b9-c4fe-f6ad43ee0a09"},"outputs":[{"name":"stdout","output_type":"stream","text":["aokvqa_v1p0_test.json\taokvqa_v1p0_val.json   specialized_vocab_train.csv\n","aokvqa_v1p0_train.json\tlarge_vocab_train.csv\n"]}],"source":["!ls /content/drive/MyDrive/VQA/data/aokvqa/"]},{"cell_type":"markdown","metadata":{"id":"xkAzH_h1y0KS"},"source":["## F-VQA"]},{"cell_type":"markdown","metadata":{"id":"CGnA7V104WlU"},"source":["# CODE\n"]},{"cell_type":"markdown","metadata":{"id":"P5lU-IQs5nax"},"source":["## Cmd\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":158,"status":"ok","timestamp":1769609014377,"user":{"displayName":"Toàn Bách Lương","userId":"04806593440338345841"},"user_tz":-420},"id":"8WLj3pOu6pxZ","outputId":"43d88dfa-1574-462a-b6e1-db3e1b8be288"},"outputs":[{"name":"stdout","output_type":"stream","text":["/content/drive/MyDrive/VQA\n"]}],"source":["!pwd"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"PzM0Of1su-hT"},"outputs":[],"source":["!mkdir -p /content/drive/MyDrive/VQA/cfgs"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"cl1U19MHvVz5"},"outputs":[],"source":["!mkdir -p /content/drive/MyDrive/VQA/data"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Z8Ufwvhbr-Li"},"outputs":[],"source":["!mkdir -p /content/drive/MyDrive/VQA/code"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"dpf-wMNuuFev"},"outputs":[],"source":["!mkdir -p /content/drive/MyDrive/VQA/code/utils"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"G9dV9vehuROa"},"outputs":[],"source":["!mkdir -p /content/drive/MyDrive/VQA/code/model"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ZN5-BfZe9Jpb"},"outputs":[],"source":["!mkdir -p /content/drive/MyDrive/VQA/code/model/fusion_net"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7pmqsoY1-r00"},"outputs":[],"source":["!mkdir -p /content/drive/MyDrive/VQA/code/model/answer_net"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"8O6Bmc1Z8JN2"},"outputs":[],"source":["!mkdir -p /content/drive/MyDrive/VQA/code/data"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"H9Ps-7jMfOzr"},"outputs":[],"source":["!mkdir -p /content/drive/MyDrive/VQA/code/bash_script"]},{"cell_type":"code","source":["!mkdir -p /content/drive/MyDrive/VQA/code/torchlight"],"metadata":{"id":"h-DXlterwXiJ","executionInfo":{"status":"ok","timestamp":1769740710160,"user_tz":-420,"elapsed":115,"user":{"displayName":"Toàn Bách Lương","userId":"04806593440338345841"}}},"execution_count":17,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":117,"status":"ok","timestamp":1769155909578,"user":{"displayName":"Toàn Bách Lương","userId":"04806593440338345841"},"user_tz":-420},"id":"8m_OOZTjsEmd","outputId":"a592ca74-8ed6-4f6d-9ecd-4b3170f49e33"},"outputs":[{"name":"stdout","output_type":"stream","text":["cfgs  code  data  kg  run.ipynb\n"]}],"source":["!ls"]},{"cell_type":"markdown","metadata":{"id":"EtoksY5d762f"},"source":["## Data\n"]},{"cell_type":"markdown","metadata":{"id":"_HaU9nXR9OYf"},"source":["###### preprocess.py"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":56,"status":"ok","timestamp":1769156729158,"user":{"displayName":"Toàn Bách Lương","userId":"04806593440338345841"},"user_tz":-420},"id":"-13__Ap29NxS","outputId":"e45d5c06-820b-4242-a759-a38f73ae5b0f"},"outputs":[{"name":"stdout","output_type":"stream","text":["Writing /content/drive/MyDrive/VQA/code/data/preprocess.py\n"]}],"source":["%%writefile /content/drive/MyDrive/VQA/code/data/preprocess.py\n","import os\n","import os.path as osp\n","import re\n","import random\n","import itertools\n","import h5py\n","import torch\n","import torch.utils.data as data\n","import pdb\n","from torch.utils.data.dataloader import default_collate\n","from collections import Counter\n","from PIL import Image\n","# this is used for normalizing questions\n","_special_chars = re.compile('[^a-z0-9 ]*')\n","\n","# these try to emulate the original normalization scheme for answers\n","_period_strip = re.compile(r'(?!<=\\d)(\\.)(?!\\d)')\n","_comma_strip = re.compile(r'(\\d)(,)(\\d)')\n","_punctuation_chars = re.escape(r';/[]\"{}()=+\\_-><@`,?!')\n","_punctuation = re.compile(r'([{}])'.format(re.escape(_punctuation_chars)))\n","_punctuation_with_a_space = re.compile(r'(?<= )([{0}])|([{0}])(?= )'.format(_punctuation_chars))\n","\n","\n","def invert_dict(d): return {v: k for k, v in d.items()}\n","\n","\n","def process_punctuation(s):\n","    # the original is somewhat broken, so things that look odd here might just be to mimic that behaviour\n","    # this version should be faster since we use re instead of repeated operations on str's\n","    original_s = s\n","    if _punctuation.search(s) is None:\n","        return s\n","    s = _punctuation_with_a_space.sub('', s)\n","    if re.search(_comma_strip, s) is not None:\n","        s = s.replace(',', '')\n","    s = _punctuation.sub(' ', s)\n","    s = _period_strip.sub('', s)\n","    if s.strip() == '':\n","        return original_s.strip()\n","    else:\n","        return s.strip()\n","\n","\n","def extract_vocab(iterable, top_k=None, start=0, input_vocab=None):\n","    \"\"\" Turns an iterable of list of tokens into a vocabulary.\n","        These tokens could be single answers or word tokens in questions.\n","    \"\"\"\n","    all_tokens = itertools.chain.from_iterable(iterable)\n","    counter = Counter(all_tokens)\n","    if top_k:\n","        most_common = counter.most_common(top_k)\n","        most_common = (t for t, c in most_common)\n","    else:\n","        most_common = counter.keys()\n","    # descending in count, then lexicographical order\n","    tokens = sorted(most_common, key=lambda x: (counter[x], x), reverse=True)\n","\n","    vocab = {t: i for i, t in enumerate(tokens, start=start)}\n","    return vocab\n","\n","\n","class CocoImages(data.Dataset):\n","    def __init__(self, path, transform=None):\n","        super(CocoImages, self).__init__()\n","        self.path = path\n","        self.id_to_filename = self._find_images()\n","        self.sorted_ids = sorted(self.id_to_filename.keys())  # used for deterministic iteration order\n","        print('found {} images in {}'.format(len(self), self.path))\n","        self.transform = transform\n","\n","    def _find_images(self):\n","        id_to_filename = {}\n","        for filename in os.listdir(self.path):\n","            if not filename.endswith('.jpg'):\n","                continue\n","            id_and_extension = filename.split('_')[-1]\n","            id = int(id_and_extension.split('.')[0])\n","            id_to_filename[id] = filename\n","        return id_to_filename\n","\n","    def __getitem__(self, item):\n","        id = self.sorted_ids[item]\n","        path = os.path.join(self.path, self.id_to_filename[id])\n","        img = Image.open(path).convert('RGB')\n","\n","        if self.transform is not None:\n","            img = self.transform(img)\n","        return id, img\n","\n","    def __len__(self):\n","        return len(self.sorted_ids)\n","\n","\n","class Composite(data.Dataset):\n","    \"\"\" Dataset that is a composite of several Dataset objects. Useful for combining splits of a dataset. \"\"\"\n","\n","    def __init__(self, *datasets):\n","        self.datasets = datasets\n","\n","    def __getitem__(self, item):\n","        current = self.datasets[0]\n","        for d in self.datasets:\n","            if item < len(d):\n","                return d[item]\n","            item -= len(d)\n","        else:\n","            raise IndexError('Index too large for composite dataset')\n","\n","    def __len__(self):\n","        return sum(map(len, self.datasets))\n","\n","    def _get_answer_vectors(self, answer_indices):\n","        return self.datasets[0]._get_answer_vectors(answer_indices)\n","\n","    def _get_answer_sequences(self, answer_indices):\n","        return self.datasets[0]._get_answer_sequences(answer_indices)\n","\n","    @property\n","    def vector(self):\n","        return self.datasets[0].vector\n","\n","    @property\n","    def token_to_index(self):\n","        return self.datasets[0].token_to_index\n","\n","    @property\n","    def answer_to_index(self):\n","        return self.datasets[0].answer_to_index\n","\n","    @property\n","    def index_to_answer(self):\n","        return self.datasets[0].index_to_answer\n","\n","    @property\n","    def num_tokens(self):\n","        return self.datasets[0].num_tokens\n","\n","    @property\n","    def num_answer_tokens(self):\n","        return self.datasets[0].num_answer_tokens\n","\n","    @property\n","    def vocab(self):\n","        return self.datasets[0].vocab\n","\n","\n","def eval_collate_fn(batch):\n","    # put question lengths in descending order so that we can use packed sequences later\n","    batch.sort(key=lambda x: x[-1], reverse=True)\n","    return data.dataloader.default_collate(batch)"]},{"cell_type":"markdown","metadata":{"id":"H5BEEa3k8sks"},"source":["###### base.py"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":91,"status":"ok","timestamp":1769156608927,"user":{"displayName":"Toàn Bách Lương","userId":"04806593440338345841"},"user_tz":-420},"id":"nC8mQUkE8u1V","outputId":"96c8258f-310d-4e33-a557-705a8f655f7a"},"outputs":[{"name":"stdout","output_type":"stream","text":["Writing /content/drive/MyDrive/VQA/code/data/base.py\n"]}],"source":["%%writefile /content/drive/MyDrive/VQA/code/data/base.py\n","import json\n","import os\n","import os.path as osp\n","import nltk\n","import h5py\n","import torch\n","import torch.utils.data as data\n","import pdb\n","from nltk import word_tokenize, pos_tag\n","import re\n","import numpy as np\n","import sys\n","import pickle as pkl\n","\n","################\n","from .preprocess import invert_dict\n","\n","\n","class VisualQA(data.Dataset):\n","    def __init__(self,\n","                 args,\n","                 vector):\n","        super(VisualQA, self).__init__()\n","\n","        # vocab\n","        self.vector = vector\n","        self.args = args\n","        # process question\n","        # self.args.question_vocab_path = osp.join(project_root, 'data', 'question.vocab.json') # a joint question vocab across all dataset\n","        with open(self.args.question_vocab_path, 'r') as fd:\n","            question_vocab = json.load(fd)\n","        self.token_to_index = question_vocab['question']\n","        self._max_question_length = question_vocab['max_question_length']\n","        self.image_features_path = args.FVQA.feature_path\n","        self.index_to_token = invert_dict(self.token_to_index)\n","\n","        answer_vocab_path = self.args.FVQA.answer_vocab_path\n","        fact_vocab_path = self.args.FVQA.fact_vocab_path\n","        relation_vocab_path = self.args.FVQA.relation_vocab_path\n","\n","        if self.args.fact_map:\n","            with open(fact_vocab_path, 'r') as fd:\n","                answer_vocab = json.load(fd)\n","        elif self.args.relation_map:\n","            with open(relation_vocab_path, 'r') as fd:\n","                answer_vocab = json.load(fd)\n","        else:\n","            with open(answer_vocab_path, 'r') as fd:\n","                answer_vocab = json.load(fd)\n","        self.answer_to_index = answer_vocab['answer']\n","        self.index_to_answer = invert_dict(self.answer_to_index)\n","\n","        self.cached_answers_g2v = {}  # 只编码KGE\n","        self.cached_answers_w2v = {}  # 只编码序列\n","        self.cached_answers_gae = {}\n","        self.cached_answers_bert = {}\n","        self.unk_vector = self.vector['UNK']\n","        if \"KG\" in self.args.method_choice:\n","            self._map_kg()\n","        if \"GAE\" in self.args.method_choice:\n","            # self._map_gae()\n","            self._map_bert()\n","\n","    @property\n","    def max_question_length(self):\n","        return self._max_question_length\n","\n","    @property\n","    def max_answer_length(self):\n","        assert hasattr(self, answers), 'Dataloader must have access to answers'\n","        if not hasattr(self, '_max_answer_length'):\n","            self._max_answer_length = max(map(len, self.answers))\n","        return self._max_answer_length\n","\n","    @property\n","    def num_tokens(self):\n","        return len(self.token_to_index)\n","\n","    @property\n","    def num_answers(self):\n","        return len(self.answer_to_index)\n","\n","    def __len__(self):\n","        return len(self.questions)\n","\n","    # Internal data utility---------------------------------------\n","\n","    def _load_image(self, image_id):\n","        \"\"\" Load an image \"\"\"\n","        # pdb.set_trace()\n","        index = self.image_id_to_index[image_id]\n","        spa = torch.zeros([1, 1])  # init\n","\n","        if self.args.fusion_model == 'UD' or self.args.fusion_model == 'BAN':\n","            spatials = self.features_file['spatial_features']\n","            dataset = self.features_file['image_features']  # 直接读取特征文件\n","            spa = spatials[index].astype('float32')\n","            spa = torch.from_numpy(spa)\n","        else:\n","            dataset = self.features_file['features']  # 直接读取特征文件\n","\n","        img = dataset[index].astype('float32')\n","\n","        return torch.from_numpy(img), spa\n","\n","    def _create_image_id_to_index(self):\n","        \"\"\" Create a mapping from a COCO image id into the corresponding index into the h5 file \"\"\"\n","        if not hasattr(self, 'features_file'):\n","            # Loading the h5 file has to be done here and not in __init__ because when the DataLoader\n","            # forks for multiple works, every child would use the same file object and fail\n","            # Having multiple readers using different file objects is fine though, so we just init in here.\n","            self.features_file = h5py.File(self.image_features_path, 'r')\n","\n","        if self.args.fusion_model == 'UD' or self.args.fusion_model == 'BAN':\n","            import _pickle as cPickle\n","            image_id_to_index = cPickle.load(open(self.args.FVQA.img_id2idx, \"rb\"))\n","            # pdb.set_trace()\n","            self.s_dim = self.features_file['spatial_features'].shape[2]\n","            self.v_dim = self.features_file['image_features'].shape[2]  # 直接读取特征文件\n","\n","        else:\n","            with h5py.File(self.image_features_path, 'r') as features_file:\n","                image_ids = features_file['ids'][()]\n","            image_id_to_index = {id: i for i, id in enumerate(image_ids)}\n","        return image_id_to_index\n","\n","    def _encode_question(self, question):\n","        \"\"\" Turn a question into a vector of indices and a question length \"\"\"\n","        vec = torch.zeros(self.max_question_length).long()\n","        for i, token in enumerate(question):\n","            index = self.token_to_index.get(token, 0)\n","            vec[i] = index\n","        return vec, len(question)\n","\n","    def _map_kg(self):\n","        if \"KG\" not in self.args.method_choice:\n","            return\n","        # print(\"using kg embedding\")\n","        kg_path = self.args.FVQA.kg_path\n","        entity_path = self.args.FVQA.entity_path  # 来源中的词对应的向量\n","        relation_path = self.args.FVQA.relation_path  # 同上\n","        relation2id_path = self.args.FVQA.relation2id_path  # 搜寻候选答案的来源\n","        entity2id_path = self.args.FVQA.entity2id_path  # 搜寻候选答案的来源\n","\n","        a = np.load(entity_path)\n","        b = np.load(relation_path)\n","        self.map_kg = np.vstack((a, b))\n","\n","        # 随机得到一个矩阵，以模拟随机的情况\n","        # self.map_ran=torch.zeros(self.map_kg.shape)\n","        # self.map_ran = torch.rand(self.map_kg.shape)\n","        # self.map_ran = torch.randn(self.map_kg.shape)\n","        # self.map_kg = self.map_ran\n","\n","        self.map_kg = torch.Tensor(self.map_kg).view(-1, 300)\n","\n","        self.stoi_kg = {}\n","        with open(os.path.join(entity2id_path), \"r\") as f:\n","            while 1:\n","                line = f.readline()\n","                if not line:\n","                    break\n","                line = re.split('\\t|\\n', line)[:2]\n","                self.stoi_kg[line[0]] = int(line[1])\n","        sz = len(self.stoi_kg)\n","        with open(os.path.join(relation2id_path), \"r\") as f:\n","            while 1:\n","                line = f.readline()\n","                if not line:\n","                    break\n","                line = re.split('\\t|\\n', line)[:2]\n","                self.stoi_kg[line[0]] = int(line[1]) + sz\n","\n","    def _map_gae(self):\n","        if \"GAE\" not in self.args.method_choice:\n","            return\n","        # print(\"using kg embedding\")\n","\n","        _gae_path = self.args.FVQA.gae_path\n","        gae_path = osp.join(_gae_path, str(self.args.FVQA.gae_node_num) + \"_init_\" + self.args.FVQA.gae_init + \".pkl\")\n","        print(\"gae file:\", gae_path)\n","        with open(gae_path, 'rb') as f:\n","            if sys.version_info > (3, 0):\n","                features = pkl.load(f, encoding='latin1')\n","            else:\n","                features = pkl.load(f)\n","        # 下标到gae向量的映射\n","        self.map_gae = torch.FloatTensor(np.array(features)).view(-1, 300)\n","        vertices_f = osp.join(_gae_path, \"g_nodes_\" + str(self.args.FVQA.gae_node_num) + \".json\")\n","        self.stoi_gae = {}\n","        with open(vertices_f) as fp:\n","            vertices_list = json.load(fp)\n","\n","        for i, vertex in enumerate(vertices_list):\n","            self.stoi_gae[vertex] = i\n","        # print(\"test map gae\")\n","        # pdb.set_trace()\n","\n","    def _map_bert(self):\n","        if \"GAE\" not in self.args.method_choice:\n","            return\n","        # print(\"using kg embedding\")\n","\n","        cache_path = osp.join(self.args.FVQA.bert_path, \"map_bert.pt\")\n","        if not osp.exists(cache_path):\n","            _bert_path = self.args.FVQA.bert_path\n","\n","            bert_path = osp.join(_bert_path, \"conceptnet_bert_embeddings.pt\")\n","            print(\"bert file:\", bert_path)\n","            _cache = torch.load(bert_path)  # torch.Size([78334, 1024])\n","\n","            self.map_bert = torch.FloatTensor(self.args.FVQA.max_ans, self.args.bert_dim)\n","            # 下标到gae向量的映射\n","            all = []\n","\n","            with open(osp.join(_bert_path, \"cn_node_names_for_embeddings.txt\"), 'r', encoding='utf-8') as f:\n","                while 1:\n","                    line = f.readline()\n","                    if not line:\n","                        break\n","                    line = re.split('\\n', line)\n","                    all.append(line[0])\n","\n","            self.stoi_bert = {}  # answer to vector文件的 id 下标\n","            for key, value in self.answer_to_index.items():\n","                self.stoi_bert[key] = value\n","                if key in all:\n","                    self.map_bert[value] = _cache[all.index(key), :]\n","                else:\n","                    cnt = 0.0\n","                    tmp = torch.zeros(1, self.args.bert_dim).cuda()\n","                    for i, j in enumerate(all):\n","                        if len(j) >= 4 and len(key) >= 3 and (key in j or j in key):\n","                            # pdb.set_trace()\n","                            tmp += _cache[i, :]  # 取平均\n","                            cnt += 1\n","                        if cnt >= 3:\n","                            break\n","                    if cnt == 0:\n","                        raise TypeError('cnt can not = 0 !!!')\n","                    self.map_bert[value] = tmp / (cnt + 1e-12)\n","\n","            if (self.map_bert != self.map_bert).any():\n","                raise TypeError('cnt can not = 0 !!!')\n","            # pdb.set_trace()\n","            torch.save({'map_bert': self.map_bert, 'stoi_bert': self.stoi_bert}, cache_path)\n","        else:\n","            _cache = torch.load(cache_path)\n","            self.map_bert = _cache['map_bert']  # 词向量列表 + 长度\n","            self.stoi_bert = _cache['stoi_bert']  # 答案下标\n","\n","        # print(\"test map gae\")\n","        # pdb.set_trace()\n","\n","    def _get_answer_vectors(self, ways, answer_indices):\n","        dim = self.vector.dim\n","        if ways == 'GAE':\n","            dim = self.args.bert_dim\n","            return self._encode_answer_vector(self._encode_answer_vector_bert, dim, answer_indices)\n","            # return self._encode_answer_vector(self._encode_answer_vector_gae, dim, answer_indices)\n","        elif ways == 'KG':\n","            return self._encode_answer_vector(self._encode_answer_vector_g2v, dim, answer_indices)\n","        elif ways == 'W2V':\n","            return self._encode_answer_vector(self._encode_answer_vector_w2v, dim, answer_indices)\n","\n","    def _encode_answer_vector(self, encode_model, dim, answer_indices):\n","        if isinstance(answer_indices[0], list):\n","            N, C = len(answer_indices), len(answer_indices[0])\n","            vector = torch.zeros(N, C, dim)\n","            for i, answer_ids in enumerate(answer_indices):\n","                for j, answer_id in enumerate(answer_ids):\n","                    if answer_id != -1:\n","                        vector[i, j, :] = encode_model(self.index_to_answer[answer_id])\n","                    else:\n","                        vector[i, j, :] = self.unk_vector\n","        else:\n","            vector = torch.zeros(len(answer_indices), dim)\n","            for idx, answer_id in enumerate(answer_indices):\n","\n","                if answer_id != -1:\n","                    if type(answer_id).__name__ == 'int':\n","                        vector[idx, :] = encode_model(self.index_to_answer[answer_id])\n","                    else:\n","                        vector[idx, :] = encode_model(self.index_to_answer[answer_id.item()])\n","                else:\n","                    vector[idx, :] = self.unk_vector\n","        return vector, []\n","\n","    def _get_answer_sequences_w2v(self, answer_indices):\n","        seqs, lengths = [], []\n","        max_seq_length = 0\n","        if isinstance(answer_indices[0], list):\n","            N, C = len(answer_indices), len(answer_indices[0])\n","            for i, answer_ids in enumerate(answer_indices):\n","                _seqs = []\n","                for j, answer_id in enumerate(answer_ids):\n","                    if answer_id != -1:\n","                        _seqs.append(self._encode_answer_sequence_w2v(self.index_to_answer[answer_id]))\n","                    else:\n","                        _seqs.append([self.unk_vector])\n","                    if max_seq_length < len(_seqs[-1]):\n","                        max_seq_length = len(_seqs[-1])  # determing max length\n","                seqs.append(_seqs)\n","\n","            vector = torch.zeros(N, C, max_seq_length, self.vector.dim)\n","            for i, _seqs in enumerate(seqs):\n","                for j, seq in enumerate(_seqs):\n","                    if len(seq) != 0:\n","                        vector[i, j, :len(seq), :] = torch.cat(seq, dim=0)\n","                    lengths.append(len(seq))\n","            assert len(lengths) == N * \\\n","                C, 'Wrong lengths - length: {} vs N: {}, C: {} vs seqs: {}'.format(len(lengths), N, C, len(seqs))\n","        else:\n","            for idx, answer_id in enumerate(answer_indices):\n","                if answer_id != -1:\n","                    if type(answer_id).__name__ == 'int':\n","                        seqs.append(self._encode_answer_sequence_w2v(self.index_to_answer[answer_id]))\n","                    else:\n","                        seqs.append(self._encode_answer_sequence_w2v(self.index_to_answer[answer_id.item()]))\n","                else:\n","                    seqs.append([self.unk_vector])\n","\n","                if max_seq_length < len(seqs[-1]):\n","                    max_seq_length = len(seqs[-1])  # determing max length\n","\n","            vector = torch.zeros(len(answer_indices), max_seq_length, self.vector.dim)\n","            for idx, seq in enumerate(seqs):\n","                if len(seq) != 0:\n","                    vector[idx, :len(seq), :] = torch.cat(seq, dim=0)\n","                lengths.append(len(seq))\n","\n","        return vector, lengths\n","\n","    def _encode_answer_vector_bert(self, answer):  # 向量求平均\n","\n","        if isinstance(self.cached_answers_bert.get(answer, -1), int):\n","            answer_vec = torch.zeros(1, self.args.bert_dim)\n","            idk = self.stoi_bert.get(answer, -1)\n","            if idk >= 0:\n","                answer_vec = self.map_bert[idk]\n","            self.cached_answers_bert[answer] = answer_vec\n","        return self.cached_answers_bert[answer]\n","\n","    def _encode_answer_vector_gae(self, answer):  # 向量求平均\n","        if isinstance(self.cached_answers_gae.get(answer, -1), int):\n","            answer_vec = torch.zeros(1, self.vector.dim)\n","            idk = self.stoi_gae.get(answer, -1)\n","            if idk >= 0:\n","                answer_vec = self.map_gae[idk].reshape(1, 300)\n","            self.cached_answers_gae[answer] = answer_vec\n","        return self.cached_answers_gae[answer]\n","\n","    def _encode_answer_vector_g2v(self, answer):  # 向量求平均\n","        if isinstance(self.cached_answers_g2v.get(answer, -1), int):\n","            answer_vec = torch.zeros(1, self.vector.dim)\n","\n","            idk = self.stoi_kg.get(answer, -1)\n","            if idk >= 0:\n","                answer_vec = self.map_kg[idk].reshape(1, 300)\n","            self.cached_answers_g2v[answer] = answer_vec\n","        return self.cached_answers_g2v[answer]\n","\n","    def _encode_answer_vector_w2v(self, answer):  # 向量求平均\n","        if isinstance(self.cached_answers_w2v.get(answer, -1), int):\n","            tokens = nltk.word_tokenize(answer)\n","            answer_vec = torch.zeros(1, self.vector.dim)\n","            cnt = 0\n","            for i, token in enumerate(tokens):\n","                if self.vector.check(token):\n","                    answer_vec += self.vector[token]\n","                    cnt += 1\n","            self.cached_answers_w2v[answer] = answer_vec / (cnt + 1e-12)\n","            # pdb.set_trace()\n","        return self.cached_answers_w2v[answer]\n","\n","    def _encode_answer_sequence_w2v(self, answer):\n","        if isinstance(self.cached_answers_w2v.get(answer, -1), int):\n","            tokens = nltk.word_tokenize(answer)\n","            answer_seq = []\n","            for i, token in enumerate(tokens):\n","                if self.vector.check(token):\n","                    answer_seq.append(self.vector[token].view(1, self.vector.dim))\n","                else:\n","                    answer_seq.append(self.vector['<unk>'].view(1, self.vector.dim))\n","            self.cached_answers_w2v[answer] = answer_seq\n","\n","        return self.cached_answers_w2v[answer]\n","\n","    def _encode_multihot_labels(self, answers):\n","        \"\"\" Turn an answer into a vector \"\"\"\n","        max_answer_index = self.args.TEST.max_answer_index\n","        answer_vec = torch.zeros(max_answer_index)\n","        for answer in answers:\n","            index = self.answer_to_index.get(answer)\n","            if index is not None:\n","                if index < max_answer_index:\n","                    answer_vec[index] += 1\n","        return answer_vec\n","\n","    def evaluate(self, predictions):\n","        raise NotImplementedError"]},{"cell_type":"markdown","metadata":{"id":"D5Fcmalh8ja-"},"source":["###### fvqa.py"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":47,"status":"ok","timestamp":1769156489492,"user":{"displayName":"Toàn Bách Lương","userId":"04806593440338345841"},"user_tz":-420},"id":"1GON1jJ58G4a","outputId":"2eec7672-7e6f-404e-f6f1-9342c1d0c08f"},"outputs":[{"name":"stdout","output_type":"stream","text":["Writing /content/drive/MyDrive/VQA/code/data/fvqa.py\n"]}],"source":["%%writefile /content/drive/MyDrive/VQA/code/data/fvqa.py\n","import json\n","import os\n","import os.path as osp\n","import nltk\n","from collections import Counter\n","import torch\n","import torch.utils.data as data\n","import pdb\n","\n","################\n","from .base import VisualQA\n","from .preprocess import process_punctuation\n","\n","\n","def get_loader(args, vector, train=False, val=False):\n","    \"\"\" Returns a data loader for the desired split \"\"\"\n","    assert train + val == 1, 'need to set exactly one of {train, val, test} to True'  # 必须有一个为真\n","    id = args.FVQA.data_choice\n","    if train:\n","        filepath = \"train\" + id\n","        print(\"use train data:\", id)\n","        filepath = os.path.join(args.FVQA.train_data_path, filepath)\n","    else:\n","        filepath = \"test\" + id\n","        filepath = os.path.join(args.FVQA.test_data_path, filepath)\n","\n","    split = FVQA(  # 定义每一次训练的VQA输入 # ok\n","        args,\n","        path_for(args, train=train, val=val, filepath=filepath),  # train的问题\n","        vector,  # 对应的词向量\n","        file_path=filepath\n","    )\n","    batch_size = args.TRAIN.batch_size\n","    if val:\n","        batch_size = args.TEST.batch_size\n","    loader = torch.utils.data.DataLoader(  # 定义传统的DataLoader\n","        split,\n","        batch_size=batch_size,\n","        shuffle=True,  # only shuffle the data in training\n","        pin_memory=True,\n","        num_workers=args.TRAIN.data_workers,\n","    )\n","\n","    return loader\n","\n","\n","class FVQA(VisualQA):  # ok\n","    \"\"\" FVQA dataset, open-ended \"\"\"\n","\n","    def __init__(self, args, qa_path, vector, file_path=None):\n","        self.args = args\n","        answer_vocab_path = self.args.FVQA.answer_vocab_path\n","        super(FVQA, self).__init__(args, vector)\n","        # load annotation\n","        with open(qa_path, 'r') as fd:\n","            self.qa_json = json.load(fd)\n","\n","        # print('extracting answers...')\n","\n","        # 把问题变成id向量+长度的表示, 答案变成id向量\n","        if args.fact_map:\n","            #  得到对应的名字\n","            name = \"fact\"\n","            self.answers = list(prepare_fact(self.qa_json))  # 候选答案列表的列表 [[answer1,answer2,...][....]] 每个问题对应的答案. 单词表示\n","        elif args.relation_map:\n","            name = \"relation\"\n","            self.answers = list(prepare_relation(self.qa_json))  # 候选答案列表的列表 [[answer1,answer2,...][....]] 每个问题对应的答案. 单词表示\n","        else:\n","            name = \"answer\"\n","            self.answers = list(prepare_answers(self.qa_json))  # 候选答案列表的列表 [[answer1,answer2,...][....]] 每个问题对应的答案. 单词表示\n","\n","        cache_filepath = self._get_cache_path(qa_path, file_path, name)\n","\n","        # self.support_relation = list(prepare_relation(self.qa_json))\n","        self.questions, self.answer_indices = self._qa_id_represent(cache_filepath)\n","        # pdb.set_trace()\n","        # process images 处理图片\n","\n","    def open_hdf5(self):\n","        self.image_features_path = self.args.FVQA.feature_path\n","        self.image_id_to_index = self._create_image_id_to_index()  # 得到图片编号到下标的表示\n","        # self.image_ids = [q['image_id'] for q in questions_json['questions']]\n","        self.image_ids = self._get_img_id()\n","\n","    def __getitem__(self, item):  # ok\n","        if not hasattr(self, 'image_ids'):\n","            self.open_hdf5()\n","        # if item > len(self.answers):\n","        #     pdb.set_trace()\n","\n","        question, question_length = self.questions[item]  # 问题向量列表\n","        # sample answers\n","        # self.answer_indices[item]：[1,2,3] or [-1, -1 ...]\n","        # answer_cands = Counter(self.answer_indices[item])  # 单个答案 返回类型：Counter({1: 1, 2: 1, 3: 1})\n","        # answer_indices = list(answer_cands.keys())  # 答案有哪几个（下标）[[1,2,3]]\n","        # counts = list(answer_cands.values())  # 这几个答案分别出现了多少次[10]\n","\n","        label = self._encode_multihot_labels(self.answers[item])  # 答案的multihot表示 前百分之多少的答案\n","        image_id = self.image_ids[item]\n","        image, spa = self._load_image(image_id)  # 直接获得图片的特征\n","        # unique_answers, answer_vectors = self._generate_batch_answer(answer_indices, counts)\n","        # answer_vectors == label\n","        # assert answer_vectors == label\n","        # return image, spa, question, unique_answers, answer_vectors, label, item, question_length\n","        # pdb.set_trace()\n","        return image, spa, question, label, item, question_length\n","\n","    def _get_cache_path(self, qa_path, file_path, name):\n","        w2v = \"\"\n","        if \"KG\" in self.args.method_choice:\n","            if \"w2v\" in self.args.FVQA.entity_path:\n","                w2v = \"(w2vinit)_\" + self.args.FVQA.entity_num + \"_\" + self.args.FVQA.KGE\n","            else:\n","                w2v = \"_\" + self.args.FVQA.entity_num + \"_\" + self.args.FVQA.KGE\n","        if \"train\" in qa_path:\n","            cache_filepath = osp.join(file_path, \"fvqa_\" + name + \"_and_question_train_\" +\n","                                      self.args.method_choice + w2v + \"_\" + str(self.args.FVQA.max_ans) + \".pt\")\n","        else:\n","            cache_filepath = osp.join(file_path, \"fvqa_\" + name + \"_and_question_test_\" + self.args.method_choice + w2v + \"_\" + str(\n","                self.args.FVQA.max_ans) + \".pt\")\n","        return cache_filepath\n","\n","    def _qa_id_represent(self, cache_filepath):\n","        if not os.path.exists(cache_filepath):\n","            # print('encoding questions...')\n","            questions = list(prepare_questions(self.qa_json))  # 问题词列表的列表\n","            questions = [self._encode_question(q) for q in questions]  # 把问题变成id向量+长度的表示\n","\n","            # 对于候选答案列表中的每一个问题对应的候选答案列表，转换成下标表示[[1,2,3],[2,3,4]......]  1——>一个答案\n","            answer_indices = [[self.answer_to_index.get(_a, -1) for _a in a] for a in self.answers]  # 如果没有匹配就是 -1\n","            torch.save({'questions': questions, 'answer_indices': answer_indices}, cache_filepath)\n","\n","        else:\n","            # 已经有，对应这个训练/测试集 的问题w2v表，[train 和 test是不一样的]\n","            _cache = torch.load(cache_filepath)\n","            questions = _cache['questions']  # 词向量列表 + 长度\n","            answer_indices = _cache['answer_indices']  # 答案下标\n","            # self.answer_vectors = _cache['answer_vectors']  # 答案的向量表示[平均]\n","\n","        return questions, answer_indices\n","\n","    def _get_img_id(self):\n","        image_ids = []\n","        keys = list(self.qa_json.keys())\n","        for a in keys:\n","            filename = self.qa_json[a][\"img_file\"]\n","            id_and_extension = filename.split('_')[-1]\n","            id = int(id_and_extension.split('.')[0])\n","            if not filename.endswith('.jpg'):\n","                id += 1000000  # 把jpg和jpeg的分开\n","                # pdb.set_trace()\n","            image_ids.append(id)\n","        return image_ids\n","\n","    # def _generate_batch_answer(self, indices, counts):  # 获得每一个batch的500个候选答案。\n","    #     unique_answers = list(range(0, self.args.FVQA.max_ans))\n","    #     # unique_answers = list(set( aid for aids in indices for aid in aids ))\n","    #     answer_dict = {k: i for i, k in enumerate(unique_answers)}\n","    #     answer_vector = torch.zeros(len(indices), len(unique_answers))  # 128,500\n","    #\n","    #     for i in range(len(counts)):  # 128\n","    #         for j, c in zip(indices[i], counts[i]):\n","    #             answer_vector[i, answer_dict[j]] = c  # 把出现的次数附上\n","    #\n","    #     return unique_answers, answer_vector\n","\n","\n","def path_for(args, train=False, val=False, filepath=\"\"):\n","    # tra = \"all_qs_dict_release_train_\" + str(args.FVQA.max_ans) + \".json\"\n","    # tes = \"all_qs_dict_release_test_\" + str(args.FVQA.max_ans) + \".json\"\n","    tra = \"all_qs_dict_release_train_500.json\"\n","    tes = \"all_qs_dict_release_test_500.json\"\n","    if train == True:\n","        return os.path.join(args.FVQA.train_data_path, filepath, tra)\n","    else:\n","        return os.path.join(args.FVQA.test_data_path, filepath, tes)\n","\n","\n","def prepare_questions(questions_json):  # ok\n","    \"\"\" Tokenize and normalize questions from a given question json in the usual VQA format. \"\"\"\n","    keys = list(questions_json.keys())\n","    questions = []\n","    for a in keys:\n","        questions.append(questions_json[a]['question'])  # question的list\n","    for question in questions:\n","        question = question.lower()[:-1]\n","        yield nltk.word_tokenize(process_punctuation(question))  # 得到一个词的list，例如['I', 'LOVE', 'YOU']\n","\n","\n","def prepare_answers(answers_json):  # ok\n","    \"\"\" Normalize answers from a given answer json in the usual VQA format. \"\"\"\n","    keys = list(answers_json.keys())\n","    answers = []\n","\n","    for a in keys:\n","        answer = answers_json[a][\"answer\"]\n","        answers.append([answer] * 10)  # 双层list，内层的list对应一个问题的答案序列\n","    for answer_list in answers:\n","        ret = list(map(process_punctuation, answer_list))  # 去除标点等操作\n","        yield ret\n","\n","\n","def prepare_fact(answers_json):  # ok\n","    \"\"\" Normalize answers from a given answer json in the usual VQA format. \"\"\"\n","    keys = list(answers_json.keys())\n","    support_facts = []\n","    for a in keys:\n","        answer = answers_json[a][\"answer\"]\n","        facts = answers_json[a][\"fact\"]\n","        f1 = facts[0]\n","        f2 = facts[2]\n","        if answer != f1 and answer != f2:\n","            pdb.set_trace()\n","        assert (answer == f1 or answer == f2)\n","        if answer == f1:\n","            fact = f2\n","        else:\n","            fact = f1\n","        support_facts.append([fact] * 10)  # 双层list，内层的list对应一个问题的答案序列\n","    for support_facts_list in support_facts:\n","        ret = list(map(process_punctuation, support_facts_list))  # 去除标点等操作\n","        yield ret\n","\n","\n","def prepare_relation(answers_json):  # ok\n","    \"\"\" Normalize answers from a given answer json in the usual VQA format. \"\"\"\n","    keys = list(answers_json.keys())\n","    relations = []\n","    for a in keys:\n","        facts = answers_json[a][\"fact\"]\n","        relation = facts[1]\n","\n","        relations.append([relation] * 10)  # 双层list，内层的list对应一个问题的答案序列\n","    for relation_list in relations:\n","        ret = list(map(process_punctuation, relation_list))  # 去除标点等操作\n","        yield ret"]},{"cell_type":"markdown","metadata":{"id":"Ac4KWctO86tU"},"source":["###### aokvqa.py"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":37,"status":"ok","timestamp":1769156791046,"user":{"displayName":"Toàn Bách Lương","userId":"04806593440338345841"},"user_tz":-420},"id":"zdPudDbk89Wp","outputId":"51232c3c-3d73-4cf5-c093-ff3dc397b20c"},"outputs":[{"name":"stdout","output_type":"stream","text":["Writing /content/drive/MyDrive/VQA/code/data/aokvqa.py\n"]}],"source":["%%writefile /content/drive/MyDrive/VQA/code/data/aokvqa.py\n","import json\n","import os\n","import os.path as osp\n","import nltk\n","from collections import Counter\n","import torch\n","import torch.utils.data as data\n","import pdb\n","\n","################\n","from .base import VisualQA\n","from .preprocess import process_punctuation\n","\n","\n","def get_loader(args, vector, train=False, val=False):\n","    \"\"\" Returns a data loader for the desired split \"\"\"\n","    assert train + val == 1, 'need to set exactly one of {train, val, test} to True'  # 必须有一个为真\n","    id = args.FVQA.data_choice\n","    if train:\n","        filepath = \"train\" + id\n","        print(\"use train data:\", id)\n","        filepath = os.path.join(args.FVQA.train_data_path, filepath)\n","    else:\n","        filepath = \"test\" + id\n","        filepath = os.path.join(args.FVQA.test_data_path, filepath)\n","\n","    split = FVQA(  # 定义每一次训练的VQA输入 # ok\n","        args,\n","        path_for(args, train=train, val=val, filepath=filepath),  # train的问题\n","        vector,  # 对应的词向量\n","        file_path=filepath\n","    )\n","    batch_size = args.TRAIN.batch_size\n","    if val:\n","        batch_size = args.TEST.batch_size\n","    loader = torch.utils.data.DataLoader(  # 定义传统的DataLoader\n","        split,\n","        batch_size=batch_size,\n","        shuffle=True,  # only shuffle the data in training\n","        pin_memory=True,\n","        num_workers=args.TRAIN.data_workers,\n","    )\n","\n","    return loader\n","\n","\n","class FVQA(VisualQA):  # ok\n","    \"\"\" FVQA dataset, open-ended \"\"\"\n","\n","    def __init__(self, args, qa_path, vector, file_path=None):\n","        self.args = args\n","        answer_vocab_path = self.args.FVQA.answer_vocab_path\n","        super(FVQA, self).__init__(args, vector)\n","        # load annotation\n","        with open(qa_path, 'r') as fd:\n","            self.qa_json = json.load(fd)\n","\n","        # print('extracting answers...')\n","\n","        # 把问题变成id向量+长度的表示, 答案变成id向量\n","        if args.fact_map:\n","            #  得到对应的名字\n","            name = \"fact\"\n","            self.answers = list(prepare_fact(self.qa_json))  # 候选答案列表的列表 [[answer1,answer2,...][....]] 每个问题对应的答案. 单词表示\n","        elif args.relation_map:\n","            name = \"relation\"\n","            self.answers = list(prepare_relation(self.qa_json))  # 候选答案列表的列表 [[answer1,answer2,...][....]] 每个问题对应的答案. 单词表示\n","        else:\n","            name = \"answer\"\n","            self.answers = list(prepare_answers(self.qa_json))  # 候选答案列表的列表 [[answer1,answer2,...][....]] 每个问题对应的答案. 单词表示\n","\n","        cache_filepath = self._get_cache_path(qa_path, file_path, name)\n","\n","        # self.support_relation = list(prepare_relation(self.qa_json))\n","        self.questions, self.answer_indices = self._qa_id_represent(cache_filepath)\n","        # pdb.set_trace()\n","        # process images 处理图片\n","\n","    def open_hdf5(self):\n","        self.image_features_path = self.args.FVQA.feature_path\n","        self.image_id_to_index = self._create_image_id_to_index()  # 得到图片编号到下标的表示\n","        # self.image_ids = [q['image_id'] for q in questions_json['questions']]\n","        self.image_ids = self._get_img_id()\n","\n","    def __getitem__(self, item):  # ok\n","        if not hasattr(self, 'image_ids'):\n","            self.open_hdf5()\n","        # if item > len(self.answers):\n","        #     pdb.set_trace()\n","\n","        question, question_length = self.questions[item]  # 问题向量列表\n","        # sample answers\n","        # self.answer_indices[item]：[1,2,3] or [-1, -1 ...]\n","        # answer_cands = Counter(self.answer_indices[item])  # 单个答案 返回类型：Counter({1: 1, 2: 1, 3: 1})\n","        # answer_indices = list(answer_cands.keys())  # 答案有哪几个（下标）[[1,2,3]]\n","        # counts = list(answer_cands.values())  # 这几个答案分别出现了多少次[10]\n","\n","        label = self._encode_multihot_labels(self.answers[item])  # 答案的multihot表示 前百分之多少的答案\n","        image_id = self.image_ids[item]\n","        image, spa = self._load_image(image_id)  # 直接获得图片的特征\n","        # unique_answers, answer_vectors = self._generate_batch_answer(answer_indices, counts)\n","        # answer_vectors == label\n","        # assert answer_vectors == label\n","        # return image, spa, question, unique_answers, answer_vectors, label, item, question_length\n","        # pdb.set_trace()\n","        return image, spa, question, label, item, question_length\n","\n","    def _get_cache_path(self, qa_path, file_path, name):\n","        w2v = \"\"\n","        if \"KG\" in self.args.method_choice:\n","            if \"w2v\" in self.args.FVQA.entity_path:\n","                w2v = \"(w2vinit)_\" + self.args.FVQA.entity_num + \"_\" + self.args.FVQA.KGE\n","            else:\n","                w2v = \"_\" + self.args.FVQA.entity_num + \"_\" + self.args.FVQA.KGE\n","        if \"train\" in qa_path:\n","            cache_filepath = osp.join(file_path, \"fvqa_\" + name + \"_and_question_train_\" +\n","                                      self.args.method_choice + w2v + \"_\" + str(self.args.FVQA.max_ans) + \".pt\")\n","        else:\n","            cache_filepath = osp.join(file_path, \"fvqa_\" + name + \"_and_question_test_\" + self.args.method_choice + w2v + \"_\" + str(\n","                self.args.FVQA.max_ans) + \".pt\")\n","        return cache_filepath\n","\n","    def _qa_id_represent(self, cache_filepath):\n","        if not os.path.exists(cache_filepath):\n","            # print('encoding questions...')\n","            questions = list(prepare_questions(self.qa_json))  # 问题词列表的列表\n","            questions = [self._encode_question(q) for q in questions]  # 把问题变成id向量+长度的表示\n","\n","            # 对于候选答案列表中的每一个问题对应的候选答案列表，转换成下标表示[[1,2,3],[2,3,4]......]  1——>一个答案\n","            answer_indices = [[self.answer_to_index.get(_a, -1) for _a in a] for a in self.answers]  # 如果没有匹配就是 -1\n","            torch.save({'questions': questions, 'answer_indices': answer_indices}, cache_filepath)\n","\n","        else:\n","            # 已经有，对应这个训练/测试集 的问题w2v表，[train 和 test是不一样的]\n","            _cache = torch.load(cache_filepath)\n","            questions = _cache['questions']  # 词向量列表 + 长度\n","            answer_indices = _cache['answer_indices']  # 答案下标\n","            # self.answer_vectors = _cache['answer_vectors']  # 答案的向量表示[平均]\n","\n","        return questions, answer_indices\n","\n","    def _get_img_id(self):\n","        image_ids = []\n","        keys = list(self.qa_json.keys())\n","        for a in keys:\n","            filename = self.qa_json[a][\"img_file\"]\n","            id_and_extension = filename.split('_')[-1]\n","            id = int(id_and_extension.split('.')[0])\n","            if not filename.endswith('.jpg'):\n","                id += 1000000  # 把jpg和jpeg的分开\n","                # pdb.set_trace()\n","            image_ids.append(id)\n","        return image_ids\n","\n","    # def _generate_batch_answer(self, indices, counts):  # 获得每一个batch的500个候选答案。\n","    #     unique_answers = list(range(0, self.args.FVQA.max_ans))\n","    #     # unique_answers = list(set( aid for aids in indices for aid in aids ))\n","    #     answer_dict = {k: i for i, k in enumerate(unique_answers)}\n","    #     answer_vector = torch.zeros(len(indices), len(unique_answers))  # 128,500\n","    #\n","    #     for i in range(len(counts)):  # 128\n","    #         for j, c in zip(indices[i], counts[i]):\n","    #             answer_vector[i, answer_dict[j]] = c  # 把出现的次数附上\n","    #\n","    #     return unique_answers, answer_vector\n","\n","\n","def path_for(args, train=False, val=False, filepath=\"\"):\n","    # tra = \"all_qs_dict_release_train_\" + str(args.FVQA.max_ans) + \".json\"\n","    # tes = \"all_qs_dict_release_test_\" + str(args.FVQA.max_ans) + \".json\"\n","    tra = \"all_qs_dict_release_train_500.json\"\n","    tes = \"all_qs_dict_release_test_500.json\"\n","    if train == True:\n","        return os.path.join(args.FVQA.train_data_path, filepath, tra)\n","    else:\n","        return os.path.join(args.FVQA.test_data_path, filepath, tes)\n","\n","\n","def prepare_questions(questions_json):  # ok\n","    \"\"\" Tokenize and normalize questions from a given question json in the usual VQA format. \"\"\"\n","    keys = list(questions_json.keys())\n","    questions = []\n","    for a in keys:\n","        questions.append(questions_json[a]['question'])  # question的list\n","    for question in questions:\n","        question = question.lower()[:-1]\n","        yield nltk.word_tokenize(process_punctuation(question))  # 得到一个词的list，例如['I', 'LOVE', 'YOU']\n","\n","\n","def prepare_answers(answers_json):  # ok\n","    \"\"\" Normalize answers from a given answer json in the usual VQA format. \"\"\"\n","    keys = list(answers_json.keys())\n","    answers = []\n","\n","    for a in keys:\n","        answer = answers_json[a][\"answer\"]\n","        answers.append([answer] * 10)  # 双层list，内层的list对应一个问题的答案序列\n","    for answer_list in answers:\n","        ret = list(map(process_punctuation, answer_list))  # 去除标点等操作\n","        yield ret\n","\n","\n","def prepare_fact(answers_json):  # ok\n","    \"\"\" Normalize answers from a given answer json in the usual VQA format. \"\"\"\n","    keys = list(answers_json.keys())\n","    support_facts = []\n","    for a in keys:\n","        answer = answers_json[a][\"answer\"]\n","        facts = answers_json[a][\"fact\"]\n","        f1 = facts[0]\n","        f2 = facts[2]\n","        if answer != f1 and answer != f2:\n","            pdb.set_trace()\n","        assert (answer == f1 or answer == f2)\n","        if answer == f1:\n","            fact = f2\n","        else:\n","            fact = f1\n","        support_facts.append([fact] * 10)  # 双层list，内层的list对应一个问题的答案序列\n","    for support_facts_list in support_facts:\n","        ret = list(map(process_punctuation, support_facts_list))  # 去除标点等操作\n","        yield ret\n","\n","\n","def prepare_relation(answers_json):  # ok\n","    \"\"\" Normalize answers from a given answer json in the usual VQA format. \"\"\"\n","    keys = list(answers_json.keys())\n","    relations = []\n","    for a in keys:\n","        facts = answers_json[a][\"fact\"]\n","        relation = facts[1]\n","\n","        relations.append([relation] * 10)  # 双层list，内层的list对应一个问题的答案序列\n","    for relation_list in relations:\n","        ret = list(map(process_punctuation, relation_list))  # 去除标点等操作\n","        yield ret"]},{"cell_type":"markdown","metadata":{"id":"wXzgiSGB5UK1"},"source":["## Model\n"]},{"cell_type":"markdown","metadata":{"id":"Y1cuCeNi_h_S"},"source":["###### \\_\\_init__.py"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":8,"status":"ok","timestamp":1769155911901,"user":{"displayName":"Toàn Bách Lương","userId":"04806593440338345841"},"user_tz":-420},"id":"O1ABSmhv_3Ny","outputId":"84cf967b-88b3-449f-90ed-55dee2b40fa4"},"outputs":[{"name":"stdout","output_type":"stream","text":["Writing /content/drive/MyDrive/VQA/code/model/__init__.py\n"]}],"source":["%%writefile /content/drive/MyDrive/VQA/code/model/__init__.py\n","from .attention import BiAttention\n","from .classifier import SimpleClassifier\n","from .counting import Counter\n","from .fc import FCNet, BCNet"]},{"cell_type":"markdown","metadata":{"id":"XmZjuTqqKqoa"},"source":["###### fc.py"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":27,"status":"ok","timestamp":1769155935213,"user":{"displayName":"Toàn Bách Lương","userId":"04806593440338345841"},"user_tz":-420},"id":"61KutEkPKu0T","outputId":"af100270-5770-41de-b3f3-bc1ba973e5fc"},"outputs":[{"name":"stdout","output_type":"stream","text":["Writing /content/drive/MyDrive/VQA/code/model/fc.py\n"]}],"source":["%%writefile /content/drive/MyDrive/VQA/code/model/fc.py\n","from __future__ import print_function\n","import torch.nn as nn\n","from torch.nn.utils.weight_norm import weight_norm\n","import torch\n","\n","\n","class FCNet(nn.Module):\n","    \"\"\"Simple class for non-linear fully connect network\n","    \"\"\"\n","\n","    def __init__(self, dims, act='ReLU', dropout=0):\n","        super(FCNet, self).__init__()\n","\n","        layers = []\n","        for i in range(len(dims) - 2):\n","            in_dim = dims[i]\n","            out_dim = dims[i + 1]\n","            if 0 < dropout:\n","                layers.append(nn.Dropout(dropout))\n","            layers.append(weight_norm(nn.Linear(in_dim, out_dim), dim=None))\n","            if '' != act:\n","                layers.append(getattr(nn, act)())\n","        if 0 < dropout:\n","            layers.append(nn.Dropout(dropout))\n","        layers.append(weight_norm(nn.Linear(dims[-2], dims[-1]), dim=None))\n","        if '' != act:\n","            layers.append(getattr(nn, act)())\n","\n","        self.main = nn.Sequential(*layers)\n","\n","    def forward(self, x):\n","        return self.main(x)\n","\n","\n","class BCNet(nn.Module):\n","    \"\"\"Simple class for non-linear bilinear connect network\n","    \"\"\"\n","\n","    def __init__(self, v_dim, q_dim, h_dim, h_out, act='ReLU', dropout=[.2, .5], k=3):\n","        super(BCNet, self).__init__()\n","\n","        self.c = 32\n","        self.k = k\n","        self.v_dim = v_dim\n","        self.q_dim = q_dim\n","        self.h_dim = h_dim\n","        self.h_out = h_out\n","\n","        self.v_net = FCNet([v_dim, h_dim * self.k], act=act, dropout=dropout[0])\n","        self.q_net = FCNet([q_dim, h_dim * self.k], act=act, dropout=dropout[0])\n","        self.dropout = nn.Dropout(dropout[1])  # attention\n","        if 1 < k:\n","            self.p_net = nn.AvgPool1d(self.k, stride=self.k)\n","\n","        if None == h_out:\n","            pass\n","        elif h_out <= self.c:\n","            self.h_mat = nn.Parameter(torch.Tensor(1, h_out, 1, h_dim * self.k).normal_())\n","            self.h_bias = nn.Parameter(torch.Tensor(1, h_out, 1, 1).normal_())\n","        else:\n","            self.h_net = weight_norm(nn.Linear(h_dim, h_out), dim=None)\n","\n","    def forward(self, v, q):\n","        if None == self.h_out:\n","            v_ = self.v_net(v).transpose(1, 2).unsqueeze(3)\n","            q_ = self.q_net(q).transpose(1, 2).unsqueeze(2)\n","            d_ = torch.matmul(v_, q_)  # b x h_dim x v x q\n","            logits = d_.transpose(1, 2).transpose(2, 3)  # b x v x q x h_dim\n","            return logits\n","\n","        # broadcast Hadamard product, matrix-matrix production\n","        # fast computation but memory inefficient\n","        # epoch 1, time: 157.84\n","        elif self.h_out <= self.c:\n","            v_ = self.dropout(self.v_net(v)).unsqueeze(1)\n","            q_ = self.q_net(q)\n","            h_ = v_ * self.h_mat  # broadcast, b x h_out x v x h_dim\n","            logits = torch.matmul(h_, q_.unsqueeze(1).transpose(2, 3))  # b x h_out x v x q\n","            logits = logits + self.h_bias\n","            return logits  # b x h_out x v x q\n","\n","        # batch outer product, linear projection\n","        # memory efficient but slow computation\n","        # epoch 1, time: 304.87\n","        else:\n","            v_ = self.dropout(self.v_net(v)).transpose(1, 2).unsqueeze(3)\n","            q_ = self.q_net(q).transpose(1, 2).unsqueeze(2)\n","            d_ = torch.matmul(v_, q_)  # b x h_dim x v x q\n","            logits = self.h_net(d_.transpose(1, 2).transpose(2, 3))  # b x v x q x h_out\n","            return logits.transpose(2, 3).transpose(1, 2)  # b x h_out x v x q\n","\n","    def forward_with_weights(self, v, q, w):\n","        v_ = self.v_net(v).transpose(1, 2).unsqueeze(2)  # b x d x 1 x v\n","        q_ = self.q_net(q).transpose(1, 2).unsqueeze(3)  # b x d x q x 1\n","        logits = torch.matmul(torch.matmul(v_, w.unsqueeze(1)), q_)  # b x d x 1 x 1\n","        logits = logits.squeeze(3).squeeze(2)\n","        if 1 < self.k:\n","            logits = logits.unsqueeze(1)  # b x 1 x d\n","            logits = self.p_net(logits).squeeze(1) * self.k  # sum-pooling\n","        return logits\n","\n","\n","class GroupMLP(nn.Module):\n","    def __init__(self, in_features, mid_features, out_features, drop=0.5, groups=1):\n","        super(GroupMLP, self).__init__()\n","\n","        self.conv1 = nn.Conv1d(in_features, mid_features, 1)\n","        self.drop = nn.Dropout(p=drop)\n","        self.relu = nn.LeakyReLU()\n","        self.conv2 = nn.Conv1d(mid_features, out_features, 1, groups=groups)\n","\n","    def forward(self, a):\n","        N, C = a.size()\n","        h = self.relu(self.conv1(a.view(N, C, 1)))\n","        return self.conv2(self.drop(h)).view(N, -1)\n","\n","\n","class GroupMLP_1lay(nn.Module):\n","    def __init__(self, in_features, mid_features, out_features, drop=0.5, groups=1):\n","        super(GroupMLP_1lay, self).__init__()\n","\n","        self.conv1 = nn.Conv1d(in_features, mid_features, 1)\n","        self.batch_norm_fusion = nn.BatchNorm1d(mid_features, affine=False)\n","        self.drop = nn.Dropout(p=drop)\n","        self.relu = nn.LeakyReLU()\n","        self.conv2 = nn.Conv1d(mid_features, out_features, 1, groups=groups)\n","\n","    def forward(self, a):\n","        N, C = a.size()\n","        h = self.conv1(a.view(N, C, 1))\n","        h = self.batch_norm_fusion(h)\n","        h = self.relu(h)\n","        return self.conv2(self.drop(h)).view(N, -1)\n","\n","\n","class GroupMLP_2lay(nn.Module):\n","    def __init__(self, in_features, mid_features, out_features, drop=0.5, groups=1):\n","        super(GroupMLP_2lay, self).__init__()\n","\n","        self.conv1 = nn.Conv1d(in_features, mid_features, 1)\n","        self.batch_norm_fusion = nn.BatchNorm1d(mid_features, affine=False)\n","        self.drop = nn.Dropout(p=drop)\n","        self.relu = nn.LeakyReLU()\n","        self.conv2 = nn.Conv1d(mid_features, mid_features, 1, groups=groups)\n","        self.conv3 = nn.Conv1d(mid_features, out_features, 1, groups=groups)\n","\n","    def forward(self, a):\n","        N, C = a.size()\n","        h = self.conv1(a.view(N, C, 1))\n","        h = self.relu(h)\n","        h = self.conv2(h)\n","        h = self.batch_norm_fusion(h)\n","        h = self.relu(h)\n","        return self.conv3(self.drop(h)).view(N, -1)"]},{"cell_type":"markdown","metadata":{"id":"CJ1vexQVEKHd"},"source":["###### vector.py"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":448,"status":"ok","timestamp":1765514744879,"user":{"displayName":"Toàn Bách Lương","userId":"04806593440338345841"},"user_tz":-420},"id":"ukpOiIGBezTz","outputId":"4358c6c5-9e76-4283-8caa-35afe75f4f86"},"outputs":[{"name":"stdout","output_type":"stream","text":["Overwriting /content/drive/MyDrive/VQA/code/model/vector.py\n"]}],"source":["%%writefile /content/drive/MyDrive/VQA/code/model/vector.py\n","import array\n","import zipfile\n","from tqdm import tqdm\n","from six.moves.urllib.request import urlretrieve\n","import os\n","import os.path as osp\n","import torch\n","import io\n","\n","class Vector(object):\n","    def __init__(self, cache_path,\n","                 vector_type='glove.840B', unk_init=torch.Tensor.zero_) -> object:\n","        urls = {\n","            'glove.42B': 'http://nlp.stanford.edu/data/glove.42B.300d.zip',\n","            'glove.840B': 'http://nlp.stanford.edu/data/glove.840B.300d.zip',\n","            'glove.6B': 'http://nlp.stanford.edu/data/glove.6B.zip',\n","        }\n","        url = urls[vector_type] if urls.get(vector_type, False) != False else None\n","        name = osp.splitext(osp.basename(url))[0] + '.txt'  # glove.840B.300d.txt\n","\n","        self.unk_init = unk_init\n","        self.cache(name, cache_path, url=url)\n","\n","    def __getitem__(self, token):\n","        if self.stoi.get(token, -1) != -1:\n","            return self.vectors[self.stoi[token]]\n","        else:\n","            return self.unk_init(torch.Tensor(1, self.dim))\n","\n","    def _prepare(self, vocab):\n","        word2vec = torch.Tensor(len(vocab), self.dim)\n","        for token, idx in vocab.items():\n","            word2vec[idx, :] = self[token]\n","\n","        return word2vec\n","\n","    def check(self, token):\n","        if self.stoi.get(token, -1) != -1:\n","            return True\n","        else:\n","            return False\n","\n","    def cache(self, name, cache_path, url=None):\n","        # cache_path='.vector_cache',\n","        #name= \"glove.840B.300d.txt\"\n","        #url = 'http://nlp.stanford.edu/data/glove.840B.300d.zip'\n","\n","        path = osp.join(cache_path, name)\n","        path_pt = \"{}.pt\".format(path)\n","\n","        if not osp.isfile(path_pt):\n","            # download vocab file if it does not exists\n","            if not osp.exists(path) and url:\n","                dest = osp.join(cache_path, os.path.basename(url))\n","                if not osp.exists(dest):\n","                    print('[-] Downloading vectors from {}'.format(url))\n","                    if not osp.exists(cache_path):\n","                        os.mkdir(cache_path)\n","\n","                    with tqdm(unit='B', unit_scale=True, miniters=1, desc=dest) as t:\n","                        urlretrieve(url, dest, reporthook=reporthook(t))\n","\n","                print('[-] Extracting vectors into {}'.format(path))\n","                ext = os.path.splitext(dest)[1][1:]\n","                if ext == 'zip':\n","                    with zipfile.ZipFile(dest, \"r\") as zf:\n","                        zf.extractall(cache_path)\n","\n","            if not os.path.isfile(path):\n","                raise RuntimeError('no vectors found at {}'.format(path))\n","\n","            # build vocab list\n","            itos, vectors, dim = [], array.array(str('d')), None\n","\n","            # Try to read the whole file with utf-8 encoding.\n","            binary_lines = False\n","            try:\n","                with io.open(path, encoding=\"utf8\") as f:\n","                    lines = [line for line in f]\n","            # If there are malformed lines, read in binary mode\n","            # and manually decode each word from utf-8\n","            except:\n","                print(\"[!] Could not read {} as UTF8 file, \"\n","                      \"reading file as bytes and skipping \"\n","                      \"words with malformed UTF8.\".format(path))\n","                with open(path, 'rb') as f:\n","                    lines = [line for line in f]\n","                binary_lines = True\n","\n","            print(\"[-] Loading vectors from {}\".format(path))  # 读取vector\n","            for line in tqdm(lines, total=len(lines)):\n","                # Explicitly splitting on \" \" is important, so we don't\n","                # get rid of Unicode non-breaking spaces in the vectors.\n","                entries = line.rstrip().split(\" \")\n","                word, entries = entries[0], entries[1:]\n","                if dim is None and len(entries) > 1:\n","                    dim = len(entries)\n","                elif len(entries) == 1:\n","                    print(\"Skipping token {} with 1-dimensional \"\n","                          \"vector {}; likely a header\".format(word, entries))\n","                    continue\n","                elif dim != len(entries):\n","                    raise RuntimeError(\n","                        \"Vector for token {} has {} dimensions, but previously \"\n","                        \"read vectors have {} dimensions. All vectors must have \"\n","                        \"the same number of dimensions.\".format(word, len(entries), dim))\n","\n","                vectors.extend(float(x) for x in entries)\n","                itos.append(word)\n","\n","            self.itos = itos\n","            self.stoi = {word: i for i, word in enumerate(itos)}\n","            self.vectors = torch.Tensor(vectors).view(-1, dim)\n","            self.dim = dim\n","            print('* Caching vectors to {}'.format(path_pt))\n","            torch.save((self.itos, self.stoi, self.vectors, self.dim), path_pt)\n","        else:\n","            print('* Loading vectors to {}'.format(path_pt))\n","            self.itos, self.stoi, self.vectors, self.dim = torch.load(path_pt)"]},{"cell_type":"markdown","metadata":{"id":"qPnG22YpEGFQ"},"source":["###### attetion.py"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":753,"status":"ok","timestamp":1769155922664,"user":{"displayName":"Toàn Bách Lương","userId":"04806593440338345841"},"user_tz":-420},"id":"zQojqDH0eWMb","outputId":"6e4eadc8-29e6-4d7d-c462-075e08f2a6f0"},"outputs":[{"name":"stdout","output_type":"stream","text":["Overwriting /content/drive/MyDrive/VQA/code/model/attention.py\n"]}],"source":["%%writefile /content/drive/MyDrive/VQA/code/model/attention.py\n","import torch\n","import torch.nn as nn\n","from torch.nn.utils.weight_norm import weight_norm\n","from .fc import FCNet, BCNet\n","import torch.nn.functional as F\n","\n","class BaseAttention(nn.Module):\n","    def __init__(self, v_dim, q_dim, num_hid):\n","        super(BaseAttention, self).__init__()\n","        self.nonlinear = FCNet([v_dim + q_dim, num_hid])\n","        self.linear = weight_norm(nn.Linear(num_hid, 1), dim=None)\n","\n","    def forward(self, v, q):\n","        \"\"\"\n","        v: [batch, k, vdim]\n","        q: [batch, qdim]\n","        \"\"\"\n","        logits = self.logits(v, q)\n","        w = nn.functional.softmax(logits, 1)\n","        return w\n","\n","    def logits(self, v, q):\n","        num_objs = v.size(1)\n","        q = q.unsqueeze(1).repeat(1, num_objs, 1)\n","        vq = torch.cat((v, q), 2)\n","        joint_repr = self.nonlinear(vq)\n","        logits = self.linear(joint_repr)\n","        return logits\n","\n","\n","class UpDnAttention(nn.Module):\n","    def __init__(self, v_dim, q_dim, num_hid, dropout=0.2):\n","        super(UpDnAttention, self).__init__()\n","\n","        self.v_proj = FCNet([v_dim, num_hid])\n","        self.q_proj = FCNet([q_dim, num_hid])\n","        self.dropout = nn.Dropout(dropout)\n","        self.linear = weight_norm(nn.Linear(num_hid, 1), dim=None)\n","\n","    def forward(self, v, q):\n","        \"\"\"\n","        v: [batch, k, vdim]\n","        q: [batch, qdim]\n","        \"\"\"\n","        logits = self.logits(v, q)\n","        w = nn.functional.softmax(logits, 1)\n","        return w\n","\n","    def logits(self, v, q):\n","        batch, k, _ = v.size()\n","        v_proj = self.v_proj(v)  # [batch, k, qdim]\n","        q_proj = self.q_proj(q).unsqueeze(1).repeat(1, k, 1)\n","        joint_repr = v_proj * q_proj\n","        joint_repr = self.dropout(joint_repr)\n","        logits = self.linear(joint_repr)\n","        return logits\n","\n","class SanAttention(nn.Module):\n","  def __init__(self, v_features, q_features, mid_features, glimpses, drop=0.0):\n","    super(SanAttention, self).__init__()\n","    self.v_conv = nn.Conv2d(v_features, mid_features, 1, bias=False)  # let self.lin take care of bias\n","    self.q_lin = nn.Linear(q_features, mid_features)\n","    self.x_conv = nn.Conv2d(mid_features, glimpses, 1)\n","\n","    self.drop = nn.Dropout(drop)\n","    self.relu = nn.LeakyReLU(inplace=True)\n","\n","  def forward(self, v, q):\n","    v = self.v_conv(self.drop(v))\n","    q = self.q_lin(self.drop(q))\n","    q = tile_2d_over_nd(q, v)\n","    x = self.relu(v + q)\n","    x = self.x_conv(self.drop(x))\n","    return x\n","\n","def tile_2d_over_nd(feature_vector, feature_map):\n","  \"\"\" Repeat the same feature vector over all spatial positions of a given feature map.\n","    The feature vector should have the same batch size and number of features as the feature map.\n","  \"\"\"\n","  n, c = feature_vector.size()\n","  spatial_size = feature_map.dim() - 2\n","  tiled = feature_vector.view(n, c, *([1] * spatial_size)).expand_as(feature_map)\n","  return tiled\n","\n","def apply_attention(input, attention):\n","  \"\"\" Apply any number of attention maps over the input.\n","    The attention map has to have the same size in all dimensions except dim=1.\n","  \"\"\"\n","  # import pdb\n","  # pdb.set_trace()\n","  n, c = input.size()[:2]\n","  glimpses = attention.size(1)\n","\n","  # flatten the spatial dims into the third dim, since we don't need to care about how they are arranged\n","  input = input.view(n, c, -1)\n","  attention = attention.view(n, glimpses, -1)\n","  s = input.size(2)\n","\n","  # apply a softmax to each attention map separately\n","  # since softmax only takes 2d inputs, we have to collapse the first two dimensions together\n","  # so that each glimpse is normalized separately\n","  attention = attention.view(n * glimpses, -1)\n","  attention = F.softmax(attention)\n","\n","  # apply the weighting by creating a new dim to tile both tensors over\n","  target_size = [n, glimpses, c, s]\n","  input = input.view(n, 1, c, s).expand(*target_size)\n","  attention = attention.view(n, glimpses, 1, s).expand(*target_size)\n","  weighted = input * attention\n","  # sum over only the spatial dimension\n","  weighted_mean = weighted.sum(dim=3)\n","  # the shape at this point is (n, glimpses, c, 1)\n","  return weighted_mean.view(n, -1)\n","\n","\n","class BiAttention(nn.Module):\n","    def __init__(self, x_dim, y_dim, z_dim, glimpse, dropout=[.2, .5]):\n","        super(BiAttention, self).__init__()\n","\n","        self.glimpse = glimpse\n","        self.logits = weight_norm(BCNet(x_dim, y_dim, z_dim, glimpse, dropout=dropout, k=3),\n","                                  name='h_mat', dim=None)\n","\n","    def forward(self, v, q, v_mask=True):\n","        \"\"\"\n","        v: [batch, k, vdim]\n","        q: [batch, qdim]\n","        \"\"\"\n","        p, logits = self.forward_all(v, q, v_mask)\n","        return p, logits\n","\n","    def forward_all(self, v, q, v_mask=True):\n","        v_num = v.size(1)\n","        q_num = q.size(1)\n","        logits = self.logits(v, q)  # b x g x v x q\n","\n","        if v_mask:\n","            mask = (0 == v.abs().sum(2)).unsqueeze(1).unsqueeze(3).expand(logits.size())\n","            logits.data.masked_fill_(mask.data, -float('inf'))\n","\n","        p = nn.functional.softmax(logits.view(-1, self.glimpse, v_num * q_num), 2)\n","        return p.view(-1, self.glimpse, v_num, q_num), logits"]},{"cell_type":"markdown","metadata":{"id":"pqTx0OfLEN9U"},"source":["###### language_model.py"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":473,"status":"ok","timestamp":1769155924794,"user":{"displayName":"Toàn Bách Lương","userId":"04806593440338345841"},"user_tz":-420},"id":"44ugMuVeeQN_","outputId":"16676b0d-d033-4321-8e1d-380c8411a9d1"},"outputs":[{"name":"stdout","output_type":"stream","text":["Overwriting /content/drive/MyDrive/VQA/code/model/language_model.py\n"]}],"source":["%%writefile /content/drive/MyDrive/VQA/code/model/language_model.py\n","import torch\n","import torch.nn as nn\n","from torch.autograd import Variable\n","import numpy as np\n","from torch.nn.utils.rnn import pack_padded_sequence\n","import torch.nn.init as init\n","import pdb\n","\n","\n","class WordEmbedding(nn.Module):\n","    \"\"\"Word Embedding\n","\n","    The ntoken-th dim is used for padding_idx, which agrees *implicitly*\n","    with the definition in Dictionary.\n","    \"\"\"\n","\n","    def __init__(self, ntoken, emb_dim, dropout=0):\n","        super(WordEmbedding, self).__init__()\n","        self.emb = nn.Embedding(ntoken + 1, emb_dim, padding_idx=ntoken)\n","        self.dropout = nn.Dropout(dropout)\n","        self.ntoken = ntoken\n","        self.emb_dim = emb_dim\n","\n","    def init_embedding(self, np_file):\n","        # weight_init = torch.from_numpy(np.load(np_file))\n","        weight_init = np_file\n","        assert weight_init.shape == (self.ntoken, self.emb_dim)\n","        self.emb.weight.data[:self.ntoken] = weight_init\n","\n","    def forward(self, x):\n","        emb = self.emb(x)\n","        emb = self.dropout(emb)\n","        return emb\n","\n","\n","class UpDnQuestionEmbedding(nn.Module):\n","    def __init__(self, in_dim, num_hid, nlayers, bidirect, dropout=0, rnn_type='GRU'):\n","        \"\"\"Module for question embedding\n","        \"\"\"\n","        super(UpDnQuestionEmbedding, self).__init__()\n","        assert rnn_type == 'LSTM' or rnn_type == 'GRU'\n","        rnn_cls = nn.LSTM if rnn_type == 'LSTM' else nn.GRU\n","\n","        self.rnn = rnn_cls(\n","            in_dim, num_hid, nlayers,\n","            bidirectional=bidirect,\n","            dropout=dropout,\n","            batch_first=True)\n","\n","        self.in_dim = in_dim\n","        self.num_hid = num_hid\n","        self.nlayers = nlayers\n","        self.rnn_type = rnn_type\n","        self.ndirections = 1 + int(bidirect)\n","\n","    def init_hidden(self, batch):\n","        # just to get the type of tensor\n","        weight = next(self.parameters()).data\n","        hid_shape = (self.nlayers * self.ndirections, batch, self.num_hid)\n","        if self.rnn_type == 'LSTM':\n","            return (Variable(weight.new(*hid_shape).zero_()),\n","                    Variable(weight.new(*hid_shape).zero_()))\n","        else:\n","            return Variable(weight.new(*hid_shape).zero_())\n","\n","    def forward(self, x):\n","        # x: [batch, sequence, in_dim]\n","        batch = x.size(0)\n","        hidden = self.init_hidden(batch)\n","        self.rnn.flatten_parameters()\n","        output, hidden = self.rnn(x, hidden)\n","\n","        if self.ndirections == 1:\n","            return output[:, -1]\n","\n","        forward_ = output[:, -1, :self.num_hid]\n","        backward = output[:, 0, self.num_hid:]\n","        return torch.cat((forward_, backward), dim=1)\n","\n","    def forward_all(self, x):\n","        # x: [batch, sequence, in_dim]\n","        batch = x.size(0)\n","        hidden = self.init_hidden(batch)\n","        self.rnn.flatten_parameters()\n","        output, hidden = self.rnn(x, hidden)\n","        return output\n","\n","\n","class QuestionEmbedding(nn.Module):\n","    def __init__(self, in_dim, num_hid, nlayers=1, bidirect=True, dropout=0, rnn_type='GRU', words_dropout=None,\n","                 dropout_before_rnn=None,\n","                 dropout_after_rnn=None):\n","        \"\"\"Module for question embedding\n","        \"\"\"\n","        super(QuestionEmbedding, self).__init__()\n","        assert rnn_type == 'LSTM' or rnn_type == 'GRU'\n","        rnn_cls = nn.LSTM if rnn_type == 'LSTM' else nn.GRU\n","        self.bidirect = bidirect\n","        self.ndirections = 1 + int(bidirect)\n","        if bidirect:\n","            num_hid = int(num_hid / 2)\n","        self.words_dropout = words_dropout\n","        if dropout_before_rnn is not None:\n","            self.dropout_before_rnn = nn.Dropout(p=dropout_before_rnn)\n","        else:\n","            self.dropout_before_rnn = None\n","        self.rnn = rnn_cls(\n","            in_dim, num_hid, nlayers,\n","            bidirectional=bidirect,\n","            dropout=dropout,\n","            batch_first=True)\n","        if dropout_after_rnn is not None:\n","            self.dropout_after_rnn = nn.Dropout(p=dropout_after_rnn)\n","        else:\n","            self.dropout_after_rnn = None\n","\n","        self.in_dim = in_dim\n","        self.num_hid = num_hid\n","        self.nlayers = nlayers\n","        self.rnn_type = rnn_type\n","\n","    def init_hidden(self, batch):\n","        # just to get the type of tensor\n","        weight = next(self.parameters()).data\n","        hid_shape = (self.nlayers * self.ndirections, batch, self.num_hid)\n","        if self.rnn_type == 'LSTM':\n","            return (Variable(weight.new(*hid_shape).zero_()),\n","                    Variable(weight.new(*hid_shape).zero_()))\n","        else:\n","            return Variable(weight.new(*hid_shape).zero_())\n","\n","    def forward(self, x, qlen=None):\n","        # x: [batch, sequence, in_dim]\n","        batch = x.size(0)\n","        num_tokens = x.size(1)\n","        if self.words_dropout is not None and self.words_dropout > 0:\n","            num_dropout = int(self.words_dropout * num_tokens)\n","            rand_ixs = np.random.randint(0, num_tokens, (batch, num_dropout))\n","            for bix, token_ixs in enumerate(rand_ixs):\n","                x[bix, token_ixs] *= 0\n","        hidden = self.init_hidden(batch)\n","        self.rnn.flatten_parameters()\n","        if self.dropout_before_rnn is not None:\n","            x = self.dropout_before_rnn(x)\n","\n","        q_words_emb, hidden = self.rnn(x, hidden)  # q_words_emb: B x num_words x gru_dim, hidden: 1 x B x gru_dim\n","\n","        out = None\n","        if self.bidirect:\n","            forward_ = q_words_emb[:, -1, :self.num_hid]\n","            backward = q_words_emb[:, 0, self.num_hid:]\n","            hid = torch.cat((forward_, backward), dim=1)\n","            out = hid\n","            # return q_words_emb, hid\n","        else:\n","            out = q_words_emb[:, -1]\n","            # return q_words_emb, q_words_emb[:, -1]\n","\n","        if self.dropout_after_rnn is not None:\n","            out = self.dropout_after_rnn(out)\n","        return out\n","\n","class Seq2SeqRNN(nn.Module):\n","  def __init__(self, input_features, rnn_features, num_layers=1, drop=0.0,\n","               rnn_type='LSTM', rnn_bidirectional=False):\n","    super(Seq2SeqRNN, self).__init__()\n","    self.bidirectional = rnn_bidirectional\n","\n","    if rnn_type == 'LSTM':\n","      self.rnn = nn.LSTM(input_size=input_features,\n","                hidden_size=rnn_features, dropout=drop,\n","                num_layers=num_layers, batch_first=True,\n","                bidirectional=rnn_bidirectional)\n","    elif rnn_type == 'GRU':\n","      self.rnn = nn.GRU(input_size=input_features,\n","                hidden_size=rnn_features, dropout=drop,\n","                num_layers=num_layers, batch_first=True,\n","                bidirectional=rnn_bidirectional)\n","    else:\n","      raise ValueError('Unsupported Type')\n","\n","    self.init_weight(rnn_bidirectional, rnn_type)\n","\n","  def init_weight(self, bidirectional, rnn_type):\n","    self._init_rnn(self.rnn.weight_ih_l0, rnn_type)\n","    self._init_rnn(self.rnn.weight_hh_l0, rnn_type)\n","    self.rnn.bias_ih_l0.data.zero_()\n","    self.rnn.bias_hh_l0.data.zero_()\n","\n","    if bidirectional:\n","      self._init_rnn(self.rnn.weight_ih_l0_reverse, rnn_type)\n","      self._init_rnn(self.rnn.weight_hh_l0_reverse, rnn_type)\n","      self.rnn.bias_ih_l0_reverse.data.zero_()\n","      self.rnn.bias_hh_l0_reverse.data.zero_()\n","\n","  def _init_rnn(self, weight, rnn_type):\n","    chunk_size = 4 if rnn_type == 'LSTM' else 3\n","    for w in weight.chunk(chunk_size, 0):\n","      init.xavier_uniform(w)\n","\n","  def forward(self, q_emb, q_len):\n","    lengths = torch.LongTensor(q_len)\n","    lens, indices = torch.sort(lengths, 0, True)\n","\n","    packed = pack_padded_sequence(q_emb[indices.cuda()], lens.tolist(), batch_first=True)\n","    if isinstance(self.rnn, nn.LSTM):\n","        # pdb.set_trace()\n","        _, ( outputs, _ ) = self.rnn(packed)\n","    elif isinstance(self.rnn, nn.GRU):\n","        _, outputs = self.rnn(packed)\n","\n","    if self.bidirectional:\n","      outputs = torch.cat([ outputs[0, :, :], outputs[1, :, :] ], dim=1)\n","    else:\n","      outputs = outputs.squeeze(0)\n","\n","    _, _indices = torch.sort(indices, 0)\n","    outputs = outputs[_indices.cuda()]\n","\n","    return outputs"]},{"cell_type":"markdown","metadata":{"id":"90sOllcu8X-Z"},"source":["#### Fusion Network\n"]},{"cell_type":"markdown","source":["###### \\_\\_init__.py"],"metadata":{"id":"uksHBN-UsNbI"}},{"cell_type":"code","source":["%%writefile /content/drive/MyDrive/VQA/code/model/fusion_net/__init__.py\n","from .updn import UD\n","from .ban import BAN\n","from .san import SAN\n","from .mlp import MLP"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"2NLDhl6ksNnP","executionInfo":{"status":"ok","timestamp":1769739505557,"user_tz":-420,"elapsed":478,"user":{"displayName":"Toàn Bách Lương","userId":"04806593440338345841"}},"outputId":"4303eb33-8170-4ec5-ce15-07f384815e3f"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["Overwriting /content/drive/MyDrive/VQA/code/model/fusion_net/__init__.py\n"]}]},{"cell_type":"markdown","metadata":{"id":"pSFyIrdRCFts"},"source":["###### mlp.py"]},{"cell_type":"code","execution_count":7,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":194,"status":"ok","timestamp":1769739762838,"user":{"displayName":"Toàn Bách Lương","userId":"04806593440338345841"},"user_tz":-420},"id":"-mkrfzpwCKuO","outputId":"3f8e2075-a0ab-4b66-877d-085ec22b89b3"},"outputs":[{"output_type":"stream","name":"stdout","text":["Overwriting /content/drive/MyDrive/VQA/code/model/fusion_net/mlp.py\n"]}],"source":["%%writefile /content/drive/MyDrive/VQA/code/model/fusion_net/mlp.py\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torch.nn.init as init\n","from torch.autograd import Variable\n","\n","from ..fc import GroupMLP\n","from ..language_model import WordEmbedding\n","\n","from utils import freeze_layer\n","\n","\n","class MLP(nn.Module):\n","    #args, self.train_loader.dataset, self.question_word2vec\n","    # def __init__(self, args, dataset, question_word2vec):\n","    def __init__(self, args, dataset, embedding_weights=None, rnn_bidirectional=True):\n","        super(MLP, self).__init__()\n","        embedding_requires_grad = not args.freeze_w2v  # freeze 则不需要grad\n","        question_features = 300\n","        vision_features = args.output_features  # 图片的\n","\n","        # self.text = BagOfWordsMLPProcessor(\n","        self.text = BagOfWordsProcessor(\n","            embedding_tokens=embedding_weights.size(0) if embedding_weights is not None else dataset.num_tokens,\n","            embedding_weights=embedding_weights,\n","            embedding_features=300,\n","            embedding_requires_grad=embedding_requires_grad,\n","        )\n","        self.mlp = GroupMLP(\n","            in_features=vision_features + question_features,\n","            mid_features= 4 * args.hidden_size,\n","            out_features=args.embedding_size,\n","            drop=0.5,\n","            groups=64,\n","        )\n","\n","        for m in self.modules():\n","            if isinstance(m, nn.Linear) or isinstance(m, nn.Conv2d):\n","                init.xavier_uniform(m.weight)\n","                if m.bias is not None:\n","                    m.bias.data.zero_()\n","\n","    def forward(self, v, b, q, q_len):\n","        q = F.normalize(self.text(q, list(q_len.data)), p=2, dim=1)  # 问题向量求平均值\n","        v = F.normalize(F.avg_pool2d(v, (v.size(2), v.size(3))).squeeze(), p=2, dim=1)\n","\n","        combined = torch.cat([v, q], dim=1)\n","        embedding = self.mlp(combined)\n","        return embedding\n","\n","\n","class BagOfWordsProcessor(nn.Module):\n","    def __init__(self, embedding_tokens, embedding_features,\n","                 embedding_weights, embedding_requires_grad):\n","        super(BagOfWordsProcessor, self).__init__()\n","        self.embedding = nn.Embedding(embedding_tokens, embedding_features, padding_idx=0)\n","        self.embedding.weight.data = embedding_weights\n","        self.embedding.weight.requires_grad = embedding_requires_grad\n","\n","    def forward(self, q, q_len):\n","        embedded = self.embedding(q)\n","        q_len = Variable(torch.Tensor(q_len).view(-1, 1) + 1e-12, requires_grad=False).cuda()\n","\n","        return torch.div(torch.sum(embedded, 1), q_len)"]},{"cell_type":"markdown","metadata":{"id":"mhHcAwnFB3hV"},"source":["###### ban.py"]},{"cell_type":"code","execution_count":6,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":261,"status":"ok","timestamp":1769739741071,"user":{"displayName":"Toàn Bách Lương","userId":"04806593440338345841"},"user_tz":-420},"id":"5TkZ-uc28Xp-","outputId":"b2cadff1-551d-4198-889a-f142d3dc5983"},"outputs":[{"output_type":"stream","name":"stdout","text":["Overwriting /content/drive/MyDrive/VQA/code/model/fusion_net/ban.py\n"]}],"source":["%%writefile /content/drive/MyDrive/VQA/code/model/fusion_net/ban.py\n","\"\"\"\n","Bilinear Attention Networks\n","Jin-Hwa Kim, Jaehyun Jun, Byoung-Tak Zhang\n","https://arxiv.org/abs/1805.07932\n","\n","This code is adapted from: https://github.com/jnhwkim/ban-vqa (written by Jin-Hwa Kim)\n","\"\"\"\n","import torch.nn as nn\n","\n","from ..attention import BiAttention\n","from ..classifier import SimpleClassifier\n","from ..counting import Counter\n","from ..fc import FCNet, BCNet\n","from ..language_model import WordEmbedding, UpDnQuestionEmbedding\n","\n","from utils import freeze_layer\n","\n","\n","class BAN(nn.Module):\n","    #args, self.train_loader.dataset, self.question_word2vec\n","    # def __init__(self, args, dataset, question_word2vec):\n","    def __init__(self, args, dataset, question_word2vec):\n","        super(BAN, self).__init__()\n","        self.args = args\n","        self.w_emb = WordEmbedding(question_word2vec.size(0), 300, .0)\n","        if args.freeze_w2v:\n","            self.w_emb.init_embedding(question_word2vec)\n","            freeze_layer(self.w_emb)\n","        self.q_emb = UpDnQuestionEmbedding(300, args.embedding_size, 1, False, .0)\n","        self.v_att = BiAttention(args.v_dim, self.q_emb.num_hid, self.q_emb.num_hid, args.glimpse)\n","        self.b_net = []\n","        self.q_prj = []\n","        self.c_prj = []\n","        self.objects = 10  # minimum number of boxes\n","        for i in range(args.glimpse):\n","            self.b_net.append(BCNet(args.v_dim, self.q_emb.num_hid, self.q_emb.num_hid, None, k=1))\n","            self.q_prj.append(FCNet([self.q_emb.num_hid, self.q_emb.num_hid], '', .2))\n","            self.c_prj.append(FCNet([self.objects + 1, self.q_emb.num_hid], 'ReLU', .0))\n","\n","        self.b_net = nn.ModuleList(self.b_net)\n","        self.q_prj = nn.ModuleList(self.q_prj)\n","        self.c_prj = nn.ModuleList(self.c_prj)\n","        self.counter = Counter(self.objects)\n","        self.drop = nn.Dropout(.5)\n","        self.tanh = nn.Tanh()\n","\n","    def forward(self, v, b, q, q_len):\n","        \"\"\"Forward\n","\n","        v: [batch, num_objs, obj_dim]\n","        b: [batch, num_objs, b_dim]\n","        q: [batch_size, seq_length]\n","\n","        return: logits, not probs\n","        \"\"\"\n","        w_emb = self.w_emb(q)\n","        q_emb = self.q_emb.forward_all(w_emb)  # [batch, q_len, q_dim]\n","        boxes = b[:, :, :4].transpose(1, 2)\n","\n","        b_emb = [0] * self.args.glimpse\n","        att, logits = self.v_att.forward_all(v, q_emb)  # b x g x v x q\n","\n","        for g in range(self.args.glimpse):\n","            b_emb[g] = self.b_net[g].forward_with_weights(v, q_emb, att[:, g, :, :])  # b x l x h\n","\n","            atten, _ = logits[:, g, :, :].max(2)\n","            embed = self.counter(boxes, atten)\n","\n","            q_emb = self.q_prj[g](b_emb[g].unsqueeze(1)) + q_emb\n","            q_emb = q_emb + self.c_prj[g](embed).unsqueeze(1)\n","\n","        return q_emb.sum(1)"]},{"cell_type":"markdown","metadata":{"id":"bs1YQHcXB0JV"},"source":["###### san.py"]},{"cell_type":"code","execution_count":9,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":205,"status":"ok","timestamp":1769739810225,"user":{"displayName":"Toàn Bách Lương","userId":"04806593440338345841"},"user_tz":-420},"id":"65hdTizH8hFy","outputId":"8f393858-e14b-4a5c-8b81-b4c5417ad772"},"outputs":[{"output_type":"stream","name":"stdout","text":["Overwriting /content/drive/MyDrive/VQA/code/model/fusion_net/san.py\n"]}],"source":["%%writefile /content/drive/MyDrive/VQA/code/model/fusion_net/san.py\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torch.nn.init as init\n","\n","from torch.autograd import Variable\n","\n","from ..attention import SanAttention, apply_attention\n","from ..fc import GroupMLP\n","from ..language_model import Seq2SeqRNN, WordEmbedding\n","\n","import pdb\n","from utils import freeze_layer\n","\n","class SAN(nn.Module):\n","    #args, self.train_loader.dataset, self.question_word2vec\n","    #def __init__(self, args, dataset, question_word2vec):\n","    def __init__(self, args, dataset,embedding_weights=None,rnn_bidirectional=True):\n","        super(SAN, self).__init__()\n","        embedding_requires_grad = not args.freeze_w2v\n","        question_features = 1024\n","        rnn_features = int(question_features // 2) if rnn_bidirectional else int(question_features)\n","        vision_features = args.output_features\n","        glimpses = 2\n","\n","        # vocab_size = embedding_weights.size(0)\n","        # vector_dim = embedding_weights.size(1)\n","        # self.embedding = nn.Embedding(vocab_size, vector_dim, padding_idx=0)\n","        # self.embedding.weight.data = embedding_weights\n","        # self.embedding.weight.requires_grad = embedding_requires_grad\n","        self.w_emb = WordEmbedding(embedding_weights.size(0), 300, .0)\n","        if args.freeze_w2v:\n","            self.w_emb.init_embedding(embedding_weights)\n","            freeze_layer(self.w_emb)\n","\n","        self.drop = nn.Dropout(0.5)\n","        self.text = Seq2SeqRNN(\n","            input_features=embedding_weights.size(1),\n","            rnn_features=int(rnn_features),\n","            rnn_type='LSTM',\n","            rnn_bidirectional=rnn_bidirectional,\n","        )\n","        self.attention = SanAttention(\n","            v_features=vision_features,\n","            q_features=question_features,\n","            mid_features=512,\n","            glimpses=2,\n","            drop=0.5,\n","        )\n","        self.mlp = GroupMLP(\n","            in_features=glimpses * vision_features + question_features,\n","            mid_features= 4 * args.hidden_size,\n","            out_features=args.embedding_size,\n","            drop=0.5,\n","            groups=64,\n","        )\n","\n","        for m in self.modules():\n","            if isinstance(m, nn.Linear) or isinstance(m, nn.Conv2d):\n","                init.xavier_uniform(m.weight)\n","                if m.bias is not None:\n","                    m.bias.data.zero_()\n","\n","\n","\n","    def forward(self, v, b, q, q_len):\n","        # pdb.set_trace()\n","        q = self.text(self.drop(self.w_emb(q)), list(q_len.data))\n","        # q = self.text(self.embedding(q), list(q_len.data))\n","\n","        v = F.normalize(v, p=2, dim=1)\n","        a = self.attention(v, q)\n","        v = apply_attention(v, a)\n","\n","        combined = torch.cat([v, q], dim=1)\n","        embedding = self.mlp(combined)\n","        return embedding"]},{"cell_type":"markdown","metadata":{"id":"7gYdxg8zDDTq"},"source":["###### updn.py"]},{"cell_type":"code","execution_count":10,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1492,"status":"ok","timestamp":1769739841192,"user":{"displayName":"Toàn Bách Lương","userId":"04806593440338345841"},"user_tz":-420},"id":"pjal-g27DCca","outputId":"c2ff7fd2-0134-4a8f-8f59-5253f7fc0067"},"outputs":[{"output_type":"stream","name":"stdout","text":["Overwriting /content/drive/MyDrive/VQA/code/model/fusion_net/updn.py\n"]}],"source":["%%writefile /content/drive/MyDrive/VQA/code/model/fusion_net/updn.py\n","\n","import torch\n","import torch.nn as nn\n","\n","from ..language_model import WordEmbedding, UpDnQuestionEmbedding\n","from ..attention import UpDnAttention\n","from ..classifier import SimpleClassifier\n","from ..fc import FCNet\n","\n","from utils import freeze_layer\n","\n","class UD(nn.Module):\n","    def __init__(self, args, dataset, question_word2vec):\n","        super(UD, self).__init__()\n","        self.w_emb = WordEmbedding(question_word2vec.size(0), 300, 0.0)\n","        if args.freeze_w2v:\n","            self.w_emb.init_embedding(question_word2vec)\n","            freeze_layer(self.w_emb)\n","            # self.w_emb.weight.requires_grad = False\n","\n","        self.q_emb = UpDnQuestionEmbedding(300, args.embedding_size, 1, False, 0.0)\n","        self.v_att = UpDnAttention(args.v_dim, self.q_emb.num_hid, args.embedding_size)\n","        self.q_net = FCNet([self.q_emb.num_hid, args.embedding_size])\n","        self.v_net = FCNet([args.v_dim, args.embedding_size])\n","        # self.classifier = SimpleClassifier(\n","        #     args.embedding_size, args.embedding_size * 2, args.num_ans_candidates, 0.5)\n","\n","    def forward(self, v, b, q, qlen):\n","        \"\"\"Forward\n","\n","        v: [batch, num_objs, obj_dim]\n","        b: [batch, num_objs, b_dim]\n","        q: [batch_size, seq_length]\n","\n","        return: logits, not probs\n","        \"\"\"\n","        # print(\"q = {}\".format(q))\n","        w_emb = self.w_emb(q)\n","        # print(\"w_emb = {}\".format(w_emb))\n","        q_emb = self.q_emb(w_emb)  # [batch, q_dim]\n","\n","        att = self.v_att(v, q_emb) # [spa, 1]\n","        v_emb = (att * v).sum(1)  # [batch, v_dim]\n","\n","        q_repr = self.q_net(q_emb)\n","        v_repr = self.v_net(v_emb)\n","        joint_repr = q_repr * v_repr\n","       # logits = self.classifier(joint_repr)\n","        return joint_repr"]},{"cell_type":"markdown","metadata":{"id":"S-Qxepl-8kPv"},"source":["#### Answer Network"]},{"cell_type":"markdown","source":["###### \\_\\_init__.py"],"metadata":{"id":"YYFO06aBscFv"}},{"cell_type":"code","source":["%%writefile /content/drive/MyDrive/VQA/code/model/answer_net/__init__.py\n","from .mlp import MLP"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"eRem2auFsb_M","executionInfo":{"status":"ok","timestamp":1769739588948,"user_tz":-420,"elapsed":2012,"user":{"displayName":"Toàn Bách Lương","userId":"04806593440338345841"}},"outputId":"7608ed48-96c1-435d-d2eb-ca70fbdd2e06"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["Overwriting /content/drive/MyDrive/VQA/code/model/answer_net/__init__.py\n"]}]},{"cell_type":"markdown","source":["###### mlp.py"],"metadata":{"id":"NnFXP8owtDxo"}},{"cell_type":"code","source":["%%writefile /content/drive/MyDrive/VQA/code/model/answer_net/mlp.py\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torch.nn.init as init\n","import pdb\n","import model.fc as FC\n","from ..fc import GroupMLP, GroupMLP_2lay, GroupMLP_1lay\n","\n","\n","class MLP(nn.Module):\n","    def __init__(self, args, dataset):\n","        super(MLP, self).__init__()\n","        ans_net_list = [\"GroupMLP\", \"GroupMLP_1lay\", \"GroupMLP_2lay\"]\n","        ans_net = ans_net_list[args.ans_net_lay]\n","        self.mlp = getattr(FC, ans_net)(\n","            in_features=args.ans_feature_len,  # fan\n","            mid_features=args.hidden_size,  # 2048\n","            out_features=args.embedding_size,  # fan\n","            drop=0.0,\n","            groups=64,  # 64\n","        )\n","\n","        for m in self.modules():\n","            if isinstance(m, nn.Linear) or isinstance(m, nn.Conv2d):\n","                init.xavier_uniform(m.weight)\n","                if m.bias is not None:\n","                    m.bias.data.zero_()\n","\n","    def forward(self, a, a_len=None):\n","        # pdb.set_trace()\n","        return self.mlp(F.normalize(a, p=2))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"jXAAtOV1tEW7","executionInfo":{"status":"ok","timestamp":1769739690784,"user_tz":-420,"elapsed":276,"user":{"displayName":"Toàn Bách Lương","userId":"04806593440338345841"}},"outputId":"043228a1-7eda-4994-836b-ffb14635bc63"},"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["Overwriting /content/drive/MyDrive/VQA/code/model/answer_net/mlp.py\n"]}]},{"cell_type":"markdown","metadata":{"id":"BiyTbn5Z5GNM"},"source":["## Content\n"]},{"cell_type":"markdown","metadata":{"id":"UDIFGBnkeYG8"},"source":["###### deal_data.py"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"12bSzVRVeYY_"},"outputs":[],"source":["%%writefile /content/drive/MyDrive/VQA/code/data/deal_data.py\n","from config import cfg\n","import os.path as osp\n","import pickle\n","import json\n","import pdb\n","import re\n","from utils import dele_a, transfer, hand_remove, deal_fact\n","from collections import defaultdict\n","import tqdm\n","import Levenshtein\n","import wordninja\n","from data import fvqa, preprocess\n","import random\n","import numpy as np\n","from collections import defaultdict\n","\n","\n","class Runner:\n","    def __init__(self, args):\n","        self.args = args\n","        self.path = osp.join(args.data_root, \"data\", \"FVQA/\")\n","        self.data_path = osp.join(self.path, \"new_dataset_release\")\n","        self.split_path = osp.join(self.path, \"Name_Lists\")\n","        self.exp_data = osp.join(args.data_root, \"fvqa\", \"exp_data\")\n","        self.e1_list = []\n","        self.r_list = []\n","        self.e2_list = []\n","        self.entity_list = []\n","        # 这个entity 在哪些VQA pair中出现过。\n","        self.e1_show_key = defaultdict(list)\n","        self.e2_show_key = defaultdict(list)\n","        self.all_entity = []\n","\n","    # 得到一个所有fact都直接被包含在内的 json文件（不需要跳转）\n","    def get_new_all_json(self):\n","        path = osp.join(self.data_path, \"all_qs_dict_release_combine_all.json\")\n","\n","        if not osp.exists(path):\n","            with open(osp.join(self.data_path, \"all_fact_triples_release.json\"), \"r\", encoding='utf8') as ffp:\n","                dic_all = json.load(ffp)\n","                # pdb.set_trace()\n","                for i in dic_all.keys():\n","                    # fact_source = dic[i][\"fact\"][0]\n","                    fact = dic_all[i]\n","                    fact['e1'] = deal_fact(dic_all[i], fact['e1'])\n","                    fact['e2'] = deal_fact(dic_all[i], fact['e2'])\n","                    dic_all[i][\"fact\"] = []\n","                    dic_all[i][\"fact\"].append(fact['e1'])\n","                    dic_all[i][\"fact\"].append(fact['r'].split('/')[-1])\n","                    dic_all[i][\"fact\"].append(fact['e2'])\n","                    # pdb.set_trace()\n","                    del dic_all[i]['KB']\n","                    del dic_all[i]['e1_label']\n","                    # del dic_all[i]['uri']\n","                    del dic_all[i]['e2_label']\n","                    # del dic_all[i]['sources']\n","                    # del dic_all[i]['context']\n","                    del dic_all[i]['score']\n","        else:\n","            # 需要人工去噪\n","            with open(path, 'w') as fd:\n","                json.dump(dic_all, fd)\n","                print(\"get_new_json_combile done!（remember to do some human check !!!）\")\n","\n","    # 得到一个所有fact都直接被包含在内的 json文件（不需要跳转）\n","    def get_new_json(self):\n","        path = osp.join(self.data_path, \"all_qs_dict_release_combine.json\")\n","        if not osp.exists(path):\n","            with open(osp.join(self.data_path, \"all_qs_dict_release_cp.json\"), \"r\") as fp:\n","                dic = json.load(fp)\n","                with open(osp.join(self.data_path, \"all_fact_triples_release.json\"), \"r\", encoding='utf8') as ffp:\n","                    dic_all = json.load(ffp)\n","                    # pdb.set_trace()\n","                    for i in dic_all.keys():\n","                        fact_source = dic[i][\"fact\"][0]\n","                        fact = dic_all[fact_source]\n","                        fact['e1'] = deal_fact(dic[i], fact['e1'])\n","                        fact['e2'] = deal_fact(dic[i], fact['e2'])\n","                        dic[i][\"fact\"][0] = fact['e1']\n","                        dic[i][\"fact\"].append(fact['r'].split('/')[-1])\n","                        dic[i][\"fact\"].append(fact['e2'])\n","                        del dic[i]['ans_source']\n","                        del dic[i]['visual_concept']\n","            # 需要人工去噪\n","            with open(path, 'w') as fd:\n","                json.dump(dic, fd)\n","                print(\"get_new_json done!（remember to do some human check !!!）\")\n","\n","    def get_entity_filter(self):\n","        # 把头尾实体筛选一遍，并且储存\n","\n","        with open(osp.join(self.data_path, \"all_qs_dict_release_combine.json\"), 'r') as fp:\n","            dic = json.load(fp)\n","            for i in dic.keys():\n","                for j in [0, 1, 2]:\n","                    dic[i][\"fact\"][j] = dic[i][\"fact\"][j].lower().replace(\"  \", \" \")\n","                    if dic[i][\"fact\"][j][0] == \" \":\n","                        dic[i][\"fact\"][j] = dic[i][\"fact\"][j][1:]\n","                    if len(dic[i][\"fact\"][j]) > 2 and dic[i][\"fact\"][j][-2] == \"#\":\n","                        dic[i][\"fact\"][j] = dic[i][\"fact\"][j][:-2]\n","\n","                self.e1_list.append(dic[i][\"fact\"][0])\n","                self.r_list.append(dic[i][\"fact\"][1])\n","                self.e2_list.append(dic[i][\"fact\"][2])\n","                self.e1_show_key[dic[i][\"fact\"][0]].append(i)\n","                self.e2_show_key[dic[i][\"fact\"][2]].append(i)\n","            self.entity_list = set(self.e1_list + self.e2_list)\n","            self.entity_list = list(self.entity_list)\n","            self.r_list = list(set(self.r_list))\n","            # pdb.set_trace()\n","            print(\"get_entity_filter done!\")\n","\n","    def get_all_entity(self):\n","        path = osp.join(self.data_path, 'ids_new.data')\n","        if not osp.exists(path):\n","\n","            # 得到所有的头尾实体，并且排序\n","            entity_list = []\n","            with open(osp.join(self.data_path, \"FVQA_triple_new_2.txt\"), 'r', encoding='utf-8') as f:\n","                # k = 0\n","                while 1:\n","                    line = f.readline()\n","                    if not line:\n","                        break\n","                    if line[:3] == '***':\n","                        continue\n","                    # k += 1\n","                    # if k % 1000 == 0:\n","                    #     print(k, len(lis))\n","                    line = re.split('\\t|\\n', line)\n","                    entity_list.append(line[0].lower().replace(\"-\", \" \"))\n","                    entity_list.append(line[2].lower().replace(\"-\", \" \"))\n","            entity_set = set(entity_list)\n","\n","            def rule_4(a):\n","                return entity_list.count(a)\n","\n","            entity_sort = list(set(entity_list))\n","            entity_sort.sort(key=rule_4, reverse=True)\n","\n","            with open(path, 'wb') as f:\n","                pickle.dump(entity_sort, f)\n","\n","        else:\n","            with open(path, 'rb') as f:  # 按出现数量排序过了的实体\n","                print(\"load ids_new.data\")\n","                entity_sort = pickle.load(f)\n","\n","        entity_sort.remove('y')\n","        entity_sort.remove('and')\n","        entity_sort.remove('yes')\n","        entity_sort.remove('no')\n","\n","        path = osp.join(self.data_path, \"all_qs_dict_release_combine_filter.json\")\n","\n","        if not osp.exists(path):\n","            with open(osp.join(self.data_path, \"all_qs_dict_release_combine.json\"), 'r') as fp:\n","                dic = json.load(fp)\n","                Noin = []\n","                for entity in tqdm.tqdm(self.entity_list):\n","                    entity_orig = entity\n","                    entity = entity.replace(\"_\", \" \").replace(\"-\", \" \")\n","                    entity = entity.replace(\"Category:\", \"\").replace(\"category:\", \"\")\n","                    entity = entity.replace(\"(\", \"\").replace(\")\", \"\")\n","                    entity_list = [entity]\n","\n","                    dele_a_list = dele_a(entity)\n","                    transfer_a = [transfer(entity)]\n","                    # entity_list.append(no_)\n","                    entity_list = entity_list + transfer_a  # 变形\n","                    entity_list = entity_list + dele_a_list  # 去冠词\n","                    entity_list = entity_list + dele_a(transfer_a[0])  # 变形后去冠词\n","                    for i in dele_a_list:\n","                        entity_list = entity_list + [transfer(i)]  # 去冠词后变形\n","\n","                    entity_list = list(set(entity_list))\n","                    hand_list = []\n","                    for k in entity_list:\n","                        hand_list = hand_list + hand_remove(k)  # 手动去特殊形式\n","                    entity_list = entity_list + list(set(hand_list))\n","                    entity_list = list(set(entity_list))\n","                    flag = 0\n","\n","                    # print(\"change entity...\")\n","                    for key in entity_sort:\n","                        if key in entity_list:\n","                            flag = 1\n","                            self.all_entity.append(key)\n","                            for j in self.e1_show_key[entity_orig]:  # 答案是这个的编号\n","                                dic[j]['fact'][0] = key\n","                            for j in self.e2_show_key[entity_orig]:  # 答案是这个的编号\n","                                dic[j]['fact'][2] = key\n","                            break\n","                    if flag:\n","                        continue\n","\n","                    Noin.append(entity_orig)\n","            print(\"all entity num:\", len(list(set(self.all_entity))))\n","            print(\"no in :\", Noin)\n","            print(\"no in num :\", len(Noin))\n","\n","            # entity 筛选过的。此时答案和entity 统一了\n","\n","            with open(path, 'w') as fp:\n","                json.dump(dic, fp)\n","            print(\"get_all_entity filter done!\")\n","\n","    def fusion_answer_and_entity(self):\n","        # 把答案里面出现的entity对齐到entity中。\n","        # 使用编辑距离\n","        path = osp.join(self.data_path, \"all_qs_dict_release_combine_filter_fusion.json\")\n","        if not osp.exists(path):\n","            with open(osp.join(self.data_path, \"all_qs_dict_release_combine_filter.json\"), 'r') as fp:\n","                dic = json.load(fp)\n","                with open(osp.join(self.data_path, \"ans_entity_map.txt\"), 'w') as ffp:\n","                    for key in dic.keys():\n","                        strout = \"not match: \"\n","                        e1 = dic[key][\"fact\"][0]\n","                        e2 = dic[key][\"fact\"][2]\n","                        ans = dic[key][\"answer\"]\n","                        # 和头实体相似度大于尾实体\n","                        if Levenshtein.ratio(ans, e1) > Levenshtein.ratio(ans, e2):\n","                            strout += dic[key][\"fact\"][2]\n","                            strout += \"\\t\\t\\t\\t\\t  match: \"\n","                            strout += dic[key][\"fact\"][0]\n","                            strout += \" -> \"\n","                            strout += ans\n","                            dic[key][\"fact\"][0] = ans\n","\n","                        else:\n","                            strout += dic[key][\"fact\"][0]\n","                            strout += \"\\t\\t\\t\\t\\t  match: \"\n","                            strout += dic[key][\"fact\"][2]\n","                            strout += \" -> \"\n","                            strout += ans\n","                            dic[key][\"fact\"][2] = ans\n","                        ffp.write(strout + \"\\n\")\n","\n","                    print(\"fusion_answer_and_entity done!\")\n","\n","                with open(osp.join(self.data_path, \"all_qs_dict_release_combine_filter_fusion.json\"), 'w') as fp:\n","                    json.dump(dic, fp)\n","\n","    def statistics_of_ans_and_entity(self, name=None, path=None):\n","        # 数据统计\n","        if path == None:\n","            path = osp.join(self.data_path, name)\n","\n","        with open(path, 'r') as fp:\n","            dic = json.load(fp)\n","            ans_set = set()\n","            entity_set = set()\n","            relation_set = set()\n","            dic_len = 0\n","            for key in dic.keys():\n","                dic_len += 1\n","                e1 = dic[key][\"fact\"][0]\n","                r = dic[key][\"fact\"][1]\n","                e2 = dic[key][\"fact\"][2]\n","                ans = dic[key][\"answer\"]\n","                ans_set.add(ans)\n","                entity_set.add(e1)\n","                entity_set.add(e2)\n","                relation_set.add(r)\n","\n","            ans_or_entity = ans_set | entity_set\n","            ans_and_entity = ans_set & entity_set\n","            print(\"ans_set len:\", len(ans_set))\n","            print(\"entity_set len:\", len(entity_set))\n","            print(\"ans_or_entity len:\", len(ans_or_entity))\n","            print(\"ans_and_entity len:\", len(ans_and_entity))\n","            print(\"relation len:\", len(relation_set))\n","            print(\"dic len:\", dic_len)\n","\n","    def filter_top500_IQA_pair(self):\n","        # read ans file\n","        # store the map from id to ans (with dic)\n","        # TODO: optimize the code with matrix\n","        path = osp.join(self.data_path, \"all_qs_dict_release_combine_filter_fusion_500.json\")\n","        if not osp.exists(path):\n","            ans_2_id = {}\n","            with open(osp.join(self.data_path, \"ans.txt\"), 'r', encoding='utf-8') as f:\n","                while 1:\n","                    line = f.readline()\n","                    if not line:\n","                        break\n","                    line = re.split('-|\\n', line)\n","                    ans_2_id[line[1]] = int(line[0])\n","            print(len(ans_2_id))\n","            with open(osp.join(self.data_path, \"all_qs_dict_release_combine_filter_fusion.json\"), 'r') as fp:\n","                dic = json.load(fp)\n","                dic_500 = {key: value for key, value in dic.items() if\n","                           dic[key][\"answer\"] in ans_2_id.keys() and ans_2_id[dic[key][\"answer\"]] <= 500}\n","\n","            with open(path, 'w') as fp:\n","                json.dump(dic_500, fp)\n","                print(\"filter_top500_IQA_pair done!\")\n","\n","    def deal_relation(self):\n","        path = osp.join(self.data_path, \"all_qs_dict_release_combine_filter_fusion_500.json\")\n","        if not osp.exists(path):\n","            with open(osp.join(self.data_path, \"all_qs_dict_release_combine_filter_fusion_500.json\"), 'r') as fp:\n","                dic = json.load(fp)\n","            relation_set = set()\n","            for key in dic.keys():\n","                relation_set.add(dic[key][\"fact\"][1])\n","\n","            print(\"relation len:\", len(relation_set))\n","            relation_map = {}\n","            for relation in list(relation_set):\n","                relation_orig = relation\n","                # 是否需要把关系去掉？\n","                if relation[-2] == \"#\":\n","                    relation = relation[:-2]\n","\n","                relation_split = wordninja.split(relation)\n","                for i in range(len(relation_split)):\n","                    relation_split[i] = relation_split[i].lower()\n","\n","                if relation == \"transnbhd\":\n","                    relation_map[relation_orig] = \"belong to\"\n","                else:\n","                    relation_map[relation_orig] = ' '.join(relation_split)\n","\n","            print(relation_map)\n","            for key in dic.keys():\n","                tmp = dic[key][\"fact\"][1]\n","                dic[key][\"fact\"][1] = relation_map[tmp]\n","\n","            with open(path, 'w') as fp:\n","                json.dump(dic, fp)\n","                print(\"deal_relation done!\")\n","\n","    def split_data(self):\n","        # 把数据集划分出来\n","        for i in range(0, 5):\n","            num = str(i)\n","            train_name = osp.join(self.args.FVQA.train_data_path, \"train\" + num, \"all_qs_dict_release_train_500.json\")\n","            test_name = osp.join(self.args.FVQA.test_data_path, \"test\" + num, \"all_qs_dict_release_test_500.json\")\n","\n","            if osp.exists(train_name) and osp.exists(test_name):\n","                continue\n","\n","            img_train = []\n","            img_test = []\n","\n","            with open(osp.join(self.split_path, \"train_list_\" + num + \".txt\"), \"r\") as f:\n","                while 1:\n","                    line = f.readline()\n","                    if not line:\n","                        break\n","                    line = re.split('\\n', line)\n","                    img_train.append(line[0])\n","\n","            with open(osp.join(self.split_path, \"test_list_\" + num + \".txt\"), \"r\") as f:\n","                while 1:\n","                    line = f.readline()\n","                    if not line:\n","                        break\n","                    line = re.split('\\n', line)\n","                    img_test.append(line[0])\n","\n","            with open(osp.join(self.data_path, \"all_qs_dict_release_combine_filter_fusion_500.json\"), 'r') as fp:\n","                dic = json.load(fp)\n","\n","                dic_train = {key: value for key, value in dic.items() if dic[key][\"img_file\"] in img_train}\n","                dic_test = {key: value for key, value in dic.items() if dic[key][\"img_file\"] in img_test}\n","\n","            # train_name = osp.join(cfg.FVQA.train_data_path, \"train\" + num, \"all_qs_dict_release_train_500.json\")\n","            # test_name = osp.join(cfg.FVQA.test_data_path, \"test\" + num, \"all_qs_dict_release_test_500.json\")\n","            ans_train = []\n","            ans_test = []\n","            q_train = []\n","            q_test = []\n","            for key, value in dic_train.items():\n","                ans_train.append(dic_train[key][\"answer\"])\n","                q_train.append(dic_train[key][\"question\"])\n","            for key, value in dic_test.items():\n","                ans_test.append(dic_test[key][\"answer\"])\n","                q_test.append(dic_test[key][\"question\"])\n","\n","            ans_train_set = set(ans_train)\n","            q_train_set = set(q_train)\n","            ans_test_set = set(ans_test)\n","            q_test_set = set(q_test)\n","\n","            with open(train_name, \"w\") as ff:\n","                json.dump(dic_train, ff)\n","                print(\"save to:\", train_name)\n","\n","            with open(test_name, \"w\") as ff:\n","                json.dump(dic_test, ff)\n","                print(\"save to:\", test_name)\n","            # ans_set len: 387\n","            # entity_set len: 1842\n","            # ans_or_entity len: 1842\n","            # ans_and_entity len: 387\n","            # relation len: 71\n","            # dic len: 2669\n","            print(num, \" train :\", len(dic_train), len(ans_train_set), len(q_train_set))\n","            self.statistics_of_ans_and_entity(train_name)\n","\n","            # ans_set len: 403\n","            # entity_set len: 1958\n","            # ans_or_entity len: 1958\n","            # ans_and_entity len: 403\n","            # relation len: 87\n","            # dic len: 2823\n","            print(num, \" test :\", len(dic_test), len(ans_test_set), len(q_test_set))\n","            self.statistics_of_ans_and_entity(test_name)\n","            print(\"dataset \" + num + \" done!\")\n","\n","    def preprocess_answer(self):\n","        pass\n","\n","    def preprocess_fact(self):\n","        output_format = osp.join(self.args.FVQA.common_data_path, \"answer.vocab.fvqa.fact.500.json\")\n","        if not osp.exists(output_format):\n","            num = 2\n","            vqa_train_questions = osp.join(self.args.FVQA.train_data_path, \"train\" + str(num), \"all_qs_dict_release_train_500.json\")\n","            vqa_val_questions = osp.join(self.args.FVQA.test_data_path, \"test\" + str(num), \"all_qs_dict_release_test_500.json\")\n","            with open(vqa_train_questions, 'r') as fd:\n","                qaq1 = json.load(fd)\n","            with open(vqa_val_questions, 'r') as fd:\n","                qaq2 = json.load(fd)\n","\n","            annotations = {**qaq1, **qaq2}\n","            # word2vec = Vector()\n","            facts = fvqa.prepare_fact(annotations)\n","            fact_vocab = preprocess.extract_vocab(facts, top_k=None)\n","            vocabs = {'answer': fact_vocab}\n","            print('* Dump output vocab to: {}'.format(output_format))\n","            with open(output_format, 'w') as fd:\n","                json.dump(vocabs, fd)\n","        print(\"preprocess_fact done!\")\n","\n","    def preprocess_relation(self):\n","        output_format = osp.join(self.args.FVQA.common_data_path, \"answer.vocab.fvqa.relation.500.json\")\n","        if not osp.exists(output_format):\n","            num = 2\n","            vqa_train_questions = osp.join(self.args.FVQA.train_data_path, \"train\" + str(num), \"all_qs_dict_release_train_500.json\")\n","            vqa_val_questions = osp.join(self.args.FVQA.test_data_path, \"test\" + str(num), \"all_qs_dict_release_test_500.json\")\n","            with open(vqa_train_questions, 'r') as fd:\n","                qaq1 = json.load(fd)\n","            with open(vqa_val_questions, 'r') as fd:\n","                qaq2 = json.load(fd)\n","\n","            annotations = {**qaq1, **qaq2}\n","            # word2vec = Vector()\n","            relations = fvqa.prepare_relation(annotations)\n","            relation_vocab = preprocess.extract_vocab(relations, top_k=None)\n","            vocabs = {'answer': relation_vocab}\n","            print('* Dump output vocab to: {}'.format(output_format))\n","            with open(output_format, 'w') as fd:\n","                json.dump(vocabs, fd)\n","        print(\"preprocess_relation done!\")\n","\n","    def split_unseen_data(self):\n","        ans_2_id = {}\n","\n","        with open(osp.join(self.data_path, \"ans.txt\"), 'r', encoding='utf-8') as f:\n","            while 1:\n","                line = f.readline()\n","                if not line:\n","                    break\n","                line = re.split('-|\\n', line)\n","                ans_2_id[line[1]] = int(line[0])\n","        print(len(ans_2_id))\n","        num = \"0\"\n","        img = []\n","        with open(osp.join(self.split_path, \"train_list_\" + num + \".txt\"), \"r\") as f:\n","            while 1:\n","                line = f.readline()\n","                if not line:\n","                    break\n","                line = re.split('\\n', line)\n","                img.append(line[0])\n","\n","        with open(osp.join(self.split_path, \"test_list_\" + num + \".txt\"), \"r\") as f:\n","            while 1:\n","                line = f.readline()\n","                if not line:\n","                    break\n","                line = re.split('\\n', line)\n","                img.append(line[0])\n","\n","        with open(osp.join(self.data_path, \"all_qs_dict_release_combine_filter_fusion_500.json\"), 'r') as fp:\n","            dic = json.load(fp)\n","\n","        dic_all = {key: value for key, value in dic.items() if dic[key][\"img_file\"] in img}\n","        ans_id = list(range(1, 501))\n","\n","        # split_unseen_data\n","        for i in range(5):\n","            num = str(i)\n","            train_name = osp.join(self.args.FVQA.seen_train_data_path, \"train\" + num, \"all_qs_dict_release_train_500.json\")\n","            test_name = osp.join(self.args.FVQA.unseen_test_data_path, \"test\" + num, \"all_qs_dict_release_test_500.json\")\n","\n","            if not(osp.exists(train_name) and osp.exists(train_name)):\n","                ans_seen = random.sample(ans_id, 250)\n","                ans_unseen = list(set(ans_id) - set(ans_seen))\n","\n","                dic_seen = {key: value for key, value in dic_all.items() if ans_2_id[dic[key][\"answer\"]] in ans_seen}\n","                dic_unseen = {key: value for key, value in dic_all.items() if ans_2_id[dic[key][\"answer\"]] in ans_unseen}\n","\n","                with open(train_name, \"w\") as ff:\n","                    json.dump(dic_seen, ff)\n","                with open(test_name, \"w\") as ff:\n","                    json.dump(dic_unseen, ff)\n","\n","                self.statistics_of_ans_and_entity(train_name)\n","\n","                self.statistics_of_ans_and_entity(test_name)\n","\n","                print(\"dataset \" + num + \" done!\")\n","\n","    def get_fact_relation_matrix(self):\n","        if not osp.exists(self.args.FVQA.fact_relation_to_ans_path):\n","\n","            answer_vocab_path = self.args.FVQA.answer_vocab_path\n","            fact_vocab_path = self.args.FVQA.fact_vocab_path\n","            relation_vocab_path = self.args.FVQA.relation_vocab_path\n","\n","            with open(fact_vocab_path, 'r') as fd:\n","                fact_vocab = json.load(fd)\n","\n","            with open(relation_vocab_path, 'r') as fd:\n","                relation_vocab = json.load(fd)\n","\n","            with open(answer_vocab_path, 'r') as fd:\n","                answer_vocab = json.load(fd)\n","\n","            self.answer_to_index = answer_vocab['answer']\n","            self.index_to_answer = preprocess.invert_dict(self.answer_to_index)\n","            self.fact_to_index = fact_vocab['answer']\n","            self.index_to_fact = preprocess.invert_dict(self.fact_to_index)\n","            self.relation_to_index = relation_vocab['answer']\n","            self.index_to_relation = preprocess.invert_dict(self.relation_to_index)\n","\n","            output_format = osp.join(self.args.FVQA.common_data_path, \"fact_relation_dict.data\")\n","            vqa_train_questions = osp.join(self.args.FVQA.train_data_path, \"train2\", \"all_qs_dict_release_train_500.json\")\n","            vqa_val_questions = osp.join(self.args.FVQA.test_data_path, \"test2\", \"all_qs_dict_release_test_500.json\")\n","            with open(vqa_train_questions, 'r') as fd:\n","                qaq1 = json.load(fd)\n","            with open(vqa_val_questions, 'r') as fd:\n","                qaq2 = json.load(fd)\n","\n","            annotations = {**qaq1, **qaq2}\n","\n","            fact_num = len(self.fact_to_index)\n","            ans_num = len(self.answer_to_index)\n","            rel_num = len(self.relation_to_index)\n","\n","            # fact_relation_matrix = - np.ones([fact_num,rel_num ], dtype = int)\n","            fact_relation_to_ans = defaultdict(list)\n","\n","            keys = list(annotations.keys())\n","\n","            for a in keys:\n","                answer = annotations[a][\"answer\"]\n","                facts = annotations[a][\"fact\"]\n","                f1 = facts[0]\n","                rel = facts[1]\n","                f2 = facts[2]\n","                assert (answer == f1 or answer == f2)\n","                if answer == f1:\n","                    fact = f2\n","                else:\n","                    fact = f1\n","\n","                fact = preprocess.process_punctuation(fact)\n","                rel = preprocess.process_punctuation(rel)\n","                name = str(self.fact_to_index[fact]) + \"-\" + str(self.relation_to_index[rel])\n","                fact_relation_to_ans[name].append(self.answer_to_index[answer])\n","\n","            with open(output_format, 'w') as fd:\n","                json.dump(fact_relation_to_ans, fd)\n","                print(\"dump done!\")\n","\n","        with open(self.args.FVQA.fact_relation_to_ans_path, 'r') as fd:\n","            fact_relation_to_ans = json.load(fd)\n","\n","    def preprocess_json_in_order(self):\n","        num = \"3\"\n","\n","        data_path = osp.join(self.exp_data, \"test_data\", \"test\" + num, \"all_qs_dict_release_test_500.json\")\n","        output_format = osp.join(self.exp_data, \"test_data\", \"test\" + num, \"all_qs_dict_release_test_500_inorder.json\")\n","\n","        if not osp.exists(output_format):\n","            with open(data_path, 'r') as fd:\n","                annotations = json.load(fd)\n","            keys = list(annotations.keys())\n","            tmp = 0\n","            new_annotations = {}\n","            for a in keys:\n","                new_annotations[str(tmp)] = annotations[a]\n","                tmp += 1\n","\n","            with open(output_format, 'w') as fd:\n","                json.dump(new_annotations, fd)\n","                print(\"dump done!\")\n","\n","    def disjoint_judge(self):\n","        fact_id_path = osp.join(self.args.FVQA.common_data_path, \"answer.vocab.fvqa.fact.500.json\")\n","        answer_id_path = osp.join(self.args.FVQA.common_data_path, \"answer.vocab.fvqa.500.json\")\n","        with open(fact_id_path, 'r') as fd:\n","            self.fact_id = json.load(fd)\n","            self.fact_id = self.fact_id['answer']\n","            list_fact = list(self.fact_id)\n","        with open(answer_id_path, 'r') as fd:\n","            self.answer_id = json.load(fd)\n","            self.answer_id = self.answer_id['answer']\n","            list_ans = list(self.answer_id)\n","        all = 0\n","        for i in list_ans:\n","            if i in list_fact:\n","                all += 1\n","        print(all)\n","\n","    def data_analysis(self, name):\n","        if name == \"zsl\":\n","            testpath = \"test_unseen_data\"\n","            trainpath = \"train_seen_data\"\n","        else:\n","            testpath = \"test_data\"\n","            trainpath = \"train_data\"\n","\n","        train_triplet_num = 0\n","        test_triplet_num = 0\n","        and_answer_num = 0\n","        and_entity_num = 0\n","        and_question_num = 0\n","        and_image_num = 0\n","        and_answer_class = 0\n","        and_entity_class = 0\n","        and_question_class = 0\n","        and_image_class = 0\n","        train_question_class = 0\n","        test_question_class = 0\n","        train_answer_class = 0\n","        test_answer_class = 0\n","        train_entity_class = 0\n","        test_entity_class = 0\n","        train_image_class = 0\n","        test_image_class = 0\n","\n","        for num in range(5):\n","            test_question = []\n","            train_question = []\n","            test_answer = []\n","            test_image = []\n","            train_answer = []\n","            test_entity = []\n","            train_entity = []\n","            train_image = []\n","\n","            num = str(num)\n","            datapath_test = osp.join(self.exp_data, testpath, \"test\" + num, \"all_qs_dict_release_test_500.json\")\n","            datapath_train = osp.join(self.exp_data, trainpath, \"train\" + num, \"all_qs_dict_release_train_500.json\")\n","\n","            with open(datapath_test, 'r') as fd:\n","                test_data = json.load(fd)\n","            with open(datapath_train, 'r') as fd:\n","                train_data = json.load(fd)\n","\n","            test_data_keys = list(test_data.keys())\n","            train_data_keys = list(train_data.keys())\n","\n","            for key in test_data_keys:\n","                test_question.append(test_data[key][\"question\"])\n","                test_answer.append(test_data[key][\"answer\"])\n","                test_image.append(test_data[key][\"img_file\"])\n","                e1 = test_data[key][\"fact\"][0]\n","                e2 = test_data[key][\"fact\"][2]\n","                ans = test_data[key][\"answer\"]\n","                # 和头实体相似度大于尾实体\n","                if Levenshtein.ratio(ans, e1) > Levenshtein.ratio(ans, e2):\n","                    test_entity.append(e2)\n","                else:\n","                    test_entity.append(e1)\n","\n","            for key in train_data_keys:\n","                train_question.append(train_data[key][\"question\"])\n","                train_answer.append(train_data[key][\"answer\"])\n","                train_image.append(train_data[key][\"img_file\"])\n","                e1 = train_data[key][\"fact\"][0]\n","                e2 = train_data[key][\"fact\"][2]\n","                ans = train_data[key][\"answer\"]\n","                # 和头实体相似度大于尾实体\n","                if Levenshtein.ratio(ans, e1) > Levenshtein.ratio(ans, e2):\n","                    train_entity.append(e2)\n","                else:\n","                    train_entity.append(e1)\n","\n","            # 求question/answer/entity 的数量\n","            train_triplet_num += len(train_question)\n","            test_triplet_num += len(test_question)\n","\n","            # overlap of quetsion/ans/entity\n","            q_and = [val for val in train_question if val in test_question]\n","            e_and = [val for val in train_entity if val in test_entity]\n","            a_and = [val for val in train_answer if val in test_answer]\n","            i_and = [val for val in train_image if val in test_image]\n","\n","            and_answer_num += len(a_and)\n","            and_entity_num += len(q_and)\n","            and_question_num += len(e_and)\n","            and_image_num += len(i_and)\n","\n","            and_answer_class += len(set(a_and))\n","            and_entity_class += len(set(q_and))\n","            and_question_class += len(set(e_and))\n","            and_image_class += len(set(i_and))\n","\n","            train_question_class += len(set(train_question))\n","            test_question_class += len(set(test_question))\n","            train_answer_class += len(set(train_answer))\n","            test_answer_class += len(set(test_answer))\n","            train_entity_class += len(set(train_entity))\n","            test_entity_class += len(set(test_entity))\n","            train_image_class += len(set(train_image))\n","            test_image_class += len(set(test_image))\n","\n","        train_triplet_num = train_triplet_num / 5.\n","        test_triplet_num = test_triplet_num / 5.\n","        and_answer_num = and_answer_num / 5.\n","        and_entity_num = and_entity_num / 5.\n","        and_question_num = and_question_num / 5.\n","        and_image_num = and_image_num / 5.\n","        and_answer_class = and_answer_class / 5.\n","        and_entity_class = and_entity_class / 5.\n","        and_question_class = and_question_class / 5.\n","        and_image_class = and_image_class / 5.\n","        train_question_class = train_question_class / 5.\n","        test_question_class = test_question_class / 5.\n","        train_answer_class = train_answer_class / 5.\n","        test_answer_class = test_answer_class / 5.\n","        train_entity_class = train_entity_class / 5.\n","        test_entity_class = test_entity_class / 5.\n","        train_image_class = train_image_class / 5.\n","        test_image_class = test_image_class / 5.\n","\n","        print(name + \"_train_triplet_num:\", train_triplet_num)\n","        print(name + \"_test_triplet_num:\", test_triplet_num)\n","        print(name + \"_and_answer_num:\", and_answer_num)\n","        print(name + \"_and_entity_num:\", and_entity_num)\n","        print(name + \"_and_question_num:\", and_question_num)\n","        print(name + \"_and_image_num:\", and_image_num)\n","        print(name + \"_and_answer_class:\", and_answer_class)\n","        print(name + \"_and_entity_class:\", and_entity_class)\n","        print(name + \"_and_question_class:\", and_question_class)\n","        print(name + \"_and_image_class:\", and_image_class)\n","        print(name + \"_train_question_class:\", train_question_class)\n","        print(name + \"_test_question_class:\", test_question_class)\n","        print(name + \"_train_answer_class:\", train_answer_class)\n","        print(name + \"_test_answer_class:\", test_answer_class)\n","        print(name + \"_train_entity_class:\", train_entity_class)\n","        print(name + \"_test_entity_class:\", test_entity_class)\n","        print(name + \"_train_image_class:\", train_image_class)\n","        print(name + \"_test_image_class:\", test_image_class)\n","\n","    def data_analysis_zsl_and_general(self):\n","        # self.data_analysis(\"zsl\")\n","        self.data_analysis(\"general\")\n","\n","\n","if __name__ == '__main__':\n","    cfg = cfg()\n","    args = cfg.get_args()\n","    cfg.update_train_configs(args)\n","    runner = Runner(cfg)\n","\n","    runner.get_new_json()\n","    # runner.get_new_all_json()\n","\n","    runner.get_entity_filter()\n","    runner.get_all_entity()\n","    runner.fusion_answer_and_entity()\n","\n","    # 此时得到的文件：all_qs_dict_release_combine_filter.json 是过滤好了的。\n","    # 包含三元组 5826\n","\n","    # ans_set len: 833\n","    # entity_set len: 3294\n","    # ans_or_entity len: 3294\n","    # ans_and_entity len: 833\n","    # name = \"all_qs_dict_release_combine_filter_fusion.json\"\n","\n","    # ans_set len: 500\n","    # entity_set len: 3027\n","    # ans_or_entity len: 3027\n","    # ans_and_entity len: 500\n","    # relation len: 108\n","\n","    name = \"all_qs_dict_release_combine_filter_fusion_500.json\"\n","\n","    runner.filter_top500_IQA_pair()\n","    runner.statistics_of_ans_and_entity(name=name)\n","\n","    runner.filter_top500_IQA_pair()\n","    runner.deal_relation()\n","    runner.split_data()\n","\n","    runner.preprocess_relation()\n","    runner.preprocess_fact()\n","\n","    # runner.split_unseen_data()\n","\n","    runner.get_fact_relation_matrix()\n","\n","    runner.preprocess_json_in_order()\n","\n","    # runner.data_analysis_zsl_and_general()"]},{"cell_type":"markdown","metadata":{"id":"uVonzXhy6TIb"},"source":["###### main.py\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":535,"status":"ok","timestamp":1769155911304,"user":{"displayName":"Toàn Bách Lương","userId":"04806593440338345841"},"user_tz":-420},"id":"6nIYpRQ5rc3s","outputId":"3e5c54ce-a076-4d23-87f1-a2709b6f9fc5"},"outputs":[{"name":"stdout","output_type":"stream","text":["Overwriting /content/drive/MyDrive/VQA/code/main.py\n"]}],"source":["%%writefile /content/drive/MyDrive/VQA/code/main.py\n","import os\n","import argparse\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import numpy as np\n","from torch.utils.data import DataLoader\n","from torch.utils.tensorboard import SummaryWriter\n","from tqdm import tqdm\n","import pdb\n","import torch.optim as optim\n","from torch.autograd import Variable\n","from torch.utils.data.dataloader import default_collate\n","import warnings\n","from pprint import pprint\n","\n","# self-defined\n","# import model.fusion_net as fusion_net\n","# import model.answer_net as answer_net\n","# from model import Vector, SimpleClassifier\n","# from config import cfg\n","# from torchlight import initialize_exp, set_seed, snapshot, get_dump_path, show_params\n","# from utils import unseen_mask, freeze_layer, cosine_sim, Metrics, instance_bce_with_logits\n","# from data import fvqa\n","# import copy\n","# torch.multiprocessing.set_start_method('spawn')\n","\n","warnings.filterwarnings('ignore')\n","\n","\n","class Runner:\n","    def __init__(self, args):\n","        # prepare for: data , model, loss fuction, optimizer\n","\n","        self.log_dir = get_dump_path(args)\n","        self.model_dir = os.path.join(self.log_dir, 'model')\n","\n","        self.word2vec = Vector(args.FVQA.common_data_path)\n","        # data load\n","        self.train_loader = fvqa.get_loader(args, self.word2vec, train=True)\n","        self.val_loader = fvqa.get_loader(args, self.word2vec, val=True)\n","\n","        self.avocab = default_collate(list(range(0, args.FVQA.max_ans)))\n","\n","        # question_word2vec: get the word vector (for each word in question )\n","        # the id of which could map to the vector of corresponding token\n","        self.question_word2vec = self.word2vec._prepare(self.train_loader.dataset.token_to_index)\n","\n","        # get the fusion_model and answer_net\n","        self._model_choice(args)\n","\n","        # get the mask from zsl\n","        self.negtive_mux = unseen_mask(args, self.val_loader)\n","\n","        # optimizer\n","        params_for_optimization = list(self.fusion_model.parameters()) + list(self.answer_net.parameters())\n","        self.optimizer = optim.Adam([p for p in params_for_optimization if p.requires_grad], lr=args.TRAIN.lr)\n","\n","        # loss fuction\n","        self.log_softmax = nn.LogSoftmax(dim=1).cuda()\n","\n","        # Recorder\n","        self.max_acc = [0, 0, 0, 0]\n","        self.max_zsl_acc = [0, 0, 0, 0]\n","        self.best_epoch = 0\n","        self.correspond_loss = 1e20\n","\n","        self.early_stop = 0\n","\n","        print(\"fusion_model:\")\n","        pprint(self.fusion_model)\n","        print(\"Answer Model:\")\n","        pprint(self.answer_net)\n","\n","        self.args = args\n","\n","        # test stage:\n","        if self.args.now_test:\n","            print(\"begin test! ...\")\n","            print(\"loading model  ...\")\n","            self._load_model(self.fusion_model, \"fusion\")\n","            self._load_model(self.answer_net, \"embedding\")\n","\n","    def run(self):\n","        # 1. define the parameters which are out the epoch\n","        # 2. Update statistical indicator\n","        # 3. concate of answer embedding\n","\n","        # Answer embedding :\n","        # choices belong to: ['CLS', 'W2V', 'KG', 'GAE', 'KG_W2V', 'KG_GAE', 'GAE_W2V', 'KG_GAE_W2V']\n","        # well, we recommend only use the parameter : 'CLS' or 'W2V'.\n","        # since that the resource of other choices need extra training.\n","        if args.method_choice != 'CLS':\n","            previous_var = None\n","            for method_choice in self.method_list:\n","                # get the corresponding choice embedding\n","                answer_var, answer_len = self.train_loader.dataset._get_answer_vectors(method_choice, self.avocab)\n","\n","                # normalize in row and then concate then\n","                answer_var = F.normalize(answer_var, p=2, dim=1)\n","                if previous_var is not None:\n","                    previous_var = torch.cat([previous_var, answer_var], dim=1)\n","                else:\n","                    previous_var = answer_var\n","            self.answer_var = Variable(previous_var.float()).cuda()\n","\n","        # warm up (ref: ramen)\n","        self.gradual_warmup_steps = [i * self.args.TRAIN.lr for i in torch.linspace(0.5, 2.0, 7)]\n","        self.lr_decay_epochs = range(14, 47, self.args.TRAIN.lr_decay_step)\n","\n","        # if test:\n","        if self.args.now_test:\n","            self.args.TRAIN.epochs = 2\n","\n","        for epoch in range(self.args.TRAIN.epochs):\n","\n","            self.early_stop += 1\n","            if self.args.patience < self.early_stop:\n","                # early stop\n","                break\n","            # warm up\n","            if epoch < len(self.gradual_warmup_steps):\n","                self.optimizer.param_groups[0]['lr'] = self.gradual_warmup_steps[epoch]\n","            elif epoch in self.lr_decay_epochs:\n","                self.optimizer.param_groups[0]['lr'] *= self.args.TRAIN.lr_decay_rate\n","\n","            self.train_metrics = Metrics()\n","            self.val_metrics = Metrics()\n","            self.zsl_metrics = Metrics()\n","            # use TOP50 metrics for fact mapping:\n","            if self.args.fact_map == 1:\n","                self.train_metrics = Metrics(topnum=50)\n","                self.val_metrics = Metrics(topnum=50)\n","                self.zsl_metrics = Metrics(topnum=50)\n","\n","            # train\n","            if not self.args.now_test:\n","                ######## begin training!! #######\n","                self.train(epoch)\n","                #################################\n","                lr = self.optimizer.param_groups[0]['lr']\n","                # recode:\n","                logger.info(\n","                    f'Train Epoch {epoch}: LOSS={self.train_metrics.total_loss: .5f}, lr={lr: .6f}, acc1={self.train_metrics.acc_1: .2f},acc3={self.train_metrics.acc_3: .2f},acc10={self.train_metrics.acc_10: .2f}')\n","            # eval\n","            if epoch % 1 == 0 and epoch > 0:\n","                ######## begin evaling!! #######\n","                self.eval(epoch)\n","                #################################\n","                logger.info('#################################################################################################################')\n","                logger.info(f'Test Epoch {epoch}: LOSS={self.val_metrics.total_loss: .5f}, acc1={self.val_metrics.acc_1: .2f}, acc3={self.val_metrics.acc_3: .2f}, acc10={self.val_metrics.acc_10: .2f}')\n","                if args.ZSL and not self.args.fact_map and not args.relation_map:\n","                    logger.info(f'Zsl Epoch {epoch}: LOSS={self.zsl_metrics.total_loss: .5f}, acc1={self.zsl_metrics.acc_1: .2f}, acc3={self.zsl_metrics.acc_3: .2f}, acc10={self.zsl_metrics.acc_10: .2f}')\n","                logger.info('#################################################################################################################')\n","\n","                # add 0.1 accuracy punishment, avoid for too much attention on hit@10 acc\n","                # 添加0.1的精读惩罚, 防止模型过多的关注hit@10 acc\n","                if self.val_metrics.total_loss < (self.correspond_loss - 1) or self.val_metrics.acc_all > (self.max_acc[3] + 0.2):\n","                    # reset early_stop and updata\n","                    self.early_stop = 0\n","                    self.best_epoch = epoch\n","                    self.correspond_loss = self.val_metrics.total_loss\n","                    self._updata_best_result(self.max_acc, self.val_metrics)\n","\n","                    self.best_fusion_model = copy.deepcopy(self.fusion_model)\n","                    self.best_answer_net = copy.deepcopy(self.answer_net)\n","\n","                    # ZSL result\n","                    if args.ZSL and not self.args.fact_map and not args.relation_map:\n","                        self._updata_best_result(self.max_zsl_acc, self.zsl_metrics)\n","\n","                if not args.no_tensorboard and not self.args.now_test:\n","                    writer.add_scalar('loss', self.val_metrics.total_loss, epoch)\n","                    writer.add_scalar('acc1', self.val_metrics.acc_1, epoch)\n","                    writer.add_scalar('acc3', self.val_metrics.acc_3, epoch)\n","                    writer.add_scalar('acc10', self.val_metrics.acc_10, epoch)\n","\n","        # save the model\n","        if not self.args.now_test and self.args.save_model:\n","            self.fusion_model_path = self._save_model(self.best_fusion_model, \"fusion\")\n","            self.answer_net_path = self._save_model(self.best_answer_net, \"embedding\")\n","\n","    def train(self, epoch):\n","        self.fusion_model.train()\n","        self.answer_net.train()\n","        prefix = \"train\"\n","        tq = tqdm(self.train_loader, desc='{} E{:03d}'.format(prefix, epoch), ncols=0)\n","\n","        for visual_features, boxes, question_features, answers, idx, q_len in tq:\n","            visual_features = Variable(visual_features.float()).cuda()\n","            boxes = Variable(boxes.float()).cuda()\n","            question_features = Variable(question_features).cuda()\n","            answers = Variable(answers).cuda()\n","            q_len = Variable(q_len).cuda()\n","            fusion_embedading = self.fusion_model(visual_features, boxes, question_features, q_len)\n","\n","            # Classifier-based methods\n","            if args.method_choice == 'CLS':\n","                # TODO: Normalization?\n","                predicts = self.answer_net(fusion_embedading)\n","                loss = instance_bce_with_logits(predicts, answers / 10)\n","            # Mapping-based methods\n","            else:\n","                answer_embedding = self.answer_net(self.answer_var)\n","                # notice the temperature (correspoding to specific score)\n","                predicts = cosine_sim(fusion_embedading, answer_embedding) / self.args.loss_temperature\n","                predicts = predicts.to(torch.float64)\n","                nll = -self.log_softmax(predicts).to(torch.float64)\n","                # loss = (nll * answers[0] / answers[0].sum(1, keepdim=True)).sum(dim=1).mean()\n","                loss = (nll * answers / answers.sum(1, keepdim=True)).sum(dim=1).mean()\n","\n","            self.optimizer.zero_grad()\n","            loss.backward()\n","            self.optimizer.step()\n","            self.train_metrics.update_per_batch(loss, answers.data, predicts.data)\n","        self.train_metrics.update_per_epoch()\n","\n","    def eval(self, epoch):\n","        self.fusion_model.eval()\n","        self.answer_net.eval()\n","        prefix = \"eval\"\n","        tq = tqdm(self.val_loader, desc='{} E{:03d}'.format(prefix, epoch), ncols=0)\n","\n","        for visual_features, boxes, question_features, answers, idx, q_len in tq:\n","            with torch.no_grad():\n","                visual_features = Variable(visual_features.float()).cuda()\n","                boxes = Variable(boxes.float()).cuda()\n","                question_features = Variable(question_features).cuda()\n","                answers = Variable(answers).cuda()\n","                q_len = Variable(q_len).cuda()\n","                fusion_embedading = self.fusion_model(visual_features, boxes, question_features, q_len)\n","\n","                if args.method_choice == 'CLS':\n","                    predicts = self.answer_net(fusion_embedading)\n","                    loss = instance_bce_with_logits(predicts, answers / 10)\n","\n","                else:\n","                    answer_embedding = self.answer_net(self.answer_var)\n","                    predicts = cosine_sim(fusion_embedading, answer_embedding) / self.args.loss_temperature\n","                    predicts = predicts.to(torch.float64)\n","                    nll = -self.log_softmax(predicts).to(torch.float64)\n","                    loss = (nll * answers / answers.sum(1, keepdim=True)).sum(dim=1).mean()\n","\n","                if args.ZSL == 1 and not self.args.fact_map and not args.relation_map:\n","                    # if predicts.shape[0] != self.negtive_mux.shape[0]:\n","                    #     pdb.set_trace()\n","                    zsl_predicts = predicts + self.negtive_mux[:predicts.shape[0], :]\n","\n","            self.val_metrics.update_per_batch(loss, answers.data, predicts.data)\n","            if args.ZSL == 1 and not self.args.fact_map and not args.relation_map:\n","                self.zsl_metrics.update_per_batch(loss, answers.data, zsl_predicts.data)\n","\n","        self.val_metrics.update_per_epoch()\n","        if args.ZSL == 1 and not self.args.fact_map and not args.relation_map:\n","            self.zsl_metrics.update_per_epoch()\n","\n","    def _model_choice(self, args):\n","        assert args.fusion_model in ['SAN', 'MLP', 'BAN', 'UD']\n","        # models api\n","        self.fusion_model = getattr(fusion_net, args.fusion_model)(args, self.train_loader.dataset,\n","                                                                   self.question_word2vec).cuda()\n","        # freeze word embedding\n","        if args.freeze_w2v and args.fusion_model != 'MLP':\n","            freeze_layer(self.fusion_model.w_emb)\n","\n","        # answer models\n","        assert args.method_choice in ['CLS', 'W2V', 'KG', 'GAE', 'KG_W2V', 'KG_GAE', 'GAE_W2V', 'KG_GAE_W2V']\n","        ans_len_table = {'W2V': 300, 'KG': 300, 'GAE': 1024, 'CLS': 0}\n","        self.method_list = args.method_choice.split('_')\n","        self.method_list.sort()\n","        for i in self.method_list:\n","            args.ans_feature_len += ans_len_table[i]\n","        # Mapping-based methods\n","        if args.method_choice != 'CLS':\n","            assert args.answer_embedding in ['MLP']\n","            self.answer_net = getattr(answer_net, args.answer_embedding)(args, self.train_loader.dataset).cuda()\n","        else:\n","            # Classifier-based methods\n","            self.answer_net = SimpleClassifier(args.embedding_size, 2 * args.hidden_size, args.FVQA.max_ans, 0.5).cuda()\n","\n","    def _updata_best_result(self, max_acc, metrics):\n","        max_acc[3] = metrics.acc_all\n","        max_acc[2] = metrics.acc_10\n","        max_acc[1] = metrics.acc_3\n","        max_acc[0] = metrics.acc_1\n","\n","    def _load_model(self, model, function):\n","        assert function == \"fusion\" or function == \"embedding\"\n","        # support entity mapping\n","        if self.args.fact_map:\n","            target = \"fact\"\n","        # relation mapping\n","        elif self.args.relation_map:\n","            target = \"relation\"\n","        else:\n","            target = \"answer\"\n","        model_name = type(model).__name__\n","        if not self.args.ZSL:\n","            target = \"general_\" + target\n","        save_path = os.path.join(self.args.FVQA.model_save_path, function)\n","        save_path = os.path.join(save_path, f'{target}_{model_name}_{self.args.FVQA.data_choice}.pkl')\n","\n","        model.load_state_dict(torch.load(save_path))\n","        print(f\"loading {function} model done!\")\n","\n","    def _save_model(self, model, function):\n","        assert function == \"fusion\" or function == \"embedding\"\n","        if self.args.fact_map:\n","            target = \"fact\"\n","        elif self.args.relation_map:\n","            target = \"relation\"\n","        else:\n","            target = \"answer\"\n","        model_name = type(model).__name__\n","        if not self.args.ZSL:\n","            target = \"general_\" + target\n","        save_path = os.path.join(self.args.FVQA.model_save_path, function)\n","        os.makedirs(save_path, exist_ok=True)\n","        save_path = os.path.join(save_path, f'{target}_{model_name}_{self.args.FVQA.data_choice}.pkl')\n","\n","        torch.save(model.state_dict(), save_path)\n","        return save_path\n","\n","\n","if __name__ == '__main__':\n","    # Config loading...\n","    cfg = cfg()\n","    args = cfg.get_args()\n","    cfg.update_train_configs(args)\n","    set_seed(cfg.random_seed)\n","\n","    # Environment initialization...\n","    logger = initialize_exp(cfg)\n","    logger_path = get_dump_path(cfg)\n","    if not cfg.no_tensorboard:\n","        writer = SummaryWriter(log_dir=os.path.join(logger_path, 'tensorboard'))\n","\n","    torch.cuda.set_device(cfg.gpu_id)\n","\n","    # Run...\n","    runner = Runner(cfg)\n","    runner.run()\n","\n","    #  information output:\n","    logger.info(f\"best performance = {runner.max_acc[0]: .2f},{runner.max_acc[1]: .2f},{runner.max_acc[2]: .2f}. best epoch = {runner.best_epoch}, correspond_loss={runner.correspond_loss: .4f}\")\n","    if args.ZSL == 1 and not args.fact_map and not args.relation_map:\n","        logger.info(f\" zsl performance = {runner.max_zsl_acc[0]: .2f},{runner.max_zsl_acc[1]: .2f},{runner.max_zsl_acc[2]: .2f}\")\n","    if not cfg.now_test:\n","        logger.info(f\" fusion_model_path = {runner.fusion_model_path}\")\n","        logger.info(f\" answer_net_path = {runner.answer_net_path}\")\n","    if not cfg.no_tensorboard:\n","        writer.close()"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":93,"status":"ok","timestamp":1769155911399,"user":{"displayName":"Toàn Bách Lương","userId":"04806593440338345841"},"user_tz":-420},"id":"fLb3EWlA0kEu","outputId":"ad7b0e6c-5c8e-4229-8c58-e303a25d1ea6"},"outputs":[{"name":"stdout","output_type":"stream","text":["cfgs  code  data  kg  run.ipynb\n"]}],"source":["!ls"]},{"cell_type":"markdown","metadata":{"id":"2O8VVV-YCPDZ"},"source":["###### config.py\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":111,"status":"ok","timestamp":1769609154049,"user":{"displayName":"Toàn Bách Lương","userId":"04806593440338345841"},"user_tz":-420},"id":"ESSZJbNPrxOB","outputId":"9ea291ee-76f0-4a5e-caa9-ec7f8e76bb8c"},"outputs":[{"name":"stdout","output_type":"stream","text":["Overwriting /content/drive/MyDrive/VQA/code/config.py\n"]}],"source":["%%writefile /content/drive/MyDrive/VQA/code/config.py\n","import os.path as osp\n","import numpy as np\n","import random\n","import torch\n","from easydict import EasyDict as edict\n","import argparse\n","import pdb\n","\n","\n","class cfg():\n","    def __init__(self):\n","\n","        self.fusion_model_path = \"\"\n","        self.answer_net_path = \"\"\n","\n","        self.joint_test_way = 0\n","\n","        self.this_dir = osp.dirname(__file__)\n","        self.data_root = osp.abspath(osp.join(self.this_dir, '..', '..', 'data', 'KG_VQA'))\n","        self.this_dir = osp.dirname(__file__)\n","        self.project_root = osp.abspath(osp.join(self.this_dir, '..'))\n","        self.method_choice = \"KG\"\n","        self.ans_fusion = 'RNN_concate'\n","        self.fusion_model = ''\n","        self.requires_grad = 1\n","        self.bert_dim = 1024\n","        self.KGE = \"TransE\"\n","        self.KGE_init = None  # none or w2v\n","        self.glimpse = 4\n","        self.ans_feature_len = 0\n","        self.patience = 30\n","        self.v_dim = 2048\n","\n","        self.FVQA = edict()\n","\n","        # FVQA params\n","\n","        self.FVQA.max_ans = 500\n","        self.FVQA.data_choice = \"0\"\n","\n","        self.FVQA.entity_num = \"all\"\n","        self.FVQA.data_path = osp.join(self.data_root, \"fvqa\")\n","\n","        self.FVQA.exp_data_path = osp.join(self.FVQA.data_path, \"exp_data\")\n","        self.FVQA.common_data_path = osp.join(self.FVQA.exp_data_path, \"common_data\")\n","        self.FVQA.test_data_path = osp.join(self.FVQA.exp_data_path, \"test_data\")\n","        self.FVQA.train_data_path = osp.join(self.FVQA.exp_data_path, \"train_data\")\n","        self.FVQA.seen_train_data_path = osp.join(self.FVQA.exp_data_path, \"train_seen_data\")\n","        self.FVQA.unseen_test_data_path = osp.join(self.FVQA.exp_data_path, \"test_unseen_data\")\n","        self.FVQA.seen_test_data_path = osp.join(self.FVQA.exp_data_path, \"test_seen_data\")\n","        self.FVQA.model_save_path = osp.join(self.FVQA.data_path, \"model_save\")\n","        self.FVQA.runs_path = osp.join(self.FVQA.data_path, \"model_save\")\n","\n","        self.FVQA.qa_path = self.FVQA.exp_data_path\n","        self.FVQA.feature_path = osp.join(self.FVQA.common_data_path, 'fvqa-resnet-14x14.h5')\n","        self.FVQA.answer_vocab_path = osp.join(\n","            self.FVQA.common_data_path, 'answer.vocab.fvqa.' + str(self.FVQA.max_ans) + '.json')\n","        self.FVQA.fact_vocab_path = osp.join(self.FVQA.common_data_path, 'answer.vocab.fvqa.fact.500.json')\n","        self.FVQA.relation_vocab_path = osp.join(self.FVQA.common_data_path, 'answer.vocab.fvqa.relation.500.json')\n","\n","        self.FVQA.fact_relation_to_ans_path = osp.join(self.FVQA.common_data_path, \"fact_relation_dict.data\")\n","        self.FVQA.img_path = osp.join(self.FVQA.qa_path, 'images')\n","\n","        self.FVQA.kg_path = osp.join(self.FVQA.common_data_path, \"KG_embedding\")\n","        self.FVQA.gae_path = osp.join(self.FVQA.common_data_path, \"GAE_embedding\")\n","        self.FVQA.bert_path = osp.join(self.FVQA.common_data_path, \"BERT_embedding\")\n","\n","        self.FVQA.gae_node_num = 3463\n","        self.FVQA.gae_init = \"w2v\"  # or w2v\n","        # 有问题\n","        # self.FVQA.qa = 'train2014'\n","        # self.FVQA.task = 'OpenEnded'\n","        # self.FVQA.dataset = 'mscoco'\n","\n","        # self.dataset = self.FVQA\n","\n","        self.cache_path = osp.join(self.data_root, '.cache')\n","        self.output_path = self.FVQA.model_save_path\n","        self.embedding_size = 1024  # embedding dimensionality\n","        self.hidden_size = 2 * self.embedding_size  # hidden embedding\n","        # a joint question vocab across all dataset\n","        self.question_vocab_path = osp.join(self.FVQA.common_data_path, 'question.vocab.json')  # 修改这里之后所有的预存文件（pt）都要删除\n","\n","        # preprocess config\n","        self.image_size = 448\n","        self.output_size = self.image_size // 32\n","        self.preprocess_batch_size = 100  # 64\n","        self.output_features = 2048\n","        self.central_fraction = 0.875\n","\n","        # Train params\n","        self.TRAIN = edict()\n","        self.TRAIN.epochs = 600\n","        self.TRAIN.batch_size = 128  # 128\n","        self.TRAIN.lr = 5e-4  # default Adam lr 1e-3\n","        self.TRAIN.lr_decay_step = 3\n","        self.TRAIN.lr_decay_rate = .70\n","\n","        # self.TRAIN.data_workers = 20\n","        self.TRAIN.data_workers = 8  # 10\n","        self.TRAIN.answer_batch_size = self.FVQA.max_ans  # batch size for answer network\n","        self.TRAIN.max_negative_answer = self.FVQA.max_ans  # max negative answers to sample\n","\n","        # Test params\n","        self.TEST = edict()\n","        self.TEST.batch_size = 128\n","        self.TEST.max_answer_index = self.FVQA.max_ans  # max answer index for computing acc   853\n","\n","    def get_args(self):\n","        parser = argparse.ArgumentParser()\n","        parser.add_argument('--gpu_id', default=1, type=int)\n","        parser.add_argument('--finetune', action='store_true')\n","        parser.add_argument('--batch_size', default=128, type=int)\n","        parser.add_argument('--max_ans', default=500, type=int)  # 3000 300##\n","        parser.add_argument('--loss_temperature', default=0.01, type=float)\n","        # parser.add_argument('--pretrained_model', default=None, type=str)\n","        parser.add_argument('--answer_embedding', default='MLP', choices=['RNN', 'MLP'])  # 答案编码：MLP or RNN##\n","        # parser.add_argument('--context_embedding', default='BoW', choices=['SAN', 'BoW'])  # Q I 内容编码：SAN or MLP\n","        parser.add_argument('--embedding_size', default=1024, choices=[1024, 300, 512], type=int)  # 答案编码：MLP or RNN##\n","        parser.add_argument('--epoch', default=800, type=int)  # 答案编码：MLP or RNN ##\n","        # choice model\n","        parser.add_argument('--fusion_model', default='SAN', choices=['MLP', 'SAN', 'UD', 'MUTAN', 'BAN', 'ViLBERT'])\n","        parser.add_argument('--requires_grad', default=0, type=int, choices=[0, 1])\n","        # choice class\n","        parser.add_argument('--method_choice', default='W2V',\n","                            choices=['CLS', 'W2V', 'KG', 'GAE', 'KG_W2V', 'KG_GAE', 'GAE_W2V', 'KG_GAE_W2V'])\n","        parser.add_argument('--ans_fusion', default='Simple_concate',\n","                            choices=['RNN_concate', 'GATE_attention', 'GATE', 'RNN_GATE_attention', 'Simple_concate'])\n","        # KG situation\n","        parser.add_argument('--KGE', default='TransE',\n","                            choices=['TransE', 'ComplEx', \"TransR\", \"DistMult\"])  # 答案编码：MLP or RNN ##\n","        parser.add_argument('--KGE_init', default=\"w2v\")  # None  # none or w2v ##\n","        parser.add_argument('--GAE_init', default=\"random\")  # None  # random or w2v ##\n","        parser.add_argument('--ZSL', type=int, default=0)  # None  # random or w2v ##\n","        parser.add_argument('--entity_num', default=\"all\", choices=['all', '4302'])  # todo: 完成不同子图情况的... ##\n","\n","        parser.add_argument('--data_choice', default='0', choices=['0', '1', '2', '3', '4'])\n","        parser.add_argument('--name', default=None, type=str)  # 定义名字后缀\n","\n","        parser.add_argument(\"--no-tensorboard\", default=False, action=\"store_true\")\n","        parser.add_argument(\"--exp_name\", default=\"\", type=str, required=True, help=\"Experiment name\")\n","        parser.add_argument(\"--dump_path\", default=\"dump/\", type=str, help=\"Experiment dump path\")\n","        parser.add_argument(\"--exp_id\", default=\"\", type=str, help=\"Experiment ID\")\n","        parser.add_argument(\"--random_seed\", default=4567, type=int)\n","        parser.add_argument(\"--freeze_w2v\", default=1, type=int, choices=[0, 1])\n","        parser.add_argument(\"--ans_net_lay\", default=0, type=int, choices=[0, 1, 2])\n","        parser.add_argument(\"--fact_map\", default=0, type=int, choices=[0, 1])\n","        parser.add_argument(\"--relation_map\", default=0, type=int, choices=[0, 1])\n","\n","        parser.add_argument(\"--now_test\", default=0, type=int, choices=[0, 1])\n","        parser.add_argument(\"--save_model\", default=0, type=int, choices=[0, 1])\n","\n","        parser.add_argument(\"--joint_test_way\", default=0, type=int, choices=[0, 1])\n","        parser.add_argument(\"--top_rel\", default=10, type=int)\n","        parser.add_argument(\"--top_fact\", default=100, type=int)\n","        parser.add_argument(\"--soft_score\", default=10, type=int)  # 10 or 10000\n","        parser.add_argument(\"--mrr\", default=0, type=int)\n","        args = parser.parse_args()\n","        return args\n","\n","    def update_train_configs(self, args):\n","        self.gpu_id = args.gpu_id\n","        self.finetune = args.finetune\n","        self.answer_embedding = args.answer_embedding\n","        self.name = args.name\n","        self.no_tensorboard = args.no_tensorboard\n","        self.exp_name = args.exp_name\n","        self.dump_path = args.dump_path\n","        self.exp_id = args.exp_id\n","        self.random_seed = args.random_seed\n","        self.freeze_w2v = args.freeze_w2v\n","        self.loss_temperature = args.loss_temperature\n","        self.ZSL = args.ZSL\n","        self.ans_net_lay = args.ans_net_lay\n","        self.fact_map = args.fact_map\n","        self.relation_map = args.relation_map\n","        self.now_test = args.now_test\n","        self.save_model = args.save_model\n","        self.joint_test_way = args.joint_test_way\n","        self.top_rel = args.top_rel\n","        self.top_fact = args.top_fact\n","        self.soft_score = args.soft_score\n","        self.mrr = args.mrr\n","\n","        if args.ZSL == 1:\n","            print(\"ZSL setting...\")\n","            self.FVQA.test_data_path = self.FVQA.unseen_test_data_path\n","            self.FVQA.train_data_path = self.FVQA.seen_train_data_path\n","\n","        if args.fusion_model == 'UD' or args.fusion_model == 'BAN':\n","            self.FVQA.feature_path = osp.join(self.FVQA.common_data_path, 'fvqa_36.hdf5')\n","            self.FVQA.img_id2idx = osp.join(self.FVQA.common_data_path, 'fvqa36_imgid2idx.pkl')\n","        self.requires_grad = True if args.requires_grad == 1 else False\n","        self.fusion_model = args.fusion_model\n","        self.TRAIN.batch_size = args.batch_size\n","        # self.TRAIN.answer_batch_size = args.answer_batch_size\n","        self.method_choice = args.method_choice\n","        self.ans_fusion = args.ans_fusion\n","        self.embedding_size = args.embedding_size\n","        self.FVQA.data_choice = args.data_choice\n","        self.FVQA.max_ans = args.max_ans\n","        self.TRAIN.epochs = args.epoch\n","        self.FVQA.KGE = args.KGE\n","        self.FVQA.KGE_init = args.KGE_init\n","        self.FVQA.gae_init = args.GAE_init\n","        self.FVQA.entity_num = args.entity_num\n","\n","        if self.fact_map:\n","            self.FVQA.max_ans = 2791\n","        if self.relation_map:\n","            self.FVQA.max_ans = 103\n","\n","        self.TEST.max_answer_index = self.FVQA.max_ans\n","        self.TRAIN.answer_batch_size = self.FVQA.max_ans  # batch size for answer network\n","        self.TRAIN.max_negative_answer = self.FVQA.max_ans\n","\n","        self.FVQA.answer_vocab_path = osp.join(\n","            self.FVQA.common_data_path, 'answer.vocab.fvqa.' + str(self.FVQA.max_ans) + '.json')\n","\n","        if \"KG\" in self.method_choice:\n","            self.FVQA.relation2id_path = osp.join(self.FVQA.kg_path, \"relations_\" + self.FVQA.entity_num + \".tsv\")\n","            self.FVQA.entity2id_path = osp.join(self.FVQA.kg_path, \"entities_\" + self.FVQA.entity_num + \".tsv\")\n","            if self.KGE_init != \"w2v\":\n","                self.FVQA.entity_path = osp.join(self.FVQA.kg_path, \"fvqa_\" +\n","                                                 self.FVQA.entity_num + \"_\" + self.KGE + \"_entity.npy\")\n","                self.FVQA.relation_path = osp.join(self.FVQA.kg_path, \"fvqa_\" +\n","                                                   self.FVQA.entity_num + \"_\" + self.KGE + \"_relation.npy\")\n","            else:\n","                self.FVQA.entity_path = osp.join(self.FVQA.kg_path, \"fvqa_\" +\n","                                                 self.FVQA.entity_num + \"_w2v_\" + self.KGE + \"_entity.npy\")\n","                self.FVQA.relation_path = osp.join(self.FVQA.kg_path, \"fvqa_\" +\n","                                                   self.FVQA.entity_num + \"_w2v_\" + self.KGE + \"_relation.npy\")"]},{"cell_type":"markdown","source":["## Torch Light"],"metadata":{"id":"zBEKrNZdv39p"}},{"cell_type":"markdown","source":["###### \\_\\_init__.py"],"metadata":{"id":"XEz0jAUfv_Jd"}},{"cell_type":"code","source":["%%writefile /content/drive/MyDrive/VQA/code/torchlight/__init__.py\n","from .logger import initialize_exp, get_dump_path\n","from .metric import Metric, CategoricalAccuracy, PRMetric\n","from .module import LSTM4VarLenSeq\n","from .vocab import (PAD_TOKEN, UNK_TOKEN, BOS_TOKEN, EOS_TOKEN,\n","                    DefaultLookupDict,\n","                    Vocabulary)\n","from .utils import (personal_display_settings,\n","                    set_seed,\n","                    normalize,\n","                    snapshot,\n","                    show_params,\n","                    longest_substring,\n","                    pad,\n","                    to_cuda,\n","                    get_code_version,\n","                    cat_ragged_tensors,\n","                    topk_accuracy,\n","                    get_total_trainable_params)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"zybqVmHJwNmn","executionInfo":{"status":"ok","timestamp":1769740850394,"user_tz":-420,"elapsed":48,"user":{"displayName":"Toàn Bách Lương","userId":"04806593440338345841"}},"outputId":"b28d9b74-02df-4d41-cdcd-1accd37996be"},"execution_count":18,"outputs":[{"output_type":"stream","name":"stdout","text":["Writing /content/drive/MyDrive/VQA/code/torchlight/__init__.py\n"]}]},{"cell_type":"markdown","source":["###### logger.py"],"metadata":{"id":"-8oEjSBrxpeL"}},{"cell_type":"code","source":["%%writefile /content/drive/MyDrive/VQA/code/torchlight/logger.py\n","import os\n","import re\n","import sys\n","import time\n","import json\n","import torch\n","import pickle\n","import random\n","import getpass\n","import logging\n","import argparse\n","import subprocess\n","import numpy as np\n","from datetime import timedelta, date\n","from .utils import get_code_version\n","\n","\n","class LogFormatter():\n","\n","    def __init__(self):\n","        self.start_time = time.time()\n","\n","    def format(self, record):\n","        elapsed_seconds = round(record.created - self.start_time)\n","\n","        prefix = \"%s - %s - %s\" % (\n","            record.levelname,\n","            time.strftime('%x %X'),\n","            timedelta(seconds=elapsed_seconds)\n","        )\n","        message = record.getMessage()\n","        message = message.replace('\\n', '\\n' + ' ' * (len(prefix) + 3))\n","        return \"%s - %s\" % (prefix, message) if message else ''\n","\n","\n","def create_logger(filepath, rank):\n","    \"\"\"\n","    Create a logger.\n","    Use a different log file for each process.\n","    \"\"\"\n","    # create log formatter\n","    log_formatter = LogFormatter()\n","\n","    # create file handler and set level to debug\n","    if filepath is not None:\n","        if rank > 0:\n","            filepath = '%s-%i' % (filepath, rank)\n","        file_handler = logging.FileHandler(filepath, \"a\", encoding='utf-8')\n","        file_handler.setLevel(logging.DEBUG)\n","        file_handler.setFormatter(log_formatter)\n","\n","    # create console handler and set level to info\n","    console_handler = logging.StreamHandler()\n","    console_handler.setLevel(logging.INFO)\n","    console_handler.setFormatter(log_formatter)\n","\n","    # create logger and set level to debug\n","    logger = logging.getLogger()\n","    logger.handlers = []\n","    logger.setLevel(logging.DEBUG)\n","    logger.propagate = False\n","    if filepath is not None:\n","        logger.addHandler(file_handler)\n","    logger.addHandler(console_handler)\n","\n","    # reset logger elapsed time\n","    def reset_time():\n","        log_formatter.start_time = time.time()\n","    logger.reset_time = reset_time\n","\n","    return logger\n","\n","\n","def initialize_exp(params):\n","    \"\"\"\n","    Initialize the experiment:\n","    - dump parameters\n","    - create a logger\n","    \"\"\"\n","    # dump parameters\n","    exp_folder = get_dump_path(params)\n","    json.dump(vars(params), open(os.path.join(exp_folder, 'params.pkl'), 'w'), indent=4)\n","\n","    # get running command\n","    command = [\"python\", sys.argv[0]]\n","    for x in sys.argv[1:]:\n","        if x.startswith('--'):\n","            assert '\"' not in x and \"'\" not in x\n","            command.append(x)\n","        else:\n","            assert \"'\" not in x\n","            if re.match('^[a-zA-Z0-9_]+$', x):\n","                command.append(\"%s\" % x)\n","            else:\n","                command.append(\"'%s'\" % x)\n","    command = ' '.join(command)\n","    params.command = command + ' --exp_id \"%s\"' % params.exp_id\n","\n","    # check experiment name\n","    assert len(params.exp_name.strip()) > 0\n","\n","    # create a logger\n","    logger = create_logger(os.path.join(exp_folder, 'train.log'), rank=getattr(params, 'global_rank', 0))\n","    logger.info(\"============ Initialized logger ============\")\n","    # logger.info(\"\\n\".join(\"%s: %s\" % (k, str(v))\n","    #                       for k, v in sorted(dict(vars(params)).items())))\n","    # text = f'# Git Version: {get_code_version()} #'\n","    # logger.info(\"\\n\".join(['=' * 24, text, '=' * 24]))\n","    logger.info(\"The experiment will be stored in %s\\n\" % exp_folder)\n","    logger.info(\"Running command: %s\" % command)\n","    logger.info(\"\")\n","    return logger\n","\n","\n","def get_dump_path(params):\n","    \"\"\"\n","    Create a directory to store the experiment.\n","    \"\"\"\n","    assert len(params.exp_name) > 0\n","    assert not params.dump_path in ('', None), \\\n","            'Please choose your favorite destination for dump.'\n","    dump_path = params.dump_path\n","\n","    # create the sweep path if it does not exist\n","    when = date.today().strftime('%m%d-')\n","    sweep_path = os.path.join(dump_path, when + params.exp_name)\n","    if not os.path.exists(sweep_path):\n","        subprocess.Popen(\"mkdir -p %s\" % sweep_path, shell=True).wait()\n","\n","    # create an random ID for the job if it is not given in the parameters.\n","    if params.exp_id == '':\n","        chars = 'abcdefghijklmnopqrstuvwxyz0123456789'\n","        while True:\n","            exp_id = ''.join(random.choice(chars) for _ in range(10))\n","            if not os.path.isdir(os.path.join(sweep_path, exp_id)):\n","                break\n","        params.exp_id = exp_id\n","\n","    # create the dump folder / update parameters\n","    exp_folder = os.path.join(sweep_path, params.exp_id)\n","    if not os.path.isdir(exp_folder):\n","        subprocess.Popen(\"mkdir -p %s\" % exp_folder, shell=True).wait()\n","    return exp_folder\n","\n","\n","if __name__ == '__main__':\n","    pass"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"AGWEHLhvxpsK","executionInfo":{"status":"ok","timestamp":1769740901384,"user_tz":-420,"elapsed":36,"user":{"displayName":"Toàn Bách Lương","userId":"04806593440338345841"}},"outputId":"6e755427-5381-4f0d-ac80-c1c0c75047dc"},"execution_count":19,"outputs":[{"output_type":"stream","name":"stdout","text":["Writing /content/drive/MyDrive/VQA/code/torchlight/logger.py\n"]}]},{"cell_type":"markdown","source":["###### metric.py"],"metadata":{"id":"bGd1sUpYx4pR"}},{"cell_type":"code","source":["%%writefile /content/drive/MyDrive/VQA/code/torchlight/metric.py\n","# from abc import ABC, ABCMeta, abstractclassmethod\n","import torch\n","import numpy as np\n","from abc import ABC, abstractmethod, ABCMeta\n","\n","\n","class Metric(metaclass=ABCMeta):\n","    \"\"\"\n","    Abstract Base class (ABC) for all Metrics.\n","    Taken from https://github.com/pytorch/ignite/metrics/metric.py\n","        and modify a bit.\n","    Often, data is truncated into batches. In such scenario, we call\n","    -   reset() in the begining of every epoch.\n","    -   update() after every batch\n","    -   compute() whenever you want to log the training/testing performance.\n","    \"\"\"\n","\n","    @abstractmethod\n","    def __init__(self):\n","        pass\n","\n","    @abstractmethod\n","    def reset(self):\n","        \"\"\"\n","        Resets the metric to to it's initial state.\n","        This is called at the start of each epoch.\n","        \"\"\"\n","        pass\n","\n","    @abstractmethod\n","    def update(self, output):\n","        \"\"\"\n","        Updates the metric's state using the passed batch output.\n","        This is called once for each batch.\n","        Args:\n","            output: the is the output from the engine's process function\n","        \"\"\"\n","        pass\n","\n","    @abstractmethod\n","    def compute(self):\n","        \"\"\"\n","        Computes the metric based on it's accumulated state.\n","        This is called at the end of each epoch.\n","        Returns:\n","            Any: the actual quantity of interest\n","        Raises:\n","            NotComputableError: raised when the metric cannot be computed\n","        \"\"\"\n","        pass\n","\n","\n","class CategoricalAccuracy(Metric):\n","    \"\"\"\n","    Calculates the categorical accuracy.\n","    - `update` must receive output of the form `(y_pred, y)`.\n","    - `y_pred` must be in the following shape (batch_size, num_categories, ...)\n","    - `y` must be in the following shape (batch_size, ...)\n","    \"\"\"\n","\n","    def __init__(self):\n","        super().__init__()\n","        self._num_examples = 0\n","        self._num_correct = 0\n","\n","    def reset(self):\n","        self._num_examples = 0\n","        self._num_correct = 0\n","\n","    def update(self, output):\n","        y_pred, y = output\n","        _, indices = torch.max(y_pred, 1)\n","        correct = torch.eq(indices, y).view(-1)\n","        self._num_correct += torch.sum(correct).item()\n","        self._num_examples += correct.shape[0]\n","\n","    def compute(self):\n","        if self._num_examples == 0:\n","            raise ZeroDivisionError('CategoricalAccuracy must have at least'\n","                                    ' one example before it can be computed')\n","        return self._num_correct / self._num_examples\n","\n","\n","class PRMetric(Metric):\n","    \"\"\"\n","    Calculates the precision and recall.\n","    - `update` must receive output of the form `(y_pred, y)`.\n","    - `y_pred` must be in the following shape (batch_size, num_categories, ...)\n","    - `y` must be in the following shape (batch_size, ...)\n","    \"\"\"\n","\n","    def __init__(self, num_class=2):\n","        \"\"\"\n","        precision = tp / tp + fp\n","        recall = tp / tp + fn\n","        \"\"\"\n","        super().__init__()\n","        self.num_class = num_class\n","        self.confusion_matrix = np.zeros((self.num_class, self.num_class),\n","                                         dtype=np.float32)\n","\n","    def reset(self):\n","        self.confusion_matrix = np.zeros((self.num_class, self.num_class),\n","                                         dtype=np.float32)\n","\n","    def update(self, output):\n","        y_pred, y = output\n","        _, indices = torch.max(y_pred, 1)\n","        self.confusion_matrix[indices.cpu().numpy(), y.cpu().numpy()] += 1\n","\n","    def compute(self):\n","        tp = np.diag(self.confusion_matrix)\n","        total_pred = np.sum(self.confusion_matrix, axis=1)  # (-1, 1)\n","        total_gold = np.sum(self.confusion_matrix, axis=0)  # (1, -1)\n","        # tn don't care\n","        p = tp / total_pred\n","        r = tp / total_gold\n","        return p, r\n","\n","\n","if __name__ == '__main__':\n","    # unit test\n","    pr = PRMetric(3)\n","    pr.confusion_matrix = np.array([[2, 0, 2],\n","                                    [0, 1, 0],\n","                                    [1, 0, 0]])\n","    print(pr.compute())"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Bp7CqTTux4wN","executionInfo":{"status":"ok","timestamp":1769740948255,"user_tz":-420,"elapsed":101,"user":{"displayName":"Toàn Bách Lương","userId":"04806593440338345841"}},"outputId":"19c4901f-4ae3-4ac8-cd58-f8399c03e973"},"execution_count":20,"outputs":[{"output_type":"stream","name":"stdout","text":["Writing /content/drive/MyDrive/VQA/code/torchlight/metric.py\n"]}]},{"cell_type":"markdown","source":["###### module.py"],"metadata":{"id":"h2v7ydRWyAB_"}},{"cell_type":"code","source":["%%writefile /content/drive/MyDrive/VQA/code/torchlight/module.py\n","import math\n","from typing import Sequence, Union, Callable\n","import numpy as np\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n","\n","torch.manual_seed(10086)\n","# typing, everything in Python is Object.\n","tensor_activation = Callable[[torch.Tensor], torch.Tensor]\n","\n","\n","class LSTM4VarLenSeq(nn.Module):\n","    def __init__(self, input_size, hidden_size,\n","                 num_layers=1, bias=True, bidirectional=False, init='orthogonal', take_last=True):\n","        \"\"\"\n","        no dropout support\n","        batch_first support deprecated, the input and output tensors are\n","        provided as (batch, seq_len, feature).\n","\n","        Args:\n","            input_size:\n","            hidden_size:\n","            num_layers:\n","            bias:\n","            bidirectional:\n","            init: ways to init the torch.nn.LSTM parameters,\n","                supports 'orthogonal' and 'uniform'\n","            take_last: 'True' if you only want the final hidden state\n","                otherwise 'False'\n","        \"\"\"\n","        super(LSTM4VarLenSeq, self).__init__()\n","        self.lstm = nn.LSTM(input_size=input_size,\n","                            hidden_size=hidden_size,\n","                            num_layers=num_layers,\n","                            bias=bias,\n","                            bidirectional=bidirectional)\n","        self.input_size = input_size\n","        self.hidden_size = hidden_size\n","        self.num_layers = num_layers\n","        self.bias = bias\n","        self.bidirectional = bidirectional\n","        self.init = init\n","        self.take_last = take_last\n","        self.batch_first = True  # Please don't modify this\n","\n","        self.init_parameters()\n","\n","    def init_parameters(self):\n","        \"\"\"orthogonal init yields generally good results than uniform init\"\"\"\n","        if self.init == 'orthogonal':\n","            gain = 1  # use default value\n","            for nth in range(self.num_layers * self.bidirectional):\n","                # w_ih, (4 * hidden_size x input_size)\n","                nn.init.orthogonal_(self.lstm.all_weights[nth][0], gain=gain)\n","                # w_hh, (4 * hidden_size x hidden_size)\n","                nn.init.orthogonal_(self.lstm.all_weights[nth][1], gain=gain)\n","                # b_ih, (4 * hidden_size)\n","                nn.init.zeros_(self.lstm.all_weights[nth][2])\n","                # b_hh, (4 * hidden_size)\n","                nn.init.zeros_(self.lstm.all_weights[nth][3])\n","        elif self.init == 'uniform':\n","            k = math.sqrt(1 / self.hidden_size)\n","            for nth in range(self.num_layers * self.bidirectional):\n","                nn.init.uniform_(self.lstm.all_weights[nth][0], -k, k)\n","                nn.init.uniform_(self.lstm.all_weights[nth][1], -k, k)\n","                nn.init.zeros_(self.lstm.all_weights[nth][2])\n","                nn.init.zeros_(self.lstm.all_weights[nth][3])\n","        else:\n","            raise NotImplemented('Unsupported Initialization')\n","\n","    def forward(self, x, x_len, hx=None):\n","        # 1. Sort x and its corresponding length\n","        sorted_x_len, sorted_x_idx = torch.sort(x_len, descending=True)\n","        sorted_x = x[sorted_x_idx]\n","        # 2. Ready to unsort after LSTM forward pass\n","        # Note that PyTorch 0.4 has no argsort, but PyTorch 1.0 does.\n","        _, unsort_x_idx = torch.sort(sorted_x_idx, descending=False)\n","\n","        # 3. Pack the sorted version of x and x_len, as required by the API.\n","        x_emb = pack_padded_sequence(sorted_x, sorted_x_len,\n","                                     batch_first=self.batch_first)\n","\n","        # 4. Forward lstm\n","        # output_packed.data.shape is (valid_seq, num_directions * hidden_dim).\n","        # See doc of torch.nn.LSTM for details.\n","        out_packed, (hn, cn) = self.lstm(x_emb)\n","\n","        # 5. unsort h\n","        # (num_layers * num_directions, batch, hidden_size) -> (batch, ...)\n","        hn = hn.permute(1, 0, 2)[unsort_x_idx]  # swap the first two dim\n","        hn = hn.permute(1, 0, 2)  # swap the first two again to recover\n","        if self.take_last:\n","            return hn.squeeze(0)\n","        else:\n","            # unpack: out\n","            # (batch, max_seq_len, num_directions * hidden_size)\n","            out, _ = pad_packed_sequence(out_packed,\n","                                         batch_first=self.batch_first)\n","            out = out[unsort_x_idx]\n","            # unpack: c\n","            # (num_layers * num_directions, batch, hidden_size) -> (batch, ...)\n","            cn = cn.permute(1, 0, 2)[unsort_x_idx]  # swap the first two dim\n","            cn = cn.permute(1, 0, 2)  # swap the first two again to recover\n","            return out, (hn, cn)\n","\n","\n","if __name__ == '__main__':\n","    # Note that in the future we will import unittest\n","    # and port the following examples to test folder.\n","\n","    # Unit test for LSTM variable length sequences\n","    # ================\n","    net = LSTM4VarLenSeq(200, 100,\n","                         num_layers=3, bias=True, bidirectional=True, init='orthogonal', take_last=False)\n","\n","    inputs = torch.tensor([[1, 2, 3, 0],\n","                           [2, 3, 0, 0],\n","                           [2, 4, 3, 0],\n","                           [1, 4, 3, 0],\n","                           [1, 2, 3, 4]])\n","    embedding = nn.Embedding(num_embeddings=5, embedding_dim=200, padding_idx=0)\n","    lens = torch.LongTensor([3, 2, 3, 3, 4])\n","\n","    input_embed = embedding(inputs)\n","    output, (h, c) = net(input_embed, lens)\n","    # 5, 4, 200, batch, seq length, hidden_size * 2 (only last layer)\n","    print(output.shape)\n","    # 6, 5, 100, num_layers * num_directions, batch, hidden_size\n","    print(h.shape)\n","    # 6, 5, 100, num_layers * num_directions, batch, hidden_size\n","    print(c.shape)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"fD7_ZAqdyAV4","executionInfo":{"status":"ok","timestamp":1769740998716,"user_tz":-420,"elapsed":25,"user":{"displayName":"Toàn Bách Lương","userId":"04806593440338345841"}},"outputId":"63805ccb-40c2-41cf-c129-6b1bc812ddcf"},"execution_count":21,"outputs":[{"output_type":"stream","name":"stdout","text":["Writing /content/drive/MyDrive/VQA/code/torchlight/module.py\n"]}]},{"cell_type":"markdown","source":["###### utils.py"],"metadata":{"id":"aQC-BR4cwGEu"}},{"cell_type":"code","source":["%%writefile /content/drive/MyDrive/VQA/code/torchlight/utils.py\n","\"\"\"\n","Utilizations for common usages.\n","\"\"\"\n","import os\n","import random\n","import torch\n","import numpy as np\n","from difflib import SequenceMatcher\n","from unidecode import unidecode\n","from datetime import datetime\n","from torch.nn.parallel import DataParallel, DistributedDataParallel\n","\n","\n","def personal_display_settings():\n","    \"\"\"\n","    Pandas Doc\n","    https://pandas.pydata.org/pandas-docs/stable/generated/pandas.set_option.html\n","    NumPy Doc\n","        -\n","    \"\"\"\n","    from pandas import set_option\n","    set_option('display.max_rows', 500)\n","    set_option('display.max_columns', 500)\n","    set_option('display.width', 2000)\n","    set_option('display.max_colwidth', 1000)\n","    from numpy import set_printoptions\n","    set_printoptions(suppress=True)\n","\n","\n","def set_seed(seed):\n","    \"\"\"\n","    Freeze every seed for reproducibility.\n","    torch.cuda.manual_seed_all is useful when using random generation on GPUs.\n","    e.g. torch.cuda.FloatTensor(100).uniform_()\n","    \"\"\"\n","    random.seed(seed)\n","    np.random.seed(seed)\n","    torch.manual_seed(seed)\n","    torch.cuda.manual_seed_all(seed)\n","\n","\n","def normalize(s):\n","    \"\"\"\n","    German and Frence have different vowels than English.\n","    This utilization removes all the non-unicode characters.\n","    Example:\n","        āáǎà  -->  aaaa\n","        ōóǒò  -->  oooo\n","        ēéěè  -->  eeee\n","        īíǐì  -->  iiii\n","        ūúǔù  -->  uuuu\n","        ǖǘǚǜ  -->  uuuu\n","\n","    :param s: unicode string\n","    :return:  unicode string with regular English characters.\n","    \"\"\"\n","    s = s.strip().lower()\n","    s = unidecode(s)\n","    return s\n","\n","\n","def snapshot(model, epoch, save_path):\n","    \"\"\"\n","    Saving models w/ its params.\n","        Get rid of the ONNX Protocal.\n","    F-string feature new in Python 3.6+ is used.\n","    \"\"\"\n","    os.makedirs(save_path, exist_ok=True)\n","    # timestamp = datetime.now().strftime('%m%d_%H%M')\n","    save_path = os.path.join(save_path, f'{type(model).__name__}_{epoch}_epoch.pkl')\n","    if isinstance(model, (DataParallel, DistributedDataParallel)):\n","        torch.save(model.module.state_dict(), save_path)\n","    else:\n","        torch.save(model.state_dict(), save_path)\n","    return save_path\n","\n","\n","def save_checkpoint(model, optimizer, epoch, path):\n","    torch.save({\n","        'epoch': epoch,\n","        'models': model.state_dict(),\n","        'optimizer': optimizer.state_dict(),\n","    }, path)\n","\n","\n","def load_checkpoint(path, map_location):\n","    checkpoint = torch.load(path, map_location=map_location)\n","    return checkpoint\n","\n","\n","def show_params(model):\n","    \"\"\"\n","    Show models parameters for logging.\n","    \"\"\"\n","    for name, param in model.named_parameters():\n","        print('%-16s' % name, param.size())\n","\n","\n","def longest_substring(str1, str2):\n","    # initialize SequenceMatcher object with input string\n","    seqMatch = SequenceMatcher(None, str1, str2)\n","\n","    # find match of longest sub-string\n","    # output will be like Match(a=0, b=0, size=5)\n","    match = seqMatch.find_longest_match(0, len(str1), 0, len(str2))\n","\n","    # print longest substring\n","    return str1[match.a: match.a + match.size] if match.size != 0 else \"\"\n","\n","\n","def pad(sent, max_len):\n","    \"\"\"\n","    syntax \"[0] * int\" only works properly for Python 3.5+\n","    Note that in testing time, the length of a sentence\n","    might exceed the pre-defined max_len (of training data).\n","    \"\"\"\n","    length = len(sent)\n","    return (sent + [0] * (max_len - length))[:max_len] if length < max_len else sent[:max_len]\n","\n","\n","def to_cuda(*args, device=None):\n","    \"\"\"\n","    Move Tensors to CUDA.\n","    If no device provided, default to the first card in CUDA_VISIBLE_DEVICES.\n","    \"\"\"\n","    assert all(torch.is_tensor(t) for t in args), \\\n","        'Only support for tensors, please check if any nn.Module exists.'\n","    if device is None:\n","        device = torch.device('cuda:0')\n","    return [None if x is None else x.to(device) for x in args]\n","\n","\n","def get_code_version(short_sha=True):\n","    from subprocess import check_output, STDOUT, CalledProcessError\n","    try:\n","        sha = check_output('git rev-parse HEAD', stderr=STDOUT,\n","                           shell=True, encoding='utf-8')\n","        if short_sha:\n","            sha = sha[:7]\n","        return sha\n","    except CalledProcessError:\n","        # There was an error - command exited with non-zero code\n","        pwd = check_output('pwd', stderr=STDOUT, shell=True, encoding='utf-8')\n","        pwd = os.path.abspath(pwd).strip()\n","        print(f'Working dir {pwd} is not a git repo.')\n","\n","\n","def cat_ragged_tensors(left, right):\n","    assert left.size(0) == right.size(0)\n","    batch_size = left.size(0)\n","    max_len = left.size(1) + right.size(1)\n","\n","    len_left = (left != 0).sum(dim=1)\n","    len_right = (right != 0).sum(dim=1)\n","\n","    left_seq = left.unbind()\n","    right_seq = right.unbind()\n","    # handle zero padding\n","    output = torch.zeros((batch_size, max_len), dtype=torch.long, device=left.device)\n","    for i, row_left, row_right, l1, l2 in zip(range(batch_size),\n","                                              left_seq, right_seq,\n","                                              len_left, len_right):\n","        l1 = l1.item()\n","        l2 = l2.item()\n","        j = l1 + l2\n","        # concatenate rows of ragged tensors\n","        row_cat = torch.cat((row_left[:l1], row_right[:l2]))\n","        # copy to empty tensor\n","        output[i, :j] = row_cat\n","    return output\n","\n","\n","def topk_accuracy(inputs, labels, k=1, largest=True):\n","    assert len(inputs.size()) == 2\n","    assert len(labels.size()) == 2\n","    _, indices = inputs.topk(k=k, largest=largest)\n","    result = indices - labels  # boardcast\n","    nonzero_count = (result != 0).sum(dim=1, keepdim=True)\n","    num_correct = (nonzero_count != result.size(1)).sum().item()\n","    num_example = inputs.size(0)\n","    return num_correct, num_example\n","\n","\n","def get_total_trainable_params(model):\n","    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n","\n","\n","if __name__ == '__main__':\n","    print(normalize('ǖǘǚǜ'))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"n5juloEUyKMw","executionInfo":{"status":"ok","timestamp":1769741028858,"user_tz":-420,"elapsed":79,"user":{"displayName":"Toàn Bách Lương","userId":"04806593440338345841"}},"outputId":"eb843712-0ad7-4803-f3e6-9917bc133d94"},"execution_count":22,"outputs":[{"output_type":"stream","name":"stdout","text":["Writing /content/drive/MyDrive/VQA/code/torchlight/utils.py\n"]}]},{"cell_type":"markdown","source":["###### vocab.py"],"metadata":{"id":"mmWlpH3OyKDj"}},{"cell_type":"code","source":["%%writefile /content/drive/MyDrive/VQA/code/torchlight/vocab.py\n","# coding: utf-8\n","\"\"\"\n","Every NLP task needs a Vocabulary\n","Every Vocabulary is built from Instances\n","Every Instance is a collection of Fields\n","\"\"\"\n","\n","__all__ = ['DefaultLookupDict', 'Vocabulary']\n","\n","PAD_TOKEN = '<pad>'\n","UNK_TOKEN = '<unk>'\n","BOS_TOKEN = '<bos>'\n","EOS_TOKEN = '<eos>'\n","PAD_IDX = 0\n","UNK_IDX = 1\n","\n","\n","class DefaultLookupDict(dict):\n","    def __init__(self, default):\n","        super(DefaultLookupDict, self).__init__()\n","        self._default = default\n","\n","    def __getitem__(self, item):\n","        return self.get(item, self._default)\n","\n","\n","class Vocabulary:\n","    \"\"\"\n","    Define a vocabulary object that will be used to numericalize a field.\n","    Attributes:\n","        token2id: A collections.defaultdict instance mapping token strings to\n","            numerical identifiers.\n","        id2token: A list of token strings indexed by their numerical\n","        identifiers.\n","        embedding: pretrained vectors.\n","\n","    Examples:\n","    >>> from torchlight.vocab import Vocabulary\n","    >>> from collections import Counter\n","    >>> text_data = ['hello', 'world', 'hello', 'nice', 'world', 'hi', 'world']\n","    >>> vocab = Vocabulary(Counter(text_data))\n","    \"\"\"\n","    def __init__(self, counter, max_size=None, min_freq=1, specials=None):\n","        \"\"\"\n","        Create a Vocabulary given Counter.\n","        Args:\n","            counter: collections.Counter object holding the frequencies of\n","                each value found in the data.\n","            max_size: The maximum size of the vocabulary, or None for no\n","                maximum. Default: None.\n","            min_freq: The minimum frequency needed to include a token in the\n","                vocabulary. Values less than 1 will be set to 1. Default: 1.\n","            specials: The list of special tokens except ['<pad>', '<unk>'].\n","                Possible choices: [CLS] [MASK] [SEP] in BERT or <bos> <eos>\n","                in Machine Translation.\n","        \"\"\"\n","        min_freq = max(min_freq, 1)  # must be positive\n","\n","        if specials is None:\n","            self.specials = [PAD_TOKEN, UNK_TOKEN]\n","        else:\n","            assert isinstance(specials, list), \"'specials' is of type list\"\n","            self.specials = [PAD_TOKEN, UNK_TOKEN] + specials\n","\n","        assert len(set(self.specials)) == len(self.specials), \\\n","            \"specials can not contain duplicates.\"\n","\n","        if max_size is not None:\n","            max_size = len(self.specials) + max_size\n","\n","        self.id2token = self.specials[:]\n","        self.token2id = DefaultLookupDict(UNK_IDX)\n","        self.token2id.update({tok: i for i, tok in enumerate(self.id2token)})\n","\n","        # sort by frequency, then alphabetically\n","        token_freqs = sorted(counter.items(), key=lambda tup: tup[0])\n","        token_freqs.sort(key=lambda tup: tup[1], reverse=True)\n","\n","        for token, freq in token_freqs:\n","            if freq < min_freq or len(self.id2token) == max_size:\n","                break\n","            if token not in self.specials:\n","                self.id2token.append(token)\n","                self.token2id[token] = len(self.id2token) - 1\n","\n","        # TODO\n","        self.embedding = None\n","\n","    def __len__(self):\n","        return len(self.id2token)\n","\n","    def __repr__(self):\n","        return 'Vocab(size={}, specials=\"{}\")'.format(len(self), self.specials)\n","\n","    def __getitem__(self, tokens):\n","        \"\"\"Looks up indices of text tokens according to the vocabulary.\n","        If `unknown_token` of the vocabulary is None, looking up unknown tokens\n","        results in KeyError.\n","        Parameters\n","        ----------\n","        tokens : str or list of strs\n","            A source token or tokens to be converted.\n","        Returns\n","        -------\n","        int or list of ints\n","            A token index or a list of token indices according to the vocabulary.\n","        \"\"\"\n","\n","        if not isinstance(tokens, (list, tuple)):\n","            return self.token2id[tokens]\n","        else:\n","            return [self.token2id[token] for token in tokens]\n","\n","    def __call__(self, tokens):\n","        \"\"\"Looks up indices of text tokens according to the vocabulary.\n","        Parameters\n","        ----------\n","        tokens : str or list of strs\n","            A source token or tokens to be converted.\n","        Returns\n","        -------\n","        int or list of ints\n","            A token index or a list of token indices according to the\n","            vocabulary.\n","        \"\"\"\n","\n","        return self[tokens]\n","\n","    @classmethod\n","    def from_json(cls, json_str):\n","        pass\n","\n","    def to_json(self):\n","        pass\n","\n","    def set_embedding(self):\n","        pass"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"cbf82JbnylEj","executionInfo":{"status":"ok","timestamp":1769741149350,"user_tz":-420,"elapsed":29,"user":{"displayName":"Toàn Bách Lương","userId":"04806593440338345841"}},"outputId":"66463242-761b-484f-9a2b-b54364aee52c"},"execution_count":23,"outputs":[{"output_type":"stream","name":"stdout","text":["Writing /content/drive/MyDrive/VQA/code/torchlight/vocab.py\n"]}]},{"cell_type":"markdown","source":["## Utils"],"metadata":{"id":"sIFngerFu5Sk"}},{"cell_type":"markdown","source":["###### \\_\\_init__.py"],"metadata":{"id":"ZhnNu0-ovsAm"}},{"cell_type":"code","source":["%%writefile /content/drive/MyDrive/VQA/code/utils/__init__.py\n","from .tool import unseen_mask, freeze_layer, cosine_sim, batch_accuracy, instance_bce_with_logits, dele_a, transfer, hand_remove, deal_fact\n","from .metrics import Metrics"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"RPrKQ7C2vsPg","executionInfo":{"status":"ok","timestamp":1769740392040,"user_tz":-420,"elapsed":308,"user":{"displayName":"Toàn Bách Lương","userId":"04806593440338345841"}},"outputId":"8d131cd7-96dc-4f6b-e334-7d78e50a7a99"},"execution_count":16,"outputs":[{"output_type":"stream","name":"stdout","text":["Overwriting /content/drive/MyDrive/VQA/code/utils/__init__.py\n"]}]},{"cell_type":"markdown","source":["###### metrics.py"],"metadata":{"id":"2VoMJd7QvHla"}},{"cell_type":"code","source":["%%writefile /content/drive/MyDrive/VQA/code/utils/metrics.py\n","import os\n","import json\n","\n","import torch\n","import torch.nn as nn\n","import time\n","import pdb\n","\n","\n","class Metrics:\n","    \"\"\"\n","    Stores accuracy (score), loss and timing info\n","    \"\"\"\n","\n","    def __init__(self, topnum=10):\n","\n","        self.topnum = topnum\n","        self.total_loss = 0\n","\n","        self.correct_1 = 0\n","        self.correct_3 = 0\n","        self.correct_10 = 0\n","        self.acc_all = 0\n","        self.acc_1 = 0\n","        self.acc_3 = 0\n","        self.acc_10 = 0\n","        self.num_examples = 0\n","        self.num_epoch = 0\n","\n","        self.mrr = 0\n","        self.mr = 0\n","        self.mrr_all = 0\n","        self.mr_all = 0\n","\n","    def update_per_batch(self, loss, answers, pred):\n","        self.total_loss += loss\n","        self.num_epoch += 1\n","        if self.topnum == 10:\n","            top1, top3, top10 = self.batch_accuracy_10(pred, answers)\n","        elif self.topnum == 50:\n","            top1, top3, top10 = self.batch_accuracy_50(pred, answers)\n","        elif self.topnum == 200:\n","            top1, top3, top10 = self.batch_accuracy_200(pred, answers)\n","        self.num_examples += top1.shape[0]\n","\n","        self.correct_1 += top1.sum().item()\n","        self.correct_3 += top3.sum().item()\n","        self.correct_10 += top10.sum().item()\n","\n","        #\n","        mrr_tmp, mr_tmp = self.batch_mr_mrr(pred, answers)\n","        self.mrr_all += mrr_tmp.sum().item()\n","        self.mr_all += mr_tmp.sum().item()\n","\n","    def update_per_epoch(self):\n","        self.acc_1 = 100 * (self.correct_1 / self.num_examples)\n","        self.acc_3 = 100 * (self.correct_3 / self.num_examples)\n","        self.acc_10 = 100 * (self.correct_10 / self.num_examples)\n","\n","        self.mr = self.mr_all / self.num_examples\n","        self.mrr = self.mrr_all / self.num_examples\n","\n","        self.total_loss = self.total_loss / self.num_epoch\n","        self.acc_all = self.acc_1 + self.acc_3 + self.acc_10\n","\n","    def batch_accuracy_10(self, predicted, true):\n","        \"\"\" Compute the accuracies for a batch of predictions and answers \"\"\"\n","        # (Pdb) predicted.shape\n","        # torch.Size([128, 500])\n","        # (Pdb) true.shape\n","        # torch.Size([128, 500])\n","        if len(true.shape) == 3:\n","            true = true[0]\n","        _, ok = predicted.topk(10, dim=1)\n","        agreeing_all = torch.zeros([predicted.shape[0], 1], dtype=torch.float).cuda()\n","        for i in range(10):\n","            tmp = ok[:, i].reshape(-1, 1)\n","            agreeing_all += true.gather(dim=1, index=tmp)\n","            if i == 0:\n","                top1 = (agreeing_all * 0.3).clamp(max=1)\n","            if i == 2:\n","                top3 = (agreeing_all * 0.3).clamp(max=1)\n","            if i == 9:\n","                top10 = (agreeing_all * 0.3).clamp(max=1)\n","        return top1, top3, top10\n","\n","    def batch_accuracy_50(self, predicted, true):\n","        \"\"\" Compute the accuracies for a batch of predictions and answers \"\"\"\n","        if len(true.shape) == 3:\n","            true = true[0]\n","        _, ok = predicted.topk(50, dim=1)\n","        agreeing_all = torch.zeros([predicted.shape[0], 1], dtype=torch.float).cuda()\n","        for i in range(50):\n","            tmp = ok[:, i].reshape(-1, 1)\n","            agreeing_all += true.gather(dim=1, index=tmp)\n","            if i == 9:\n","                top10 = (agreeing_all * 0.3).clamp(max=1)\n","            if i == 29:\n","                top30 = (agreeing_all * 0.3).clamp(max=1)\n","            if i == 49:\n","                top50 = (agreeing_all * 0.3).clamp(max=1)\n","\n","        return top10, top30, top50\n","\n","    def batch_accuracy_200(self, predicted, true):\n","        \"\"\" Compute the accuracies for a batch of predictions and answers \"\"\"\n","        if len(true.shape) == 3:\n","            true = true[0]\n","        _, ok = predicted.topk(200, dim=1)\n","        agreeing_all = torch.zeros([predicted.shape[0], 1], dtype=torch.float).cuda()\n","        for i in range(200):\n","            tmp = ok[:, i].reshape(-1, 1)\n","            agreeing_all += true.gather(dim=1, index=tmp)\n","            if i == 79:\n","                top10 = (agreeing_all * 0.3).clamp(max=1)\n","            if i == 149:\n","                top30 = (agreeing_all * 0.3).clamp(max=1)\n","            if i == 199:\n","                top50 = (agreeing_all * 0.3).clamp(max=1)\n","\n","        return top10, top30, top50\n","\n","    def batch_mr_mrr(self, predicted, true):\n","        if len(true.shape) == 3:\n","            true = true[0]\n","\n","        # 计算\n","        top_rank = predicted.shape[1]\n","        batch_size = predicted.shape[0]\n","        _, predict_ans_rank = predicted.topk(top_rank, dim=1)  # 答案排名的坐标 batchsize * 500\n","        _, real_ans = true.topk(1, dim=1)  # 真正的答案：batchsize * 1\n","\n","        # 扩充维度\n","        real_ans = real_ans.expand(batch_size, top_rank)\n","        ans_different = torch.abs(predict_ans_rank - real_ans)\n","        # 此时为0的位置就是预测正确的位置\n","        _, real_ans_list = ans_different.topk(top_rank, dim=1)  # 此时最后一位的数值就是正确答案在预测答案里面的位置,为 0\n","        real_ans_list = real_ans_list + 1.0\n","        mr = real_ans_list[:, -1].reshape(-1, 1).to(torch.float64)\n","        mrr = 1.0 / mr\n","        # pdb.set_trace()\n","\n","        return mrr, mr\n","\n","    # def print(self, epoch):\n","    #     print(\"Epoch {} Score {:.2f} Loss {}\".format(epoch, 100 * self.raw_score / self.num_examples,\n","    #                                                  self.loss / self.num_examples))\n","\n","\n","# def accumulate_metrics(epoch, train_metrics, val_metrics, val_per_type_metric,\n","#                        best_val_score,\n","#                        best_val_epoch, save_val_metrics=True):\n","#     stats = {\n","#         \"epoch\": epoch,\n","\n","#         \"train_loss\": float(train_metrics.loss),\n","#         \"train_raw_score\": float(train_metrics.raw_score),\n","#         \"train_normalized_score\": float(train_metrics.normalized_score),\n","#         \"train_upper_bound\": float(train_metrics.upper_bound),\n","#         \"train_score\": float(train_metrics.score),\n","#         \"train_num_examples\": train_metrics.num_examples,\n","\n","#         \"train_time\": train_metrics.end_time - train_metrics.start_time,\n","#         \"val_time\": val_metrics.end_time - val_metrics.start_time\n","#     }\n","#     if save_val_metrics:\n","#         stats[\"val_raw_score\"] = float(val_metrics.raw_score)\n","#         stats[\"val_normalized_score\"] = float(val_metrics.normalized_score)\n","#         stats[\"val_upper_bound\"] = float(val_metrics.upper_bound)\n","#         stats[\"val_loss\"] = float(val_metrics.loss)\n","#         stats[\"val_score\"] = float(val_metrics.score)\n","#         stats[\"val_num_examples\"] = val_metrics.num_examples\n","#         stats[\"val_per_type_metric\"] = val_per_type_metric.get_json()\n","\n","#         stats[\"best_val_score\"] = float(best_val_score)\n","#         stats[\"best_epoch\"] = best_val_epoch\n","\n","#     print(json.dumps(stats, indent=4))\n","#     return stats"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ECc2z65JvHuC","executionInfo":{"status":"ok","timestamp":1769740326927,"user_tz":-420,"elapsed":56,"user":{"displayName":"Toàn Bách Lương","userId":"04806593440338345841"}},"outputId":"bb8cf001-e147-4705-a4b8-363454f33bcc"},"execution_count":14,"outputs":[{"output_type":"stream","name":"stdout","text":["Overwriting /content/drive/MyDrive/VQA/code/utils/metrics.py\n"]}]},{"cell_type":"markdown","source":["###### tool.py"],"metadata":{"id":"KZ7elNaHvbZP"}},{"cell_type":"code","source":["%%writefile /content/drive/MyDrive/VQA/code/utils/tool.py\n","import os\n","import json\n","\n","import torch\n","import torch.nn as nn\n","import torchvision.transforms as transforms\n","from nltk import word_tokenize, pos_tag\n","from nltk.corpus import wordnet\n","from nltk.stem import WordNetLemmatizer\n","import pdb\n","\n","\n","def instance_bce_with_logits(logits, labels):\n","    assert logits.dim() == 2\n","\n","    loss = nn.functional.binary_cross_entropy_with_logits(logits, labels)\n","    loss *= labels.size(1)\n","    return loss\n","\n","\n","def freeze_layer(layer):\n","    for param in layer.parameters():\n","        param.requires_grad = False\n","\n","\n","def unseen_mask(args, val_loader):\n","    negtive_mux = None\n","    # zsl\n","    if args.ZSL == 1:\n","        negtive_mux = torch.ones(args.TEST.batch_size, args.FVQA.max_ans)\n","        indices = val_loader.dataset.answer_indices\n","        all_ans = set(aid for aids in indices for aid in aids)\n","\n","        # unseen 类置0\n","        for i in all_ans:\n","            for j in range(args.TRAIN.batch_size):\n","                negtive_mux[j, i] = 0\n","        negtive_mux = negtive_mux * (-1e13)\n","        negtive_mux = negtive_mux.cuda()\n","        # pdb.set_trace()\n","    return negtive_mux\n","\n","\n","def cosine_sim(im, s):\n","    return im.mm(s.t())\n","\n","\n","def batch_mc_acc(predicted):\n","    \"\"\" Compute the accuracies for a batch of predictions and answers \"\"\"\n","    N, C = predicted.squeeze().size()\n","    _, predicted_index = predicted.max(dim=1, keepdim=True)\n","    return (predicted_index == C - 1).float()\n","\n","\n","def batch_top1(predicted, true):\n","    \"\"\" Compute the accuracies for a batch of predictions and answers \"\"\"\n","    _, predicted_index = predicted.max(dim=1, keepdim=True)\n","    return true.gather(dim=1, index=predicted_index).clamp(max=1)\n","\n","\n","def batch_accuracy(predicted, true):\n","    \"\"\" Compute the accuracies for a batch of predictions and answers \"\"\"\n","    # import pdb\n","    # pdb.set_trace()\n","    # _, predicted_index = predicted.max(dim=1, keepdim=True)\n","    # agreeing = true[0].gather(dim=1, index=predicted_index)\n","    # return (agreeing * 0.3).clamp(max=1)\n","    if len(true.shape) == 3:\n","        true = true[0]\n","    _, ok = predicted.topk(10, dim=1)\n","    agreeing_all = torch.zeros([predicted.shape[0], 1], dtype=torch.float).cuda()\n","    for i in range(10):\n","        tmp = ok[:, i].reshape(-1, 1)\n","        agreeing_all += true.gather(dim=1, index=tmp)\n","        if i == 0:\n","            top1 = (agreeing_all * 0.3).clamp(max=1)\n","        if i == 2:\n","            top3 = (agreeing_all * 0.3).clamp(max=1)\n","        if i == 9:\n","            top10 = (agreeing_all * 0.3).clamp(max=1)\n","\n","    top1 = top1.sum().item() / top1.shape[0]\n","    top3 = top3.sum().item() / top3.shape[0]\n","    top10 = top10.sum().item() / top10.shape[0]\n","    return top1, top3, top10\n","\n","\n","# def update_learning_rate(optimizer, epoch):\n","#     learning_rate = cfg.TRAIN.base_lr * 0.5 ** (float(epoch) / cfg.TRAIN.lr_decay)\n","#     for param_group in optimizer.param_groups:\n","#         param_group['lr'] = learning_rate\n","#\n","#     return learning_rate\n","\n","\n","class Tracker:\n","    \"\"\" Keep track of results over time, while having access to monitors to display information about them. \"\"\"\n","\n","    def __init__(self):\n","        self.data = {}\n","\n","    def track(self, name, *monitors):\n","        \"\"\" Track a set of results with given monitors under some name (e.g. 'val_acc').\n","            When appending to the returned list storage, use the monitors to retrieve useful information.\n","        \"\"\"\n","        l = Tracker.ListStorage(monitors)\n","        self.data.setdefault(name, []).append(l)\n","        return l\n","\n","    def to_dict(self):\n","        # turn list storages into regular lists\n","        return {k: list(map(list, v)) for k, v in self.data.items()}\n","\n","    class ListStorage:\n","        \"\"\" Storage of data points that updates the given monitors \"\"\"\n","\n","        def __init__(self, monitors=[]):\n","            self.data = []\n","            self.monitors = monitors\n","            for monitor in self.monitors:\n","                setattr(self, monitor.name, monitor)\n","\n","        def append(self, item):\n","            for monitor in self.monitors:\n","                monitor.update(item)\n","            self.data.append(item)\n","\n","        def __iter__(self):\n","            return iter(self.data)\n","\n","    class MeanMonitor:\n","        \"\"\" Take the mean over the given values \"\"\"\n","        name = 'mean'\n","\n","        def __init__(self):\n","            self.n = 0\n","            self.total = 0\n","\n","        def update(self, value):\n","            self.total += value\n","            self.n += 1\n","\n","        @property\n","        def value(self):\n","            return self.total / self.n\n","\n","    class MovingMeanMonitor:\n","        \"\"\" Take an exponentially moving mean over the given values \"\"\"\n","        name = 'mean'\n","\n","        def __init__(self, momentum=0.9):\n","            self.momentum = momentum\n","            self.first = True\n","            self.value = None\n","\n","        def update(self, value):\n","            if self.first:\n","                self.value = value\n","                self.first = False\n","            else:\n","                m = self.momentum\n","                self.value = m * self.value + (1 - m) * value\n","\n","\n","class data_prefetcher():\n","    def __init__(self, loader):\n","        self.loader = iter(loader)\n","        self.stream = torch.cuda.Stream()\n","        self.preload()\n","\n","    def preload(self):\n","        try:\n","            self.next_features, self.next_targets, _ = next(self.loader)\n","        except StopIteration:\n","            self.next_features = None\n","            self.next_targets = None\n","            return\n","        # self.next_features_gpu = []\n","        # self.next_targets_gpu = {}\n","        # for xaf in self.next_features:\n","        #     self.next_features_gpu.append(torch.empty_like(xaf, device='cuda'))\n","        # for key in self.next_targets.keys():\n","        #     self.next_targets_gpu[key] = torch.empty_like(self.next_targets[key], device='cuda')\n","        # self.stream.wait_stream(torch.cuda.current_stream())\n","        with torch.cuda.stream(self.stream):\n","            self.next_features = [single_feature.cuda(non_blocking=True) for single_feature in self.next_features]\n","            if isinstance(self.next_targets, dict):\n","                for key in self.next_targets.keys():\n","                    self.next_targets[key] = self.next_targets[key].cuda(non_blocking=True)\n","            else:\n","                self.next_targets = [single_target.cuda(non_blocking=True) for single_target in self.next_targets]\n","            # for index in range(len(self.next_features_gpu)):\n","            #     self.next_features_gpu[index].copy_(self.next_features[index], non_blocking=True)\n","            # for key in self.next_targets_gpu.keys():\n","            #     self.next_targets_gpu[key].copy_(self.next_targets[key], non_blocking=True)\n","\n","    def next(self):\n","        torch.cuda.current_stream().wait_stream(self.stream)\n","        # features = self.next_features_gpu\n","        # targets = self.next_targets_gpu\n","        features = self.next_features\n","        targets = self.next_targets\n","        if features is not None:\n","            features = [xaf.record_stream(torch.cuda.current_stream()) for xaf in features]\n","        if targets is not None:\n","            targets = [targets[xaf].record_stream(torch.cuda.current_stream()) for xaf in targets.keys()]\n","        self.preload()\n","        return features, targets\n","\n","\n","def get_transform(target_size, central_fraction=1.0):\n","    return transforms.Compose([\n","        transforms.Scale(int(target_size / central_fraction)),\n","        transforms.CenterCrop(target_size),\n","        transforms.ToTensor(),\n","        transforms.Normalize(mean=[0.485, 0.456, 0.406],\n","                             std=[0.229, 0.224, 0.225]),\n","    ])\n","\n","\n","def dele_a(answer):  # 去冠词\n","    answer_t = answer.replace('.', '')\n","    answer_tt = answer_t.replace(',', '')\n","    answer_ttt = answer_tt.replace(\"the \", \"\")\n","    answer_tttt = answer_ttt.replace(\"an \", \"\")\n","    answer_ttttt = answer_tttt.replace(\"a \", \"\")\n","    ans_list = [answer_t, answer_tt, answer_ttt, answer_tttt, answer_ttttt]\n","\n","    return list(set(ans_list))\n","\n","\n","def transfer(answer):  # 单复数转换\n","    tokens = word_tokenize(answer)\n","    tagged_sent = pos_tag(tokens)\n","    wnl = WordNetLemmatizer()\n","\n","    new = []\n","    for tag in tagged_sent:\n","        if tag[0] == \"as\":\n","            new.append(\"as\")\n","            continue\n","        elif tag[0] == \"grazing\" or tag[0] == \"timing\" or tag[0] == \"bicycling\":\n","            kk = tag[0].replace(\"ing\", \"\") + \"e\"\n","            new.append(kk)\n","            continue\n","        wordnet_pos = get_wordnet_pos(tag[1]) or wordnet.NOUN\n","        tmp = wnl.lemmatize(tag[0], pos=wordnet_pos)\n","        if tmp == \"ax\":\n","            tmp = \"axe\"\n","        elif tmp == \"people\":\n","            tmp = \"person\"\n","        elif tmp == \"teeth\":\n","            tmp = \"tooth\"\n","        elif tmp == \"worn\":\n","            tmp = \"wear\"\n","        new.append(tmp)  # 词形还原\n","    string = ' '\n","    key = string.join(new)\n","    return key\n","\n","\n","def hand_remove(answer):  # 手动去ing，s，es\n","    _ing = answer.replace(\"ing\", \"\")\n","    __ing = answer.replace(\"ing \", \" \")\n","    _s = answer.replace(\"s\", \"\")\n","    __s = answer.replace(\"s \", \" \")\n","    _es = answer.replace(\"es\", \"\")\n","    __es = answer.replace(\"es \", \" \")\n","    _er = answer.replace(\"er\", \"\")\n","    __er = answer.replace(\"er \", \" \")\n","    return list(set([_ing, _s, _es, _er, __ing, __s, __es, __er]))\n","\n","\n","def get_wordnet_pos(tag):\n","    if tag.startswith('J'):\n","        return wordnet.ADJ\n","    elif tag.startswith('V'):\n","        return wordnet.VERB\n","    elif tag.startswith('N'):\n","        return wordnet.NOUN\n","    elif tag.startswith('R'):\n","        return wordnet.ADV\n","    else:\n","        return None\n","\n","\n","def deal_fact(dic, fact):\n","    fact = fact.split('/')\n","    if fact[-1] == \"n\" or fact[-1] == \"v\":\n","        ans = fact[-2]\n","    else:\n","        ans = fact[-1]\n","\n","    ans = ans.split(':')\n","    if ans[0] == \"Category\":\n","        ans = ans[1]\n","    else:\n","        ans = ans[0]\n","\n","    # if ans[-1] == \")\":\n","    #     # ans = ans.split(\"(\")[0]\n","    #     pdb.set_trace()\n","    #     ans = dic[\"answer\"]\n","    return ans"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"zSIahxIuvbm5","executionInfo":{"status":"ok","timestamp":1769740341817,"user_tz":-420,"elapsed":234,"user":{"displayName":"Toàn Bách Lương","userId":"04806593440338345841"}},"outputId":"69d0acaa-5374-4259-914f-7e59710366e0"},"execution_count":15,"outputs":[{"output_type":"stream","name":"stdout","text":["Overwriting /content/drive/MyDrive/VQA/code/utils/tool.py\n"]}]},{"cell_type":"markdown","metadata":{"id":"VAKf9TiufCng"},"source":["## Bash Script"]},{"cell_type":"markdown","source":["###### run_FVQA_train.sh"],"metadata":{"id":"BcvCLDN8uVMe"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"QFkMVVBQiv_D"},"outputs":[],"source":["%%writefile /content/drive/MyDrive/VQA/code/bash_script/run_FVQA_train.sh\n","data=3;\n","python code/main.py --gpu_id 1 --exp_name knowledge_space --exp_id W2V --fusion_model SAN --data_choice 3 --method_choice W2V --save_model 1\n","python code/main.py --gpu_id 1 --exp_name semantic_space --exp_id W2V --fusion_model SAN --data_choice 3 --method_choice W2V  --save_model 1 --relation_map 1\n","python code/main.py --gpu_id 1 --exp_name object_space --exp_id W2V --fusion_model SAN --data_choice 3 --method_choice W2V  --save_model 1 --fact_map 1"]},{"cell_type":"markdown","source":["###### run_FVQA.sh"],"metadata":{"id":"b3QRdlY0uX4p"}},{"cell_type":"code","source":["%%writefile /content/drive/MyDrive/VQA/code/bash_script/run_FVQA.sh\n","data=3;\n","ke=10;\n","kr=3;\n","score=10;\n","zsl=0;\n","python joint_test.py --gpu_id 1 --exp_name fusion_prediction --ZSL \"${zsl}\" --exp_id rel\"${kr}\"_fact\"${ke}\"data_\"${data}\"score_\"${score}\" --data_choice \"${data}\" --top_rel \"${kr}\" --top_fact \"${ke}\" --soft_score \"${score}\"  --mrr 1"],"metadata":{"id":"xITCEEa5uX_5"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# RUN"],"metadata":{"id":"0SigKqFOtzi4"}},{"cell_type":"code","source":["!python code/main.py --gpu_id 1 --exp_name knowledge_space --exp_id W2V --fusion_model SAN --data_choice 3 --method_choice W2V --save_model 1"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"9u16ysXEtzQe","executionInfo":{"status":"ok","timestamp":1769739950898,"user_tz":-420,"elapsed":42695,"user":{"displayName":"Toàn Bách Lương","userId":"04806593440338345841"}},"outputId":"114ef3aa-a11c-4e57-c19e-6a78b86d0182"},"execution_count":12,"outputs":[{"output_type":"stream","name":"stdout","text":["2026-01-30 02:26:02.366887: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n","2026-01-30 02:26:02.953390: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n","2026-01-30 02:26:03.255784: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n","WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n","E0000 00:00:1769739963.659626    5312 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n","E0000 00:00:1769739963.748395    5312 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","W0000 00:00:1769739963.785135    5312 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n","W0000 00:00:1769739963.785227    5312 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n","W0000 00:00:1769739963.785233    5312 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n","W0000 00:00:1769739963.785238    5312 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n","2026-01-30 02:26:03.792557: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n","To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n","Traceback (most recent call last):\n","  File \"/content/drive/MyDrive/VQA/code/main.py\", line 22, in <module>\n","    from torchlight import initialize_exp, set_seed, snapshot, get_dump_path, show_params\n","ModuleNotFoundError: No module named 'torchlight'\n"]}]},{"cell_type":"markdown","metadata":{"id":"oxUz2vaK435k"},"source":["# GITHUB\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Sh4aNzjl5qeY"},"outputs":[],"source":["# %%bash\n","# TOKEN=$(cat /content/token.txt)\n","# export MY_ENV_VAR_NAME_1=\"$TOKEN\"\n","# echo \"Environment variable MY_ENV_VAR_NAME set to: $MY_ENV_VAR_NAME_1\""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"g7DI8mXwmhu6"},"outputs":[],"source":["# !TOKEN=$(cat /content/token.txt)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_Is0lZGBmkbX"},"outputs":[],"source":["# !export TOKEN=\"$TOKEN\""]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":117,"status":"ok","timestamp":1769603782248,"user":{"displayName":"Toàn Bách Lương","userId":"04806593440338345841"},"user_tz":-420},"id":"98xDfONbmo0n","outputId":"91b3eea7-eb44-4ed8-a5f3-fa6f7412089a"},"outputs":[{"name":"stdout","output_type":"stream","text":["Environment variable MY_ENV_VAR_NAME set to: \n"]}],"source":["# !echo \"Environment variable MY_ENV_VAR_NAME set to: $TOKEN\""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"EYuX-D_YbJXS"},"outputs":[],"source":["# !more /content/token.txt | gh auth login --with-token"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"GSq8A1sXka1p"},"outputs":[],"source":["# !value=$(cat config.txt)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":57,"status":"ok","timestamp":1769602165702,"user":{"displayName":"Toàn Bách Lương","userId":"04806593440338345841"},"user_tz":-420},"id":"wRZF6EVfZcoN","outputId":"a197364f-1614-4376-e7fd-cc3a2b833ae1"},"outputs":[{"name":"stdout","output_type":"stream","text":["Overwriting /content/drive/MyDrive/VQA/.gitignore\n"]}],"source":["%%writefile /content/drive/MyDrive/VQA/.gitignore\n","kg/\n","data/\n","run.ipynb"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Jgr9Yk0yhXjP"},"outputs":[],"source":["!git config --global --unset user.name"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"11_z8xCtfgNl"},"outputs":[],"source":["!git config user.name \"LTBach\""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"mmD0VX8ohbOH"},"outputs":[],"source":["!git config --global --unset user.email"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"FfHCGRscfbel"},"outputs":[],"source":["!git config user.email \"bach1346790852@gmail.com\""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"lYqrrzeNZ5N2"},"outputs":[],"source":["!git remote add origin https://<USERNAME>:<PASSWORD>@github.com/<USERNAME>/reponame.git"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"dNviGX0BavF_"},"outputs":[],"source":["!git add ."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":113,"status":"ok","timestamp":1769602174771,"user":{"displayName":"Toàn Bách Lương","userId":"04806593440338345841"},"user_tz":-420},"id":"U3MqyRHbgZLq","outputId":"1892af1b-2a78-4cc5-f663-1e62bc2794d9"},"outputs":[{"name":"stdout","output_type":"stream","text":["Nothing specified, nothing added.\n","\u001b[33mhint: Maybe you wanted to say 'git add .'?\u001b[m\n","\u001b[33mhint: Turn this message off by running\u001b[m\n","\u001b[33mhint: \"git config advice.addEmptyPathspec false\"\u001b[m\n"]}],"source":["!git stage"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":412,"status":"ok","timestamp":1769602177817,"user":{"displayName":"Toàn Bách Lương","userId":"04806593440338345841"},"user_tz":-420},"id":"fvxYd1j6aw2b","outputId":"0d3a2d80-a4bc-4193-f3d0-f9f7292eed2d"},"outputs":[{"name":"stdout","output_type":"stream","text":["[main 120de67] Add code\n"," 2 files changed, 2 insertions(+), 1 deletion(-)\n"]}],"source":["!git commit -m \"Add code\""]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1226,"status":"ok","timestamp":1769608457027,"user":{"displayName":"Toàn Bách Lương","userId":"04806593440338345841"},"user_tz":-420},"id":"vfPUQFMYamYf","outputId":"0980db2e-9ec1-4586-b894-421a84e82b3c"},"outputs":[{"name":"stdout","output_type":"stream","text":["Enumerating objects: 25, done.\n","Counting objects:   4% (1/25)\rCounting objects:   8% (2/25)\rCounting objects:  12% (3/25)\rCounting objects:  16% (4/25)\rCounting objects:  20% (5/25)\rCounting objects:  24% (6/25)\rCounting objects:  28% (7/25)\rCounting objects:  32% (8/25)\rCounting objects:  36% (9/25)\rCounting objects:  40% (10/25)\rCounting objects:  44% (11/25)\rCounting objects:  48% (12/25)\rCounting objects:  52% (13/25)\rCounting objects:  56% (14/25)\rCounting objects:  60% (15/25)\rCounting objects:  64% (16/25)\rCounting objects:  68% (17/25)\rCounting objects:  72% (18/25)\rCounting objects:  76% (19/25)\rCounting objects:  80% (20/25)\rCounting objects:  84% (21/25)\rCounting objects:  88% (22/25)\rCounting objects:  92% (23/25)\rCounting objects:  96% (24/25)\rCounting objects: 100% (25/25)\rCounting objects: 100% (25/25), done.\n","Delta compression using up to 2 threads\n","Compressing objects: 100% (21/21), done.\n","Writing objects: 100% (25/25), 39.52 KiB | 554.00 KiB/s, done.\n","Total 25 (delta 4), reused 0 (delta 0), pack-reused 0\n","remote: Resolving deltas: 100% (4/4), done.\u001b[K\n","To https://github.com/LTBach/VQA.git\n"," * [new branch]      main -> main\n","Branch 'main' set up to track remote branch 'main' from 'origin'.\n"]}],"source":["!git push -u origin main"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_khFN3N6ij39"},"outputs":[],"source":[]}],"metadata":{"colab":{"toc_visible":true,"provenance":[],"collapsed_sections":["zXA_eWngv7qu","P5lU-IQs5nax","EtoksY5d762f","_HaU9nXR9OYf","H5BEEa3k8sks","D5Fcmalh8ja-","Ac4KWctO86tU","wXzgiSGB5UK1","Y1cuCeNi_h_S","XmZjuTqqKqoa","CJ1vexQVEKHd","qPnG22YpEGFQ","pqTx0OfLEN9U","90sOllcu8X-Z","uksHBN-UsNbI","pSFyIrdRCFts","mhHcAwnFB3hV","bs1YQHcXB0JV","7gYdxg8zDDTq","S-Qxepl-8kPv","UDIFGBnkeYG8","uVonzXhy6TIb","2O8VVV-YCPDZ","zBEKrNZdv39p","XEz0jAUfv_Jd","-8oEjSBrxpeL","bGd1sUpYx4pR","h2v7ydRWyAB_","aQC-BR4cwGEu","mmWlpH3OyKDj","ZhnNu0-ovsAm","2VoMJd7QvHla","KZ7elNaHvbZP"],"authorship_tag":"ABX9TyNDLSsGFT58JH5C2jfmgBq6"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}