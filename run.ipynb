{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyP+xg31o5km9EnBVkLkVLPP"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# DRIVE\n","\n"],"metadata":{"id":"dwhbfOv64SrM"}},{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"urXAm-PNdN3f","executionInfo":{"status":"ok","timestamp":1769600118876,"user_tz":-420,"elapsed":19512,"user":{"displayName":"Toàn Bách Lương","userId":"04806593440338345841"}},"outputId":"1c3021dc-789b-4f0c-f7c8-148579fdf5fb"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","source":["%cd /content/drive/MyDrive/VQA"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ppM8Zh6wra9w","executionInfo":{"status":"ok","timestamp":1769600119264,"user_tz":-420,"elapsed":386,"user":{"displayName":"Toàn Bách Lương","userId":"04806593440338345841"}},"outputId":"fbcdd044-35cc-4aed-c738-003df4d88e5c"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["/content/drive/MyDrive/VQA\n"]}]},{"cell_type":"markdown","source":["# DATA"],"metadata":{"id":"uQSzD0a8CUJh"}},{"cell_type":"markdown","source":["## AOKVQA"],"metadata":{"id":"zXA_eWngv7qu"}},{"cell_type":"code","source":["AOKVQA_DIR=\"/content/drive/MyDrive/VQA/data/aokvqa/\""],"metadata":{"id":"Vpx5rdLdvn2g"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!mkdir -p $AOKVQA_DIR"],"metadata":{"id":"itiphTO0ySRH"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!curl -fsSL https://prior-datasets.s3.us-east-2.amazonaws.com/aokvqa/aokvqa_v1p0.tar.gz | tar xvz -C $AOKVQA_DIR"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"up8YkUZ1xlWq","executionInfo":{"status":"ok","timestamp":1769155909355,"user_tz":-420,"elapsed":1619,"user":{"displayName":"Toàn Bách Lương","userId":"04806593440338345841"}},"outputId":"28442855-085c-4e68-ed97-6304323f725d"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["aokvqa_v1p0_train.json\n","aokvqa_v1p0_val.json\n","aokvqa_v1p0_test.json\n","large_vocab_train.csv\n","specialized_vocab_train.csv\n"]}]},{"cell_type":"code","source":["!ls /content/drive/MyDrive/VQA/data/aokvqa/"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"l3Kzu8BG03kr","executionInfo":{"status":"ok","timestamp":1769155909459,"user_tz":-420,"elapsed":102,"user":{"displayName":"Toàn Bách Lương","userId":"04806593440338345841"}},"outputId":"7f53e89c-3b2f-43b9-c4fe-f6ad43ee0a09"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["aokvqa_v1p0_test.json\taokvqa_v1p0_val.json   specialized_vocab_train.csv\n","aokvqa_v1p0_train.json\tlarge_vocab_train.csv\n"]}]},{"cell_type":"markdown","source":["## F-VQA"],"metadata":{"id":"xkAzH_h1y0KS"}},{"cell_type":"markdown","source":["# CODE\n"],"metadata":{"id":"CGnA7V104WlU"}},{"cell_type":"markdown","source":["## Cmd\n"],"metadata":{"id":"P5lU-IQs5nax"}},{"cell_type":"code","source":["!ls"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"8m_OOZTjsEmd","executionInfo":{"status":"ok","timestamp":1769155909578,"user_tz":-420,"elapsed":117,"user":{"displayName":"Toàn Bách Lương","userId":"04806593440338345841"}},"outputId":"a592ca74-8ed6-4f6d-9ecd-4b3170f49e33"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["cfgs  code  data  kg  run.ipynb\n"]}]},{"cell_type":"code","source":["!mkdir -p /content/drive/MyDrive/VQA/cfgs"],"metadata":{"id":"PzM0Of1su-hT"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!mkdir -p /content/drive/MyDrive/VQA/data"],"metadata":{"id":"cl1U19MHvVz5"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!mkdir -p /content/drive/MyDrive/VQA/code"],"metadata":{"id":"Z8Ufwvhbr-Li"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!mkdir -p /content/drive/MyDrive/VQA/code/utils"],"metadata":{"id":"dpf-wMNuuFev"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!mkdir -p /content/drive/MyDrive/VQA/code/model"],"metadata":{"id":"G9dV9vehuROa"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!mkdir -p /content/drive/MyDrive/VQA/code/model/fusion_net"],"metadata":{"id":"ZN5-BfZe9Jpb"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!mkdir -p /content/drive/MyDrive/VQA/code/model/answer_net"],"metadata":{"id":"7pmqsoY1-r00"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!mkdir -p /content/drive/MyDrive/VQA/code/data"],"metadata":{"id":"8O6Bmc1Z8JN2"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Data\n"],"metadata":{"id":"EtoksY5d762f"}},{"cell_type":"markdown","source":["###### preprocess.py"],"metadata":{"id":"_HaU9nXR9OYf"}},{"cell_type":"code","source":["%%writefile /content/drive/MyDrive/VQA/code/data/preprocess.py\n","import os\n","import os.path as osp\n","import re\n","import random\n","import itertools\n","import h5py\n","import torch\n","import torch.utils.data as data\n","import pdb\n","from torch.utils.data.dataloader import default_collate\n","from collections import Counter\n","from PIL import Image\n","# this is used for normalizing questions\n","_special_chars = re.compile('[^a-z0-9 ]*')\n","\n","# these try to emulate the original normalization scheme for answers\n","_period_strip = re.compile(r'(?!<=\\d)(\\.)(?!\\d)')\n","_comma_strip = re.compile(r'(\\d)(,)(\\d)')\n","_punctuation_chars = re.escape(r';/[]\"{}()=+\\_-><@`,?!')\n","_punctuation = re.compile(r'([{}])'.format(re.escape(_punctuation_chars)))\n","_punctuation_with_a_space = re.compile(r'(?<= )([{0}])|([{0}])(?= )'.format(_punctuation_chars))\n","\n","\n","def invert_dict(d): return {v: k for k, v in d.items()}\n","\n","\n","def process_punctuation(s):\n","    # the original is somewhat broken, so things that look odd here might just be to mimic that behaviour\n","    # this version should be faster since we use re instead of repeated operations on str's\n","    original_s = s\n","    if _punctuation.search(s) is None:\n","        return s\n","    s = _punctuation_with_a_space.sub('', s)\n","    if re.search(_comma_strip, s) is not None:\n","        s = s.replace(',', '')\n","    s = _punctuation.sub(' ', s)\n","    s = _period_strip.sub('', s)\n","    if s.strip() == '':\n","        return original_s.strip()\n","    else:\n","        return s.strip()\n","\n","\n","def extract_vocab(iterable, top_k=None, start=0, input_vocab=None):\n","    \"\"\" Turns an iterable of list of tokens into a vocabulary.\n","        These tokens could be single answers or word tokens in questions.\n","    \"\"\"\n","    all_tokens = itertools.chain.from_iterable(iterable)\n","    counter = Counter(all_tokens)\n","    if top_k:\n","        most_common = counter.most_common(top_k)\n","        most_common = (t for t, c in most_common)\n","    else:\n","        most_common = counter.keys()\n","    # descending in count, then lexicographical order\n","    tokens = sorted(most_common, key=lambda x: (counter[x], x), reverse=True)\n","\n","    vocab = {t: i for i, t in enumerate(tokens, start=start)}\n","    return vocab\n","\n","\n","class CocoImages(data.Dataset):\n","    def __init__(self, path, transform=None):\n","        super(CocoImages, self).__init__()\n","        self.path = path\n","        self.id_to_filename = self._find_images()\n","        self.sorted_ids = sorted(self.id_to_filename.keys())  # used for deterministic iteration order\n","        print('found {} images in {}'.format(len(self), self.path))\n","        self.transform = transform\n","\n","    def _find_images(self):\n","        id_to_filename = {}\n","        for filename in os.listdir(self.path):\n","            if not filename.endswith('.jpg'):\n","                continue\n","            id_and_extension = filename.split('_')[-1]\n","            id = int(id_and_extension.split('.')[0])\n","            id_to_filename[id] = filename\n","        return id_to_filename\n","\n","    def __getitem__(self, item):\n","        id = self.sorted_ids[item]\n","        path = os.path.join(self.path, self.id_to_filename[id])\n","        img = Image.open(path).convert('RGB')\n","\n","        if self.transform is not None:\n","            img = self.transform(img)\n","        return id, img\n","\n","    def __len__(self):\n","        return len(self.sorted_ids)\n","\n","\n","class Composite(data.Dataset):\n","    \"\"\" Dataset that is a composite of several Dataset objects. Useful for combining splits of a dataset. \"\"\"\n","\n","    def __init__(self, *datasets):\n","        self.datasets = datasets\n","\n","    def __getitem__(self, item):\n","        current = self.datasets[0]\n","        for d in self.datasets:\n","            if item < len(d):\n","                return d[item]\n","            item -= len(d)\n","        else:\n","            raise IndexError('Index too large for composite dataset')\n","\n","    def __len__(self):\n","        return sum(map(len, self.datasets))\n","\n","    def _get_answer_vectors(self, answer_indices):\n","        return self.datasets[0]._get_answer_vectors(answer_indices)\n","\n","    def _get_answer_sequences(self, answer_indices):\n","        return self.datasets[0]._get_answer_sequences(answer_indices)\n","\n","    @property\n","    def vector(self):\n","        return self.datasets[0].vector\n","\n","    @property\n","    def token_to_index(self):\n","        return self.datasets[0].token_to_index\n","\n","    @property\n","    def answer_to_index(self):\n","        return self.datasets[0].answer_to_index\n","\n","    @property\n","    def index_to_answer(self):\n","        return self.datasets[0].index_to_answer\n","\n","    @property\n","    def num_tokens(self):\n","        return self.datasets[0].num_tokens\n","\n","    @property\n","    def num_answer_tokens(self):\n","        return self.datasets[0].num_answer_tokens\n","\n","    @property\n","    def vocab(self):\n","        return self.datasets[0].vocab\n","\n","\n","def eval_collate_fn(batch):\n","    # put question lengths in descending order so that we can use packed sequences later\n","    batch.sort(key=lambda x: x[-1], reverse=True)\n","    return data.dataloader.default_collate(batch)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"-13__Ap29NxS","executionInfo":{"status":"ok","timestamp":1769156729158,"user_tz":-420,"elapsed":56,"user":{"displayName":"Toàn Bách Lương","userId":"04806593440338345841"}},"outputId":"e45d5c06-820b-4242-a759-a38f73ae5b0f"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Writing /content/drive/MyDrive/VQA/code/data/preprocess.py\n"]}]},{"cell_type":"markdown","source":["###### base.py"],"metadata":{"id":"H5BEEa3k8sks"}},{"cell_type":"code","source":["%%writefile /content/drive/MyDrive/VQA/code/data/base.py\n","import json\n","import os\n","import os.path as osp\n","import nltk\n","import h5py\n","import torch\n","import torch.utils.data as data\n","import pdb\n","from nltk import word_tokenize, pos_tag\n","import re\n","import numpy as np\n","import sys\n","import pickle as pkl\n","\n","################\n","from .preprocess import invert_dict\n","\n","\n","class VisualQA(data.Dataset):\n","    def __init__(self,\n","                 args,\n","                 vector):\n","        super(VisualQA, self).__init__()\n","\n","        # vocab\n","        self.vector = vector\n","        self.args = args\n","        # process question\n","        # self.args.question_vocab_path = osp.join(project_root, 'data', 'question.vocab.json') # a joint question vocab across all dataset\n","        with open(self.args.question_vocab_path, 'r') as fd:\n","            question_vocab = json.load(fd)\n","        self.token_to_index = question_vocab['question']\n","        self._max_question_length = question_vocab['max_question_length']\n","        self.image_features_path = args.FVQA.feature_path\n","        self.index_to_token = invert_dict(self.token_to_index)\n","\n","        answer_vocab_path = self.args.FVQA.answer_vocab_path\n","        fact_vocab_path = self.args.FVQA.fact_vocab_path\n","        relation_vocab_path = self.args.FVQA.relation_vocab_path\n","\n","        if self.args.fact_map:\n","            with open(fact_vocab_path, 'r') as fd:\n","                answer_vocab = json.load(fd)\n","        elif self.args.relation_map:\n","            with open(relation_vocab_path, 'r') as fd:\n","                answer_vocab = json.load(fd)\n","        else:\n","            with open(answer_vocab_path, 'r') as fd:\n","                answer_vocab = json.load(fd)\n","        self.answer_to_index = answer_vocab['answer']\n","        self.index_to_answer = invert_dict(self.answer_to_index)\n","\n","        self.cached_answers_g2v = {}  # 只编码KGE\n","        self.cached_answers_w2v = {}  # 只编码序列\n","        self.cached_answers_gae = {}\n","        self.cached_answers_bert = {}\n","        self.unk_vector = self.vector['UNK']\n","        if \"KG\" in self.args.method_choice:\n","            self._map_kg()\n","        if \"GAE\" in self.args.method_choice:\n","            # self._map_gae()\n","            self._map_bert()\n","\n","    @property\n","    def max_question_length(self):\n","        return self._max_question_length\n","\n","    @property\n","    def max_answer_length(self):\n","        assert hasattr(self, answers), 'Dataloader must have access to answers'\n","        if not hasattr(self, '_max_answer_length'):\n","            self._max_answer_length = max(map(len, self.answers))\n","        return self._max_answer_length\n","\n","    @property\n","    def num_tokens(self):\n","        return len(self.token_to_index)\n","\n","    @property\n","    def num_answers(self):\n","        return len(self.answer_to_index)\n","\n","    def __len__(self):\n","        return len(self.questions)\n","\n","    # Internal data utility---------------------------------------\n","\n","    def _load_image(self, image_id):\n","        \"\"\" Load an image \"\"\"\n","        # pdb.set_trace()\n","        index = self.image_id_to_index[image_id]\n","        spa = torch.zeros([1, 1])  # init\n","\n","        if self.args.fusion_model == 'UD' or self.args.fusion_model == 'BAN':\n","            spatials = self.features_file['spatial_features']\n","            dataset = self.features_file['image_features']  # 直接读取特征文件\n","            spa = spatials[index].astype('float32')\n","            spa = torch.from_numpy(spa)\n","        else:\n","            dataset = self.features_file['features']  # 直接读取特征文件\n","\n","        img = dataset[index].astype('float32')\n","\n","        return torch.from_numpy(img), spa\n","\n","    def _create_image_id_to_index(self):\n","        \"\"\" Create a mapping from a COCO image id into the corresponding index into the h5 file \"\"\"\n","        if not hasattr(self, 'features_file'):\n","            # Loading the h5 file has to be done here and not in __init__ because when the DataLoader\n","            # forks for multiple works, every child would use the same file object and fail\n","            # Having multiple readers using different file objects is fine though, so we just init in here.\n","            self.features_file = h5py.File(self.image_features_path, 'r')\n","\n","        if self.args.fusion_model == 'UD' or self.args.fusion_model == 'BAN':\n","            import _pickle as cPickle\n","            image_id_to_index = cPickle.load(open(self.args.FVQA.img_id2idx, \"rb\"))\n","            # pdb.set_trace()\n","            self.s_dim = self.features_file['spatial_features'].shape[2]\n","            self.v_dim = self.features_file['image_features'].shape[2]  # 直接读取特征文件\n","\n","        else:\n","            with h5py.File(self.image_features_path, 'r') as features_file:\n","                image_ids = features_file['ids'][()]\n","            image_id_to_index = {id: i for i, id in enumerate(image_ids)}\n","        return image_id_to_index\n","\n","    def _encode_question(self, question):\n","        \"\"\" Turn a question into a vector of indices and a question length \"\"\"\n","        vec = torch.zeros(self.max_question_length).long()\n","        for i, token in enumerate(question):\n","            index = self.token_to_index.get(token, 0)\n","            vec[i] = index\n","        return vec, len(question)\n","\n","    def _map_kg(self):\n","        if \"KG\" not in self.args.method_choice:\n","            return\n","        # print(\"using kg embedding\")\n","        kg_path = self.args.FVQA.kg_path\n","        entity_path = self.args.FVQA.entity_path  # 来源中的词对应的向量\n","        relation_path = self.args.FVQA.relation_path  # 同上\n","        relation2id_path = self.args.FVQA.relation2id_path  # 搜寻候选答案的来源\n","        entity2id_path = self.args.FVQA.entity2id_path  # 搜寻候选答案的来源\n","\n","        a = np.load(entity_path)\n","        b = np.load(relation_path)\n","        self.map_kg = np.vstack((a, b))\n","\n","        # 随机得到一个矩阵，以模拟随机的情况\n","        # self.map_ran=torch.zeros(self.map_kg.shape)\n","        # self.map_ran = torch.rand(self.map_kg.shape)\n","        # self.map_ran = torch.randn(self.map_kg.shape)\n","        # self.map_kg = self.map_ran\n","\n","        self.map_kg = torch.Tensor(self.map_kg).view(-1, 300)\n","\n","        self.stoi_kg = {}\n","        with open(os.path.join(entity2id_path), \"r\") as f:\n","            while 1:\n","                line = f.readline()\n","                if not line:\n","                    break\n","                line = re.split('\\t|\\n', line)[:2]\n","                self.stoi_kg[line[0]] = int(line[1])\n","        sz = len(self.stoi_kg)\n","        with open(os.path.join(relation2id_path), \"r\") as f:\n","            while 1:\n","                line = f.readline()\n","                if not line:\n","                    break\n","                line = re.split('\\t|\\n', line)[:2]\n","                self.stoi_kg[line[0]] = int(line[1]) + sz\n","\n","    def _map_gae(self):\n","        if \"GAE\" not in self.args.method_choice:\n","            return\n","        # print(\"using kg embedding\")\n","\n","        _gae_path = self.args.FVQA.gae_path\n","        gae_path = osp.join(_gae_path, str(self.args.FVQA.gae_node_num) + \"_init_\" + self.args.FVQA.gae_init + \".pkl\")\n","        print(\"gae file:\", gae_path)\n","        with open(gae_path, 'rb') as f:\n","            if sys.version_info > (3, 0):\n","                features = pkl.load(f, encoding='latin1')\n","            else:\n","                features = pkl.load(f)\n","        # 下标到gae向量的映射\n","        self.map_gae = torch.FloatTensor(np.array(features)).view(-1, 300)\n","        vertices_f = osp.join(_gae_path, \"g_nodes_\" + str(self.args.FVQA.gae_node_num) + \".json\")\n","        self.stoi_gae = {}\n","        with open(vertices_f) as fp:\n","            vertices_list = json.load(fp)\n","\n","        for i, vertex in enumerate(vertices_list):\n","            self.stoi_gae[vertex] = i\n","        # print(\"test map gae\")\n","        # pdb.set_trace()\n","\n","    def _map_bert(self):\n","        if \"GAE\" not in self.args.method_choice:\n","            return\n","        # print(\"using kg embedding\")\n","\n","        cache_path = osp.join(self.args.FVQA.bert_path, \"map_bert.pt\")\n","        if not osp.exists(cache_path):\n","            _bert_path = self.args.FVQA.bert_path\n","\n","            bert_path = osp.join(_bert_path, \"conceptnet_bert_embeddings.pt\")\n","            print(\"bert file:\", bert_path)\n","            _cache = torch.load(bert_path)  # torch.Size([78334, 1024])\n","\n","            self.map_bert = torch.FloatTensor(self.args.FVQA.max_ans, self.args.bert_dim)\n","            # 下标到gae向量的映射\n","            all = []\n","\n","            with open(osp.join(_bert_path, \"cn_node_names_for_embeddings.txt\"), 'r', encoding='utf-8') as f:\n","                while 1:\n","                    line = f.readline()\n","                    if not line:\n","                        break\n","                    line = re.split('\\n', line)\n","                    all.append(line[0])\n","\n","            self.stoi_bert = {}  # answer to vector文件的 id 下标\n","            for key, value in self.answer_to_index.items():\n","                self.stoi_bert[key] = value\n","                if key in all:\n","                    self.map_bert[value] = _cache[all.index(key), :]\n","                else:\n","                    cnt = 0.0\n","                    tmp = torch.zeros(1, self.args.bert_dim).cuda()\n","                    for i, j in enumerate(all):\n","                        if len(j) >= 4 and len(key) >= 3 and (key in j or j in key):\n","                            # pdb.set_trace()\n","                            tmp += _cache[i, :]  # 取平均\n","                            cnt += 1\n","                        if cnt >= 3:\n","                            break\n","                    if cnt == 0:\n","                        raise TypeError('cnt can not = 0 !!!')\n","                    self.map_bert[value] = tmp / (cnt + 1e-12)\n","\n","            if (self.map_bert != self.map_bert).any():\n","                raise TypeError('cnt can not = 0 !!!')\n","            # pdb.set_trace()\n","            torch.save({'map_bert': self.map_bert, 'stoi_bert': self.stoi_bert}, cache_path)\n","        else:\n","            _cache = torch.load(cache_path)\n","            self.map_bert = _cache['map_bert']  # 词向量列表 + 长度\n","            self.stoi_bert = _cache['stoi_bert']  # 答案下标\n","\n","        # print(\"test map gae\")\n","        # pdb.set_trace()\n","\n","    def _get_answer_vectors(self, ways, answer_indices):\n","        dim = self.vector.dim\n","        if ways == 'GAE':\n","            dim = self.args.bert_dim\n","            return self._encode_answer_vector(self._encode_answer_vector_bert, dim, answer_indices)\n","            # return self._encode_answer_vector(self._encode_answer_vector_gae, dim, answer_indices)\n","        elif ways == 'KG':\n","            return self._encode_answer_vector(self._encode_answer_vector_g2v, dim, answer_indices)\n","        elif ways == 'W2V':\n","            return self._encode_answer_vector(self._encode_answer_vector_w2v, dim, answer_indices)\n","\n","    def _encode_answer_vector(self, encode_model, dim, answer_indices):\n","        if isinstance(answer_indices[0], list):\n","            N, C = len(answer_indices), len(answer_indices[0])\n","            vector = torch.zeros(N, C, dim)\n","            for i, answer_ids in enumerate(answer_indices):\n","                for j, answer_id in enumerate(answer_ids):\n","                    if answer_id != -1:\n","                        vector[i, j, :] = encode_model(self.index_to_answer[answer_id])\n","                    else:\n","                        vector[i, j, :] = self.unk_vector\n","        else:\n","            vector = torch.zeros(len(answer_indices), dim)\n","            for idx, answer_id in enumerate(answer_indices):\n","\n","                if answer_id != -1:\n","                    if type(answer_id).__name__ == 'int':\n","                        vector[idx, :] = encode_model(self.index_to_answer[answer_id])\n","                    else:\n","                        vector[idx, :] = encode_model(self.index_to_answer[answer_id.item()])\n","                else:\n","                    vector[idx, :] = self.unk_vector\n","        return vector, []\n","\n","    def _get_answer_sequences_w2v(self, answer_indices):\n","        seqs, lengths = [], []\n","        max_seq_length = 0\n","        if isinstance(answer_indices[0], list):\n","            N, C = len(answer_indices), len(answer_indices[0])\n","            for i, answer_ids in enumerate(answer_indices):\n","                _seqs = []\n","                for j, answer_id in enumerate(answer_ids):\n","                    if answer_id != -1:\n","                        _seqs.append(self._encode_answer_sequence_w2v(self.index_to_answer[answer_id]))\n","                    else:\n","                        _seqs.append([self.unk_vector])\n","                    if max_seq_length < len(_seqs[-1]):\n","                        max_seq_length = len(_seqs[-1])  # determing max length\n","                seqs.append(_seqs)\n","\n","            vector = torch.zeros(N, C, max_seq_length, self.vector.dim)\n","            for i, _seqs in enumerate(seqs):\n","                for j, seq in enumerate(_seqs):\n","                    if len(seq) != 0:\n","                        vector[i, j, :len(seq), :] = torch.cat(seq, dim=0)\n","                    lengths.append(len(seq))\n","            assert len(lengths) == N * \\\n","                C, 'Wrong lengths - length: {} vs N: {}, C: {} vs seqs: {}'.format(len(lengths), N, C, len(seqs))\n","        else:\n","            for idx, answer_id in enumerate(answer_indices):\n","                if answer_id != -1:\n","                    if type(answer_id).__name__ == 'int':\n","                        seqs.append(self._encode_answer_sequence_w2v(self.index_to_answer[answer_id]))\n","                    else:\n","                        seqs.append(self._encode_answer_sequence_w2v(self.index_to_answer[answer_id.item()]))\n","                else:\n","                    seqs.append([self.unk_vector])\n","\n","                if max_seq_length < len(seqs[-1]):\n","                    max_seq_length = len(seqs[-1])  # determing max length\n","\n","            vector = torch.zeros(len(answer_indices), max_seq_length, self.vector.dim)\n","            for idx, seq in enumerate(seqs):\n","                if len(seq) != 0:\n","                    vector[idx, :len(seq), :] = torch.cat(seq, dim=0)\n","                lengths.append(len(seq))\n","\n","        return vector, lengths\n","\n","    def _encode_answer_vector_bert(self, answer):  # 向量求平均\n","\n","        if isinstance(self.cached_answers_bert.get(answer, -1), int):\n","            answer_vec = torch.zeros(1, self.args.bert_dim)\n","            idk = self.stoi_bert.get(answer, -1)\n","            if idk >= 0:\n","                answer_vec = self.map_bert[idk]\n","            self.cached_answers_bert[answer] = answer_vec\n","        return self.cached_answers_bert[answer]\n","\n","    def _encode_answer_vector_gae(self, answer):  # 向量求平均\n","        if isinstance(self.cached_answers_gae.get(answer, -1), int):\n","            answer_vec = torch.zeros(1, self.vector.dim)\n","            idk = self.stoi_gae.get(answer, -1)\n","            if idk >= 0:\n","                answer_vec = self.map_gae[idk].reshape(1, 300)\n","            self.cached_answers_gae[answer] = answer_vec\n","        return self.cached_answers_gae[answer]\n","\n","    def _encode_answer_vector_g2v(self, answer):  # 向量求平均\n","        if isinstance(self.cached_answers_g2v.get(answer, -1), int):\n","            answer_vec = torch.zeros(1, self.vector.dim)\n","\n","            idk = self.stoi_kg.get(answer, -1)\n","            if idk >= 0:\n","                answer_vec = self.map_kg[idk].reshape(1, 300)\n","            self.cached_answers_g2v[answer] = answer_vec\n","        return self.cached_answers_g2v[answer]\n","\n","    def _encode_answer_vector_w2v(self, answer):  # 向量求平均\n","        if isinstance(self.cached_answers_w2v.get(answer, -1), int):\n","            tokens = nltk.word_tokenize(answer)\n","            answer_vec = torch.zeros(1, self.vector.dim)\n","            cnt = 0\n","            for i, token in enumerate(tokens):\n","                if self.vector.check(token):\n","                    answer_vec += self.vector[token]\n","                    cnt += 1\n","            self.cached_answers_w2v[answer] = answer_vec / (cnt + 1e-12)\n","            # pdb.set_trace()\n","        return self.cached_answers_w2v[answer]\n","\n","    def _encode_answer_sequence_w2v(self, answer):\n","        if isinstance(self.cached_answers_w2v.get(answer, -1), int):\n","            tokens = nltk.word_tokenize(answer)\n","            answer_seq = []\n","            for i, token in enumerate(tokens):\n","                if self.vector.check(token):\n","                    answer_seq.append(self.vector[token].view(1, self.vector.dim))\n","                else:\n","                    answer_seq.append(self.vector['<unk>'].view(1, self.vector.dim))\n","            self.cached_answers_w2v[answer] = answer_seq\n","\n","        return self.cached_answers_w2v[answer]\n","\n","    def _encode_multihot_labels(self, answers):\n","        \"\"\" Turn an answer into a vector \"\"\"\n","        max_answer_index = self.args.TEST.max_answer_index\n","        answer_vec = torch.zeros(max_answer_index)\n","        for answer in answers:\n","            index = self.answer_to_index.get(answer)\n","            if index is not None:\n","                if index < max_answer_index:\n","                    answer_vec[index] += 1\n","        return answer_vec\n","\n","    def evaluate(self, predictions):\n","        raise NotImplementedError"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"nC8mQUkE8u1V","executionInfo":{"status":"ok","timestamp":1769156608927,"user_tz":-420,"elapsed":91,"user":{"displayName":"Toàn Bách Lương","userId":"04806593440338345841"}},"outputId":"96c8258f-310d-4e33-a557-705a8f655f7a"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Writing /content/drive/MyDrive/VQA/code/data/base.py\n"]}]},{"cell_type":"markdown","source":["###### fvqa.py"],"metadata":{"id":"D5Fcmalh8ja-"}},{"cell_type":"code","source":["%%writefile /content/drive/MyDrive/VQA/code/data/fvqa.py\n","import json\n","import os\n","import os.path as osp\n","import nltk\n","from collections import Counter\n","import torch\n","import torch.utils.data as data\n","import pdb\n","\n","################\n","from .base import VisualQA\n","from .preprocess import process_punctuation\n","\n","\n","def get_loader(args, vector, train=False, val=False):\n","    \"\"\" Returns a data loader for the desired split \"\"\"\n","    assert train + val == 1, 'need to set exactly one of {train, val, test} to True'  # 必须有一个为真\n","    id = args.FVQA.data_choice\n","    if train:\n","        filepath = \"train\" + id\n","        print(\"use train data:\", id)\n","        filepath = os.path.join(args.FVQA.train_data_path, filepath)\n","    else:\n","        filepath = \"test\" + id\n","        filepath = os.path.join(args.FVQA.test_data_path, filepath)\n","\n","    split = FVQA(  # 定义每一次训练的VQA输入 # ok\n","        args,\n","        path_for(args, train=train, val=val, filepath=filepath),  # train的问题\n","        vector,  # 对应的词向量\n","        file_path=filepath\n","    )\n","    batch_size = args.TRAIN.batch_size\n","    if val:\n","        batch_size = args.TEST.batch_size\n","    loader = torch.utils.data.DataLoader(  # 定义传统的DataLoader\n","        split,\n","        batch_size=batch_size,\n","        shuffle=True,  # only shuffle the data in training\n","        pin_memory=True,\n","        num_workers=args.TRAIN.data_workers,\n","    )\n","\n","    return loader\n","\n","\n","class FVQA(VisualQA):  # ok\n","    \"\"\" FVQA dataset, open-ended \"\"\"\n","\n","    def __init__(self, args, qa_path, vector, file_path=None):\n","        self.args = args\n","        answer_vocab_path = self.args.FVQA.answer_vocab_path\n","        super(FVQA, self).__init__(args, vector)\n","        # load annotation\n","        with open(qa_path, 'r') as fd:\n","            self.qa_json = json.load(fd)\n","\n","        # print('extracting answers...')\n","\n","        # 把问题变成id向量+长度的表示, 答案变成id向量\n","        if args.fact_map:\n","            #  得到对应的名字\n","            name = \"fact\"\n","            self.answers = list(prepare_fact(self.qa_json))  # 候选答案列表的列表 [[answer1,answer2,...][....]] 每个问题对应的答案. 单词表示\n","        elif args.relation_map:\n","            name = \"relation\"\n","            self.answers = list(prepare_relation(self.qa_json))  # 候选答案列表的列表 [[answer1,answer2,...][....]] 每个问题对应的答案. 单词表示\n","        else:\n","            name = \"answer\"\n","            self.answers = list(prepare_answers(self.qa_json))  # 候选答案列表的列表 [[answer1,answer2,...][....]] 每个问题对应的答案. 单词表示\n","\n","        cache_filepath = self._get_cache_path(qa_path, file_path, name)\n","\n","        # self.support_relation = list(prepare_relation(self.qa_json))\n","        self.questions, self.answer_indices = self._qa_id_represent(cache_filepath)\n","        # pdb.set_trace()\n","        # process images 处理图片\n","\n","    def open_hdf5(self):\n","        self.image_features_path = self.args.FVQA.feature_path\n","        self.image_id_to_index = self._create_image_id_to_index()  # 得到图片编号到下标的表示\n","        # self.image_ids = [q['image_id'] for q in questions_json['questions']]\n","        self.image_ids = self._get_img_id()\n","\n","    def __getitem__(self, item):  # ok\n","        if not hasattr(self, 'image_ids'):\n","            self.open_hdf5()\n","        # if item > len(self.answers):\n","        #     pdb.set_trace()\n","\n","        question, question_length = self.questions[item]  # 问题向量列表\n","        # sample answers\n","        # self.answer_indices[item]：[1,2,3] or [-1, -1 ...]\n","        # answer_cands = Counter(self.answer_indices[item])  # 单个答案 返回类型：Counter({1: 1, 2: 1, 3: 1})\n","        # answer_indices = list(answer_cands.keys())  # 答案有哪几个（下标）[[1,2,3]]\n","        # counts = list(answer_cands.values())  # 这几个答案分别出现了多少次[10]\n","\n","        label = self._encode_multihot_labels(self.answers[item])  # 答案的multihot表示 前百分之多少的答案\n","        image_id = self.image_ids[item]\n","        image, spa = self._load_image(image_id)  # 直接获得图片的特征\n","        # unique_answers, answer_vectors = self._generate_batch_answer(answer_indices, counts)\n","        # answer_vectors == label\n","        # assert answer_vectors == label\n","        # return image, spa, question, unique_answers, answer_vectors, label, item, question_length\n","        # pdb.set_trace()\n","        return image, spa, question, label, item, question_length\n","\n","    def _get_cache_path(self, qa_path, file_path, name):\n","        w2v = \"\"\n","        if \"KG\" in self.args.method_choice:\n","            if \"w2v\" in self.args.FVQA.entity_path:\n","                w2v = \"(w2vinit)_\" + self.args.FVQA.entity_num + \"_\" + self.args.FVQA.KGE\n","            else:\n","                w2v = \"_\" + self.args.FVQA.entity_num + \"_\" + self.args.FVQA.KGE\n","        if \"train\" in qa_path:\n","            cache_filepath = osp.join(file_path, \"fvqa_\" + name + \"_and_question_train_\" +\n","                                      self.args.method_choice + w2v + \"_\" + str(self.args.FVQA.max_ans) + \".pt\")\n","        else:\n","            cache_filepath = osp.join(file_path, \"fvqa_\" + name + \"_and_question_test_\" + self.args.method_choice + w2v + \"_\" + str(\n","                self.args.FVQA.max_ans) + \".pt\")\n","        return cache_filepath\n","\n","    def _qa_id_represent(self, cache_filepath):\n","        if not os.path.exists(cache_filepath):\n","            # print('encoding questions...')\n","            questions = list(prepare_questions(self.qa_json))  # 问题词列表的列表\n","            questions = [self._encode_question(q) for q in questions]  # 把问题变成id向量+长度的表示\n","\n","            # 对于候选答案列表中的每一个问题对应的候选答案列表，转换成下标表示[[1,2,3],[2,3,4]......]  1——>一个答案\n","            answer_indices = [[self.answer_to_index.get(_a, -1) for _a in a] for a in self.answers]  # 如果没有匹配就是 -1\n","            torch.save({'questions': questions, 'answer_indices': answer_indices}, cache_filepath)\n","\n","        else:\n","            # 已经有，对应这个训练/测试集 的问题w2v表，[train 和 test是不一样的]\n","            _cache = torch.load(cache_filepath)\n","            questions = _cache['questions']  # 词向量列表 + 长度\n","            answer_indices = _cache['answer_indices']  # 答案下标\n","            # self.answer_vectors = _cache['answer_vectors']  # 答案的向量表示[平均]\n","\n","        return questions, answer_indices\n","\n","    def _get_img_id(self):\n","        image_ids = []\n","        keys = list(self.qa_json.keys())\n","        for a in keys:\n","            filename = self.qa_json[a][\"img_file\"]\n","            id_and_extension = filename.split('_')[-1]\n","            id = int(id_and_extension.split('.')[0])\n","            if not filename.endswith('.jpg'):\n","                id += 1000000  # 把jpg和jpeg的分开\n","                # pdb.set_trace()\n","            image_ids.append(id)\n","        return image_ids\n","\n","    # def _generate_batch_answer(self, indices, counts):  # 获得每一个batch的500个候选答案。\n","    #     unique_answers = list(range(0, self.args.FVQA.max_ans))\n","    #     # unique_answers = list(set( aid for aids in indices for aid in aids ))\n","    #     answer_dict = {k: i for i, k in enumerate(unique_answers)}\n","    #     answer_vector = torch.zeros(len(indices), len(unique_answers))  # 128,500\n","    #\n","    #     for i in range(len(counts)):  # 128\n","    #         for j, c in zip(indices[i], counts[i]):\n","    #             answer_vector[i, answer_dict[j]] = c  # 把出现的次数附上\n","    #\n","    #     return unique_answers, answer_vector\n","\n","\n","def path_for(args, train=False, val=False, filepath=\"\"):\n","    # tra = \"all_qs_dict_release_train_\" + str(args.FVQA.max_ans) + \".json\"\n","    # tes = \"all_qs_dict_release_test_\" + str(args.FVQA.max_ans) + \".json\"\n","    tra = \"all_qs_dict_release_train_500.json\"\n","    tes = \"all_qs_dict_release_test_500.json\"\n","    if train == True:\n","        return os.path.join(args.FVQA.train_data_path, filepath, tra)\n","    else:\n","        return os.path.join(args.FVQA.test_data_path, filepath, tes)\n","\n","\n","def prepare_questions(questions_json):  # ok\n","    \"\"\" Tokenize and normalize questions from a given question json in the usual VQA format. \"\"\"\n","    keys = list(questions_json.keys())\n","    questions = []\n","    for a in keys:\n","        questions.append(questions_json[a]['question'])  # question的list\n","    for question in questions:\n","        question = question.lower()[:-1]\n","        yield nltk.word_tokenize(process_punctuation(question))  # 得到一个词的list，例如['I', 'LOVE', 'YOU']\n","\n","\n","def prepare_answers(answers_json):  # ok\n","    \"\"\" Normalize answers from a given answer json in the usual VQA format. \"\"\"\n","    keys = list(answers_json.keys())\n","    answers = []\n","\n","    for a in keys:\n","        answer = answers_json[a][\"answer\"]\n","        answers.append([answer] * 10)  # 双层list，内层的list对应一个问题的答案序列\n","    for answer_list in answers:\n","        ret = list(map(process_punctuation, answer_list))  # 去除标点等操作\n","        yield ret\n","\n","\n","def prepare_fact(answers_json):  # ok\n","    \"\"\" Normalize answers from a given answer json in the usual VQA format. \"\"\"\n","    keys = list(answers_json.keys())\n","    support_facts = []\n","    for a in keys:\n","        answer = answers_json[a][\"answer\"]\n","        facts = answers_json[a][\"fact\"]\n","        f1 = facts[0]\n","        f2 = facts[2]\n","        if answer != f1 and answer != f2:\n","            pdb.set_trace()\n","        assert (answer == f1 or answer == f2)\n","        if answer == f1:\n","            fact = f2\n","        else:\n","            fact = f1\n","        support_facts.append([fact] * 10)  # 双层list，内层的list对应一个问题的答案序列\n","    for support_facts_list in support_facts:\n","        ret = list(map(process_punctuation, support_facts_list))  # 去除标点等操作\n","        yield ret\n","\n","\n","def prepare_relation(answers_json):  # ok\n","    \"\"\" Normalize answers from a given answer json in the usual VQA format. \"\"\"\n","    keys = list(answers_json.keys())\n","    relations = []\n","    for a in keys:\n","        facts = answers_json[a][\"fact\"]\n","        relation = facts[1]\n","\n","        relations.append([relation] * 10)  # 双层list，内层的list对应一个问题的答案序列\n","    for relation_list in relations:\n","        ret = list(map(process_punctuation, relation_list))  # 去除标点等操作\n","        yield ret"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"1GON1jJ58G4a","executionInfo":{"status":"ok","timestamp":1769156489492,"user_tz":-420,"elapsed":47,"user":{"displayName":"Toàn Bách Lương","userId":"04806593440338345841"}},"outputId":"2eec7672-7e6f-404e-f6f1-9342c1d0c08f"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Writing /content/drive/MyDrive/VQA/code/data/fvqa.py\n"]}]},{"cell_type":"markdown","source":["###### aokvqa.py"],"metadata":{"id":"Ac4KWctO86tU"}},{"cell_type":"code","source":["%%writefile /content/drive/MyDrive/VQA/code/data/aokvqa.py\n","import json\n","import os\n","import os.path as osp\n","import nltk\n","from collections import Counter\n","import torch\n","import torch.utils.data as data\n","import pdb\n","\n","################\n","from .base import VisualQA\n","from .preprocess import process_punctuation\n","\n","\n","def get_loader(args, vector, train=False, val=False):\n","    \"\"\" Returns a data loader for the desired split \"\"\"\n","    assert train + val == 1, 'need to set exactly one of {train, val, test} to True'  # 必须有一个为真\n","    id = args.FVQA.data_choice\n","    if train:\n","        filepath = \"train\" + id\n","        print(\"use train data:\", id)\n","        filepath = os.path.join(args.FVQA.train_data_path, filepath)\n","    else:\n","        filepath = \"test\" + id\n","        filepath = os.path.join(args.FVQA.test_data_path, filepath)\n","\n","    split = FVQA(  # 定义每一次训练的VQA输入 # ok\n","        args,\n","        path_for(args, train=train, val=val, filepath=filepath),  # train的问题\n","        vector,  # 对应的词向量\n","        file_path=filepath\n","    )\n","    batch_size = args.TRAIN.batch_size\n","    if val:\n","        batch_size = args.TEST.batch_size\n","    loader = torch.utils.data.DataLoader(  # 定义传统的DataLoader\n","        split,\n","        batch_size=batch_size,\n","        shuffle=True,  # only shuffle the data in training\n","        pin_memory=True,\n","        num_workers=args.TRAIN.data_workers,\n","    )\n","\n","    return loader\n","\n","\n","class FVQA(VisualQA):  # ok\n","    \"\"\" FVQA dataset, open-ended \"\"\"\n","\n","    def __init__(self, args, qa_path, vector, file_path=None):\n","        self.args = args\n","        answer_vocab_path = self.args.FVQA.answer_vocab_path\n","        super(FVQA, self).__init__(args, vector)\n","        # load annotation\n","        with open(qa_path, 'r') as fd:\n","            self.qa_json = json.load(fd)\n","\n","        # print('extracting answers...')\n","\n","        # 把问题变成id向量+长度的表示, 答案变成id向量\n","        if args.fact_map:\n","            #  得到对应的名字\n","            name = \"fact\"\n","            self.answers = list(prepare_fact(self.qa_json))  # 候选答案列表的列表 [[answer1,answer2,...][....]] 每个问题对应的答案. 单词表示\n","        elif args.relation_map:\n","            name = \"relation\"\n","            self.answers = list(prepare_relation(self.qa_json))  # 候选答案列表的列表 [[answer1,answer2,...][....]] 每个问题对应的答案. 单词表示\n","        else:\n","            name = \"answer\"\n","            self.answers = list(prepare_answers(self.qa_json))  # 候选答案列表的列表 [[answer1,answer2,...][....]] 每个问题对应的答案. 单词表示\n","\n","        cache_filepath = self._get_cache_path(qa_path, file_path, name)\n","\n","        # self.support_relation = list(prepare_relation(self.qa_json))\n","        self.questions, self.answer_indices = self._qa_id_represent(cache_filepath)\n","        # pdb.set_trace()\n","        # process images 处理图片\n","\n","    def open_hdf5(self):\n","        self.image_features_path = self.args.FVQA.feature_path\n","        self.image_id_to_index = self._create_image_id_to_index()  # 得到图片编号到下标的表示\n","        # self.image_ids = [q['image_id'] for q in questions_json['questions']]\n","        self.image_ids = self._get_img_id()\n","\n","    def __getitem__(self, item):  # ok\n","        if not hasattr(self, 'image_ids'):\n","            self.open_hdf5()\n","        # if item > len(self.answers):\n","        #     pdb.set_trace()\n","\n","        question, question_length = self.questions[item]  # 问题向量列表\n","        # sample answers\n","        # self.answer_indices[item]：[1,2,3] or [-1, -1 ...]\n","        # answer_cands = Counter(self.answer_indices[item])  # 单个答案 返回类型：Counter({1: 1, 2: 1, 3: 1})\n","        # answer_indices = list(answer_cands.keys())  # 答案有哪几个（下标）[[1,2,3]]\n","        # counts = list(answer_cands.values())  # 这几个答案分别出现了多少次[10]\n","\n","        label = self._encode_multihot_labels(self.answers[item])  # 答案的multihot表示 前百分之多少的答案\n","        image_id = self.image_ids[item]\n","        image, spa = self._load_image(image_id)  # 直接获得图片的特征\n","        # unique_answers, answer_vectors = self._generate_batch_answer(answer_indices, counts)\n","        # answer_vectors == label\n","        # assert answer_vectors == label\n","        # return image, spa, question, unique_answers, answer_vectors, label, item, question_length\n","        # pdb.set_trace()\n","        return image, spa, question, label, item, question_length\n","\n","    def _get_cache_path(self, qa_path, file_path, name):\n","        w2v = \"\"\n","        if \"KG\" in self.args.method_choice:\n","            if \"w2v\" in self.args.FVQA.entity_path:\n","                w2v = \"(w2vinit)_\" + self.args.FVQA.entity_num + \"_\" + self.args.FVQA.KGE\n","            else:\n","                w2v = \"_\" + self.args.FVQA.entity_num + \"_\" + self.args.FVQA.KGE\n","        if \"train\" in qa_path:\n","            cache_filepath = osp.join(file_path, \"fvqa_\" + name + \"_and_question_train_\" +\n","                                      self.args.method_choice + w2v + \"_\" + str(self.args.FVQA.max_ans) + \".pt\")\n","        else:\n","            cache_filepath = osp.join(file_path, \"fvqa_\" + name + \"_and_question_test_\" + self.args.method_choice + w2v + \"_\" + str(\n","                self.args.FVQA.max_ans) + \".pt\")\n","        return cache_filepath\n","\n","    def _qa_id_represent(self, cache_filepath):\n","        if not os.path.exists(cache_filepath):\n","            # print('encoding questions...')\n","            questions = list(prepare_questions(self.qa_json))  # 问题词列表的列表\n","            questions = [self._encode_question(q) for q in questions]  # 把问题变成id向量+长度的表示\n","\n","            # 对于候选答案列表中的每一个问题对应的候选答案列表，转换成下标表示[[1,2,3],[2,3,4]......]  1——>一个答案\n","            answer_indices = [[self.answer_to_index.get(_a, -1) for _a in a] for a in self.answers]  # 如果没有匹配就是 -1\n","            torch.save({'questions': questions, 'answer_indices': answer_indices}, cache_filepath)\n","\n","        else:\n","            # 已经有，对应这个训练/测试集 的问题w2v表，[train 和 test是不一样的]\n","            _cache = torch.load(cache_filepath)\n","            questions = _cache['questions']  # 词向量列表 + 长度\n","            answer_indices = _cache['answer_indices']  # 答案下标\n","            # self.answer_vectors = _cache['answer_vectors']  # 答案的向量表示[平均]\n","\n","        return questions, answer_indices\n","\n","    def _get_img_id(self):\n","        image_ids = []\n","        keys = list(self.qa_json.keys())\n","        for a in keys:\n","            filename = self.qa_json[a][\"img_file\"]\n","            id_and_extension = filename.split('_')[-1]\n","            id = int(id_and_extension.split('.')[0])\n","            if not filename.endswith('.jpg'):\n","                id += 1000000  # 把jpg和jpeg的分开\n","                # pdb.set_trace()\n","            image_ids.append(id)\n","        return image_ids\n","\n","    # def _generate_batch_answer(self, indices, counts):  # 获得每一个batch的500个候选答案。\n","    #     unique_answers = list(range(0, self.args.FVQA.max_ans))\n","    #     # unique_answers = list(set( aid for aids in indices for aid in aids ))\n","    #     answer_dict = {k: i for i, k in enumerate(unique_answers)}\n","    #     answer_vector = torch.zeros(len(indices), len(unique_answers))  # 128,500\n","    #\n","    #     for i in range(len(counts)):  # 128\n","    #         for j, c in zip(indices[i], counts[i]):\n","    #             answer_vector[i, answer_dict[j]] = c  # 把出现的次数附上\n","    #\n","    #     return unique_answers, answer_vector\n","\n","\n","def path_for(args, train=False, val=False, filepath=\"\"):\n","    # tra = \"all_qs_dict_release_train_\" + str(args.FVQA.max_ans) + \".json\"\n","    # tes = \"all_qs_dict_release_test_\" + str(args.FVQA.max_ans) + \".json\"\n","    tra = \"all_qs_dict_release_train_500.json\"\n","    tes = \"all_qs_dict_release_test_500.json\"\n","    if train == True:\n","        return os.path.join(args.FVQA.train_data_path, filepath, tra)\n","    else:\n","        return os.path.join(args.FVQA.test_data_path, filepath, tes)\n","\n","\n","def prepare_questions(questions_json):  # ok\n","    \"\"\" Tokenize and normalize questions from a given question json in the usual VQA format. \"\"\"\n","    keys = list(questions_json.keys())\n","    questions = []\n","    for a in keys:\n","        questions.append(questions_json[a]['question'])  # question的list\n","    for question in questions:\n","        question = question.lower()[:-1]\n","        yield nltk.word_tokenize(process_punctuation(question))  # 得到一个词的list，例如['I', 'LOVE', 'YOU']\n","\n","\n","def prepare_answers(answers_json):  # ok\n","    \"\"\" Normalize answers from a given answer json in the usual VQA format. \"\"\"\n","    keys = list(answers_json.keys())\n","    answers = []\n","\n","    for a in keys:\n","        answer = answers_json[a][\"answer\"]\n","        answers.append([answer] * 10)  # 双层list，内层的list对应一个问题的答案序列\n","    for answer_list in answers:\n","        ret = list(map(process_punctuation, answer_list))  # 去除标点等操作\n","        yield ret\n","\n","\n","def prepare_fact(answers_json):  # ok\n","    \"\"\" Normalize answers from a given answer json in the usual VQA format. \"\"\"\n","    keys = list(answers_json.keys())\n","    support_facts = []\n","    for a in keys:\n","        answer = answers_json[a][\"answer\"]\n","        facts = answers_json[a][\"fact\"]\n","        f1 = facts[0]\n","        f2 = facts[2]\n","        if answer != f1 and answer != f2:\n","            pdb.set_trace()\n","        assert (answer == f1 or answer == f2)\n","        if answer == f1:\n","            fact = f2\n","        else:\n","            fact = f1\n","        support_facts.append([fact] * 10)  # 双层list，内层的list对应一个问题的答案序列\n","    for support_facts_list in support_facts:\n","        ret = list(map(process_punctuation, support_facts_list))  # 去除标点等操作\n","        yield ret\n","\n","\n","def prepare_relation(answers_json):  # ok\n","    \"\"\" Normalize answers from a given answer json in the usual VQA format. \"\"\"\n","    keys = list(answers_json.keys())\n","    relations = []\n","    for a in keys:\n","        facts = answers_json[a][\"fact\"]\n","        relation = facts[1]\n","\n","        relations.append([relation] * 10)  # 双层list，内层的list对应一个问题的答案序列\n","    for relation_list in relations:\n","        ret = list(map(process_punctuation, relation_list))  # 去除标点等操作\n","        yield ret"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"zdPudDbk89Wp","executionInfo":{"status":"ok","timestamp":1769156791046,"user_tz":-420,"elapsed":37,"user":{"displayName":"Toàn Bách Lương","userId":"04806593440338345841"}},"outputId":"51232c3c-3d73-4cf5-c093-ff3dc397b20c"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Writing /content/drive/MyDrive/VQA/code/data/aokvqa.py\n"]}]},{"cell_type":"markdown","source":["## Content\n"],"metadata":{"id":"BiyTbn5Z5GNM"}},{"cell_type":"markdown","source":["###### main.py\n"],"metadata":{"id":"uVonzXhy6TIb"}},{"cell_type":"code","source":["%%writefile /content/drive/MyDrive/VQA/code/main.py\n","import os\n","import argparse\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import numpy as np\n","from torch.utils.data import DataLoader\n","from torch.utils.tensorboard import SummaryWriter\n","from tqdm import tqdm\n","import pdb\n","import torch.optim as optim\n","from torch.autograd import Variable\n","from torch.utils.data.dataloader import default_collate\n","import warnings\n","from pprint import pprint\n","\n","# self-defined\n","# import model.fusion_net as fusion_net\n","# import model.answer_net as answer_net\n","# from model import Vector, SimpleClassifier\n","# from config import cfg\n","# from torchlight import initialize_exp, set_seed, snapshot, get_dump_path, show_params\n","# from utils import unseen_mask, freeze_layer, cosine_sim, Metrics, instance_bce_with_logits\n","# from data import fvqa\n","# import copy\n","# torch.multiprocessing.set_start_method('spawn')\n","\n","warnings.filterwarnings('ignore')\n","\n","\n","class Runner:\n","    def __init__(self, args):\n","        # prepare for: data , model, loss fuction, optimizer\n","\n","        self.log_dir = get_dump_path(args)\n","        self.model_dir = os.path.join(self.log_dir, 'model')\n","\n","        self.word2vec = Vector(args.FVQA.common_data_path)\n","        # data load\n","        self.train_loader = fvqa.get_loader(args, self.word2vec, train=True)\n","        self.val_loader = fvqa.get_loader(args, self.word2vec, val=True)\n","\n","        self.avocab = default_collate(list(range(0, args.FVQA.max_ans)))\n","\n","        # question_word2vec: get the word vector (for each word in question )\n","        # the id of which could map to the vector of corresponding token\n","        self.question_word2vec = self.word2vec._prepare(self.train_loader.dataset.token_to_index)\n","\n","        # get the fusion_model and answer_net\n","        self._model_choice(args)\n","\n","        # get the mask from zsl\n","        self.negtive_mux = unseen_mask(args, self.val_loader)\n","\n","        # optimizer\n","        params_for_optimization = list(self.fusion_model.parameters()) + list(self.answer_net.parameters())\n","        self.optimizer = optim.Adam([p for p in params_for_optimization if p.requires_grad], lr=args.TRAIN.lr)\n","\n","        # loss fuction\n","        self.log_softmax = nn.LogSoftmax(dim=1).cuda()\n","\n","        # Recorder\n","        self.max_acc = [0, 0, 0, 0]\n","        self.max_zsl_acc = [0, 0, 0, 0]\n","        self.best_epoch = 0\n","        self.correspond_loss = 1e20\n","\n","        self.early_stop = 0\n","\n","        print(\"fusion_model:\")\n","        pprint(self.fusion_model)\n","        print(\"Answer Model:\")\n","        pprint(self.answer_net)\n","\n","        self.args = args\n","\n","        # test stage:\n","        if self.args.now_test:\n","            print(\"begin test! ...\")\n","            print(\"loading model  ...\")\n","            self._load_model(self.fusion_model, \"fusion\")\n","            self._load_model(self.answer_net, \"embedding\")\n","\n","    def run(self):\n","        # 1. define the parameters which are out the epoch\n","        # 2. Update statistical indicator\n","        # 3. concate of answer embedding\n","\n","        # Answer embedding :\n","        # choices belong to: ['CLS', 'W2V', 'KG', 'GAE', 'KG_W2V', 'KG_GAE', 'GAE_W2V', 'KG_GAE_W2V']\n","        # well, we recommend only use the parameter : 'CLS' or 'W2V'.\n","        # since that the resource of other choices need extra training.\n","        if args.method_choice != 'CLS':\n","            previous_var = None\n","            for method_choice in self.method_list:\n","                # get the corresponding choice embedding\n","                answer_var, answer_len = self.train_loader.dataset._get_answer_vectors(method_choice, self.avocab)\n","\n","                # normalize in row and then concate then\n","                answer_var = F.normalize(answer_var, p=2, dim=1)\n","                if previous_var is not None:\n","                    previous_var = torch.cat([previous_var, answer_var], dim=1)\n","                else:\n","                    previous_var = answer_var\n","            self.answer_var = Variable(previous_var.float()).cuda()\n","\n","        # warm up (ref: ramen)\n","        self.gradual_warmup_steps = [i * self.args.TRAIN.lr for i in torch.linspace(0.5, 2.0, 7)]\n","        self.lr_decay_epochs = range(14, 47, self.args.TRAIN.lr_decay_step)\n","\n","        # if test:\n","        if self.args.now_test:\n","            self.args.TRAIN.epochs = 2\n","\n","        for epoch in range(self.args.TRAIN.epochs):\n","\n","            self.early_stop += 1\n","            if self.args.patience < self.early_stop:\n","                # early stop\n","                break\n","            # warm up\n","            if epoch < len(self.gradual_warmup_steps):\n","                self.optimizer.param_groups[0]['lr'] = self.gradual_warmup_steps[epoch]\n","            elif epoch in self.lr_decay_epochs:\n","                self.optimizer.param_groups[0]['lr'] *= self.args.TRAIN.lr_decay_rate\n","\n","            self.train_metrics = Metrics()\n","            self.val_metrics = Metrics()\n","            self.zsl_metrics = Metrics()\n","            # use TOP50 metrics for fact mapping:\n","            if self.args.fact_map == 1:\n","                self.train_metrics = Metrics(topnum=50)\n","                self.val_metrics = Metrics(topnum=50)\n","                self.zsl_metrics = Metrics(topnum=50)\n","\n","            # train\n","            if not self.args.now_test:\n","                ######## begin training!! #######\n","                self.train(epoch)\n","                #################################\n","                lr = self.optimizer.param_groups[0]['lr']\n","                # recode:\n","                logger.info(\n","                    f'Train Epoch {epoch}: LOSS={self.train_metrics.total_loss: .5f}, lr={lr: .6f}, acc1={self.train_metrics.acc_1: .2f},acc3={self.train_metrics.acc_3: .2f},acc10={self.train_metrics.acc_10: .2f}')\n","            # eval\n","            if epoch % 1 == 0 and epoch > 0:\n","                ######## begin evaling!! #######\n","                self.eval(epoch)\n","                #################################\n","                logger.info('#################################################################################################################')\n","                logger.info(f'Test Epoch {epoch}: LOSS={self.val_metrics.total_loss: .5f}, acc1={self.val_metrics.acc_1: .2f}, acc3={self.val_metrics.acc_3: .2f}, acc10={self.val_metrics.acc_10: .2f}')\n","                if args.ZSL and not self.args.fact_map and not args.relation_map:\n","                    logger.info(f'Zsl Epoch {epoch}: LOSS={self.zsl_metrics.total_loss: .5f}, acc1={self.zsl_metrics.acc_1: .2f}, acc3={self.zsl_metrics.acc_3: .2f}, acc10={self.zsl_metrics.acc_10: .2f}')\n","                logger.info('#################################################################################################################')\n","\n","                # add 0.1 accuracy punishment, avoid for too much attention on hit@10 acc\n","                # 添加0.1的精读惩罚, 防止模型过多的关注hit@10 acc\n","                if self.val_metrics.total_loss < (self.correspond_loss - 1) or self.val_metrics.acc_all > (self.max_acc[3] + 0.2):\n","                    # reset early_stop and updata\n","                    self.early_stop = 0\n","                    self.best_epoch = epoch\n","                    self.correspond_loss = self.val_metrics.total_loss\n","                    self._updata_best_result(self.max_acc, self.val_metrics)\n","\n","                    self.best_fusion_model = copy.deepcopy(self.fusion_model)\n","                    self.best_answer_net = copy.deepcopy(self.answer_net)\n","\n","                    # ZSL result\n","                    if args.ZSL and not self.args.fact_map and not args.relation_map:\n","                        self._updata_best_result(self.max_zsl_acc, self.zsl_metrics)\n","\n","                if not args.no_tensorboard and not self.args.now_test:\n","                    writer.add_scalar('loss', self.val_metrics.total_loss, epoch)\n","                    writer.add_scalar('acc1', self.val_metrics.acc_1, epoch)\n","                    writer.add_scalar('acc3', self.val_metrics.acc_3, epoch)\n","                    writer.add_scalar('acc10', self.val_metrics.acc_10, epoch)\n","\n","        # save the model\n","        if not self.args.now_test and self.args.save_model:\n","            self.fusion_model_path = self._save_model(self.best_fusion_model, \"fusion\")\n","            self.answer_net_path = self._save_model(self.best_answer_net, \"embedding\")\n","\n","    def train(self, epoch):\n","        self.fusion_model.train()\n","        self.answer_net.train()\n","        prefix = \"train\"\n","        tq = tqdm(self.train_loader, desc='{} E{:03d}'.format(prefix, epoch), ncols=0)\n","\n","        for visual_features, boxes, question_features, answers, idx, q_len in tq:\n","            visual_features = Variable(visual_features.float()).cuda()\n","            boxes = Variable(boxes.float()).cuda()\n","            question_features = Variable(question_features).cuda()\n","            answers = Variable(answers).cuda()\n","            q_len = Variable(q_len).cuda()\n","            fusion_embedading = self.fusion_model(visual_features, boxes, question_features, q_len)\n","\n","            # Classifier-based methods\n","            if args.method_choice == 'CLS':\n","                # TODO: Normalization?\n","                predicts = self.answer_net(fusion_embedading)\n","                loss = instance_bce_with_logits(predicts, answers / 10)\n","            # Mapping-based methods\n","            else:\n","                answer_embedding = self.answer_net(self.answer_var)\n","                # notice the temperature (correspoding to specific score)\n","                predicts = cosine_sim(fusion_embedading, answer_embedding) / self.args.loss_temperature\n","                predicts = predicts.to(torch.float64)\n","                nll = -self.log_softmax(predicts).to(torch.float64)\n","                # loss = (nll * answers[0] / answers[0].sum(1, keepdim=True)).sum(dim=1).mean()\n","                loss = (nll * answers / answers.sum(1, keepdim=True)).sum(dim=1).mean()\n","\n","            self.optimizer.zero_grad()\n","            loss.backward()\n","            self.optimizer.step()\n","            self.train_metrics.update_per_batch(loss, answers.data, predicts.data)\n","        self.train_metrics.update_per_epoch()\n","\n","    def eval(self, epoch):\n","        self.fusion_model.eval()\n","        self.answer_net.eval()\n","        prefix = \"eval\"\n","        tq = tqdm(self.val_loader, desc='{} E{:03d}'.format(prefix, epoch), ncols=0)\n","\n","        for visual_features, boxes, question_features, answers, idx, q_len in tq:\n","            with torch.no_grad():\n","                visual_features = Variable(visual_features.float()).cuda()\n","                boxes = Variable(boxes.float()).cuda()\n","                question_features = Variable(question_features).cuda()\n","                answers = Variable(answers).cuda()\n","                q_len = Variable(q_len).cuda()\n","                fusion_embedading = self.fusion_model(visual_features, boxes, question_features, q_len)\n","\n","                if args.method_choice == 'CLS':\n","                    predicts = self.answer_net(fusion_embedading)\n","                    loss = instance_bce_with_logits(predicts, answers / 10)\n","\n","                else:\n","                    answer_embedding = self.answer_net(self.answer_var)\n","                    predicts = cosine_sim(fusion_embedading, answer_embedding) / self.args.loss_temperature\n","                    predicts = predicts.to(torch.float64)\n","                    nll = -self.log_softmax(predicts).to(torch.float64)\n","                    loss = (nll * answers / answers.sum(1, keepdim=True)).sum(dim=1).mean()\n","\n","                if args.ZSL == 1 and not self.args.fact_map and not args.relation_map:\n","                    # if predicts.shape[0] != self.negtive_mux.shape[0]:\n","                    #     pdb.set_trace()\n","                    zsl_predicts = predicts + self.negtive_mux[:predicts.shape[0], :]\n","\n","            self.val_metrics.update_per_batch(loss, answers.data, predicts.data)\n","            if args.ZSL == 1 and not self.args.fact_map and not args.relation_map:\n","                self.zsl_metrics.update_per_batch(loss, answers.data, zsl_predicts.data)\n","\n","        self.val_metrics.update_per_epoch()\n","        if args.ZSL == 1 and not self.args.fact_map and not args.relation_map:\n","            self.zsl_metrics.update_per_epoch()\n","\n","    def _model_choice(self, args):\n","        assert args.fusion_model in ['SAN', 'MLP', 'BAN', 'UD']\n","        # models api\n","        self.fusion_model = getattr(fusion_net, args.fusion_model)(args, self.train_loader.dataset,\n","                                                                   self.question_word2vec).cuda()\n","        # freeze word embedding\n","        if args.freeze_w2v and args.fusion_model != 'MLP':\n","            freeze_layer(self.fusion_model.w_emb)\n","\n","        # answer models\n","        assert args.method_choice in ['CLS', 'W2V', 'KG', 'GAE', 'KG_W2V', 'KG_GAE', 'GAE_W2V', 'KG_GAE_W2V']\n","        ans_len_table = {'W2V': 300, 'KG': 300, 'GAE': 1024, 'CLS': 0}\n","        self.method_list = args.method_choice.split('_')\n","        self.method_list.sort()\n","        for i in self.method_list:\n","            args.ans_feature_len += ans_len_table[i]\n","        # Mapping-based methods\n","        if args.method_choice != 'CLS':\n","            assert args.answer_embedding in ['MLP']\n","            self.answer_net = getattr(answer_net, args.answer_embedding)(args, self.train_loader.dataset).cuda()\n","        else:\n","            # Classifier-based methods\n","            self.answer_net = SimpleClassifier(args.embedding_size, 2 * args.hidden_size, args.FVQA.max_ans, 0.5).cuda()\n","\n","    def _updata_best_result(self, max_acc, metrics):\n","        max_acc[3] = metrics.acc_all\n","        max_acc[2] = metrics.acc_10\n","        max_acc[1] = metrics.acc_3\n","        max_acc[0] = metrics.acc_1\n","\n","    def _load_model(self, model, function):\n","        assert function == \"fusion\" or function == \"embedding\"\n","        # support entity mapping\n","        if self.args.fact_map:\n","            target = \"fact\"\n","        # relation mapping\n","        elif self.args.relation_map:\n","            target = \"relation\"\n","        else:\n","            target = \"answer\"\n","        model_name = type(model).__name__\n","        if not self.args.ZSL:\n","            target = \"general_\" + target\n","        save_path = os.path.join(self.args.FVQA.model_save_path, function)\n","        save_path = os.path.join(save_path, f'{target}_{model_name}_{self.args.FVQA.data_choice}.pkl')\n","\n","        model.load_state_dict(torch.load(save_path))\n","        print(f\"loading {function} model done!\")\n","\n","    def _save_model(self, model, function):\n","        assert function == \"fusion\" or function == \"embedding\"\n","        if self.args.fact_map:\n","            target = \"fact\"\n","        elif self.args.relation_map:\n","            target = \"relation\"\n","        else:\n","            target = \"answer\"\n","        model_name = type(model).__name__\n","        if not self.args.ZSL:\n","            target = \"general_\" + target\n","        save_path = os.path.join(self.args.FVQA.model_save_path, function)\n","        os.makedirs(save_path, exist_ok=True)\n","        save_path = os.path.join(save_path, f'{target}_{model_name}_{self.args.FVQA.data_choice}.pkl')\n","\n","        torch.save(model.state_dict(), save_path)\n","        return save_path\n","\n","\n","if __name__ == '__main__':\n","    # Config loading...\n","    cfg = cfg()\n","    args = cfg.get_args()\n","    cfg.update_train_configs(args)\n","    set_seed(cfg.random_seed)\n","\n","    # Environment initialization...\n","    logger = initialize_exp(cfg)\n","    logger_path = get_dump_path(cfg)\n","    if not cfg.no_tensorboard:\n","        writer = SummaryWriter(log_dir=os.path.join(logger_path, 'tensorboard'))\n","\n","    torch.cuda.set_device(cfg.gpu_id)\n","\n","    # Run...\n","    runner = Runner(cfg)\n","    runner.run()\n","\n","    #  information output:\n","    logger.info(f\"best performance = {runner.max_acc[0]: .2f},{runner.max_acc[1]: .2f},{runner.max_acc[2]: .2f}. best epoch = {runner.best_epoch}, correspond_loss={runner.correspond_loss: .4f}\")\n","    if args.ZSL == 1 and not args.fact_map and not args.relation_map:\n","        logger.info(f\" zsl performance = {runner.max_zsl_acc[0]: .2f},{runner.max_zsl_acc[1]: .2f},{runner.max_zsl_acc[2]: .2f}\")\n","    if not cfg.now_test:\n","        logger.info(f\" fusion_model_path = {runner.fusion_model_path}\")\n","        logger.info(f\" answer_net_path = {runner.answer_net_path}\")\n","    if not cfg.no_tensorboard:\n","        writer.close()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"6nIYpRQ5rc3s","executionInfo":{"status":"ok","timestamp":1769155911304,"user_tz":-420,"elapsed":535,"user":{"displayName":"Toàn Bách Lương","userId":"04806593440338345841"}},"outputId":"3e5c54ce-a076-4d23-87f1-a2709b6f9fc5"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Overwriting /content/drive/MyDrive/VQA/code/main.py\n"]}]},{"cell_type":"code","source":["!ls"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"fLb3EWlA0kEu","executionInfo":{"status":"ok","timestamp":1769155911399,"user_tz":-420,"elapsed":93,"user":{"displayName":"Toàn Bách Lương","userId":"04806593440338345841"}},"outputId":"ad7b0e6c-5c8e-4229-8c58-e303a25d1ea6"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["cfgs  code  data  kg  run.ipynb\n"]}]},{"cell_type":"markdown","source":["###### config.py\n"],"metadata":{"id":"2O8VVV-YCPDZ"}},{"cell_type":"code","source":["%%writefile /content/drive/MyDrive/VQA/code/config.py\n","import argparse"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ESSZJbNPrxOB","executionInfo":{"status":"ok","timestamp":1769155911883,"user_tz":-420,"elapsed":481,"user":{"displayName":"Toàn Bách Lương","userId":"04806593440338345841"}},"outputId":"824c76b1-2886-4ea0-cc51-8f752ed279f8"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Overwriting /content/drive/MyDrive/VQA/code/config.py\n"]}]},{"cell_type":"markdown","source":["## Model\n"],"metadata":{"id":"wXzgiSGB5UK1"}},{"cell_type":"markdown","source":["###### \\_\\_init__.py"],"metadata":{"id":"Y1cuCeNi_h_S"}},{"cell_type":"code","source":["%%writefile /content/drive/MyDrive/VQA/code/model/__init__.py\n","from .attention import BiAttention\n","from .classifier import SimpleClassifier\n","from .counting import Counter\n","from .fc import FCNet, BCNet"],"metadata":{"id":"O1ABSmhv_3Ny","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1769155911901,"user_tz":-420,"elapsed":8,"user":{"displayName":"Toàn Bách Lương","userId":"04806593440338345841"}},"outputId":"84cf967b-88b3-449f-90ed-55dee2b40fa4"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Writing /content/drive/MyDrive/VQA/code/model/__init__.py\n"]}]},{"cell_type":"markdown","source":["###### answer_net.py"],"metadata":{"id":"i3_ZlyvAEY0s"}},{"cell_type":"code","source":["%%writefile /content/drive/MyDrive/VQA/code/model/answer_net.py\n","from answer_net.mlp import MLP"],"metadata":{"id":"U_52cJfKEYeI","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1769155911907,"user_tz":-420,"elapsed":5,"user":{"displayName":"Toàn Bách Lương","userId":"04806593440338345841"}},"outputId":"27c50006-5534-4788-8cdf-4c3ae08d1a48"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Writing /content/drive/MyDrive/VQA/code/model/answer_net.py\n"]}]},{"cell_type":"markdown","source":["###### fusion_net.py"],"metadata":{"id":"7Hz2dwhbDvAO"}},{"cell_type":"code","source":["%%writefile /content/drive/MyDrive/VQA/code/model/fusion_net.py\n","from fusion_net.updn import UD\n","from fusion_net.ban import BAN\n","from fusion_net.san import SAN\n","from fusion_net.mlp import MLP"],"metadata":{"id":"L2kOI6XNDu0S"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["###### vector.py"],"metadata":{"id":"CJ1vexQVEKHd"}},{"cell_type":"code","source":["%%writefile /content/drive/MyDrive/VQA/code/model/vector.py\n","import array\n","import zipfile\n","from tqdm import tqdm\n","from six.moves.urllib.request import urlretrieve\n","import os\n","import os.path as osp\n","import torch\n","import io\n","\n","class Vector(object):\n","    def __init__(self, cache_path,\n","                 vector_type='glove.840B', unk_init=torch.Tensor.zero_) -> object:\n","        urls = {\n","            'glove.42B': 'http://nlp.stanford.edu/data/glove.42B.300d.zip',\n","            'glove.840B': 'http://nlp.stanford.edu/data/glove.840B.300d.zip',\n","            'glove.6B': 'http://nlp.stanford.edu/data/glove.6B.zip',\n","        }\n","        url = urls[vector_type] if urls.get(vector_type, False) != False else None\n","        name = osp.splitext(osp.basename(url))[0] + '.txt'  # glove.840B.300d.txt\n","\n","        self.unk_init = unk_init\n","        self.cache(name, cache_path, url=url)\n","\n","    def __getitem__(self, token):\n","        if self.stoi.get(token, -1) != -1:\n","            return self.vectors[self.stoi[token]]\n","        else:\n","            return self.unk_init(torch.Tensor(1, self.dim))\n","\n","    def _prepare(self, vocab):\n","        word2vec = torch.Tensor(len(vocab), self.dim)\n","        for token, idx in vocab.items():\n","            word2vec[idx, :] = self[token]\n","\n","        return word2vec\n","\n","    def check(self, token):\n","        if self.stoi.get(token, -1) != -1:\n","            return True\n","        else:\n","            return False\n","\n","    def cache(self, name, cache_path, url=None):\n","        # cache_path='.vector_cache',\n","        #name= \"glove.840B.300d.txt\"\n","        #url = 'http://nlp.stanford.edu/data/glove.840B.300d.zip'\n","\n","        path = osp.join(cache_path, name)\n","        path_pt = \"{}.pt\".format(path)\n","\n","        if not osp.isfile(path_pt):\n","            # download vocab file if it does not exists\n","            if not osp.exists(path) and url:\n","                dest = osp.join(cache_path, os.path.basename(url))\n","                if not osp.exists(dest):\n","                    print('[-] Downloading vectors from {}'.format(url))\n","                    if not osp.exists(cache_path):\n","                        os.mkdir(cache_path)\n","\n","                    with tqdm(unit='B', unit_scale=True, miniters=1, desc=dest) as t:\n","                        urlretrieve(url, dest, reporthook=reporthook(t))\n","\n","                print('[-] Extracting vectors into {}'.format(path))\n","                ext = os.path.splitext(dest)[1][1:]\n","                if ext == 'zip':\n","                    with zipfile.ZipFile(dest, \"r\") as zf:\n","                        zf.extractall(cache_path)\n","\n","            if not os.path.isfile(path):\n","                raise RuntimeError('no vectors found at {}'.format(path))\n","\n","            # build vocab list\n","            itos, vectors, dim = [], array.array(str('d')), None\n","\n","            # Try to read the whole file with utf-8 encoding.\n","            binary_lines = False\n","            try:\n","                with io.open(path, encoding=\"utf8\") as f:\n","                    lines = [line for line in f]\n","            # If there are malformed lines, read in binary mode\n","            # and manually decode each word from utf-8\n","            except:\n","                print(\"[!] Could not read {} as UTF8 file, \"\n","                      \"reading file as bytes and skipping \"\n","                      \"words with malformed UTF8.\".format(path))\n","                with open(path, 'rb') as f:\n","                    lines = [line for line in f]\n","                binary_lines = True\n","\n","            print(\"[-] Loading vectors from {}\".format(path))  # 读取vector\n","            for line in tqdm(lines, total=len(lines)):\n","                # Explicitly splitting on \" \" is important, so we don't\n","                # get rid of Unicode non-breaking spaces in the vectors.\n","                entries = line.rstrip().split(\" \")\n","                word, entries = entries[0], entries[1:]\n","                if dim is None and len(entries) > 1:\n","                    dim = len(entries)\n","                elif len(entries) == 1:\n","                    print(\"Skipping token {} with 1-dimensional \"\n","                          \"vector {}; likely a header\".format(word, entries))\n","                    continue\n","                elif dim != len(entries):\n","                    raise RuntimeError(\n","                        \"Vector for token {} has {} dimensions, but previously \"\n","                        \"read vectors have {} dimensions. All vectors must have \"\n","                        \"the same number of dimensions.\".format(word, len(entries), dim))\n","\n","                vectors.extend(float(x) for x in entries)\n","                itos.append(word)\n","\n","            self.itos = itos\n","            self.stoi = {word: i for i, word in enumerate(itos)}\n","            self.vectors = torch.Tensor(vectors).view(-1, dim)\n","            self.dim = dim\n","            print('* Caching vectors to {}'.format(path_pt))\n","            torch.save((self.itos, self.stoi, self.vectors, self.dim), path_pt)\n","        else:\n","            print('* Loading vectors to {}'.format(path_pt))\n","            self.itos, self.stoi, self.vectors, self.dim = torch.load(path_pt)"],"metadata":{"id":"ukpOiIGBezTz","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1765514744879,"user_tz":-420,"elapsed":448,"user":{"displayName":"Toàn Bách Lương","userId":"04806593440338345841"}},"outputId":"4358c6c5-9e76-4283-8caa-35afe75f4f86"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Overwriting /content/drive/MyDrive/VQA/code/model/vector.py\n"]}]},{"cell_type":"markdown","source":["###### attetion.py"],"metadata":{"id":"qPnG22YpEGFQ"}},{"cell_type":"code","source":["%%writefile /content/drive/MyDrive/VQA/code/model/attention.py\n","import torch\n","import torch.nn as nn\n","from torch.nn.utils.weight_norm import weight_norm\n","from .fc import FCNet, BCNet\n","import torch.nn.functional as F\n","\n","class BaseAttention(nn.Module):\n","    def __init__(self, v_dim, q_dim, num_hid):\n","        super(BaseAttention, self).__init__()\n","        self.nonlinear = FCNet([v_dim + q_dim, num_hid])\n","        self.linear = weight_norm(nn.Linear(num_hid, 1), dim=None)\n","\n","    def forward(self, v, q):\n","        \"\"\"\n","        v: [batch, k, vdim]\n","        q: [batch, qdim]\n","        \"\"\"\n","        logits = self.logits(v, q)\n","        w = nn.functional.softmax(logits, 1)\n","        return w\n","\n","    def logits(self, v, q):\n","        num_objs = v.size(1)\n","        q = q.unsqueeze(1).repeat(1, num_objs, 1)\n","        vq = torch.cat((v, q), 2)\n","        joint_repr = self.nonlinear(vq)\n","        logits = self.linear(joint_repr)\n","        return logits\n","\n","\n","class UpDnAttention(nn.Module):\n","    def __init__(self, v_dim, q_dim, num_hid, dropout=0.2):\n","        super(UpDnAttention, self).__init__()\n","\n","        self.v_proj = FCNet([v_dim, num_hid])\n","        self.q_proj = FCNet([q_dim, num_hid])\n","        self.dropout = nn.Dropout(dropout)\n","        self.linear = weight_norm(nn.Linear(num_hid, 1), dim=None)\n","\n","    def forward(self, v, q):\n","        \"\"\"\n","        v: [batch, k, vdim]\n","        q: [batch, qdim]\n","        \"\"\"\n","        logits = self.logits(v, q)\n","        w = nn.functional.softmax(logits, 1)\n","        return w\n","\n","    def logits(self, v, q):\n","        batch, k, _ = v.size()\n","        v_proj = self.v_proj(v)  # [batch, k, qdim]\n","        q_proj = self.q_proj(q).unsqueeze(1).repeat(1, k, 1)\n","        joint_repr = v_proj * q_proj\n","        joint_repr = self.dropout(joint_repr)\n","        logits = self.linear(joint_repr)\n","        return logits\n","\n","class SanAttention(nn.Module):\n","  def __init__(self, v_features, q_features, mid_features, glimpses, drop=0.0):\n","    super(SanAttention, self).__init__()\n","    self.v_conv = nn.Conv2d(v_features, mid_features, 1, bias=False)  # let self.lin take care of bias\n","    self.q_lin = nn.Linear(q_features, mid_features)\n","    self.x_conv = nn.Conv2d(mid_features, glimpses, 1)\n","\n","    self.drop = nn.Dropout(drop)\n","    self.relu = nn.LeakyReLU(inplace=True)\n","\n","  def forward(self, v, q):\n","    v = self.v_conv(self.drop(v))\n","    q = self.q_lin(self.drop(q))\n","    q = tile_2d_over_nd(q, v)\n","    x = self.relu(v + q)\n","    x = self.x_conv(self.drop(x))\n","    return x\n","\n","def tile_2d_over_nd(feature_vector, feature_map):\n","  \"\"\" Repeat the same feature vector over all spatial positions of a given feature map.\n","    The feature vector should have the same batch size and number of features as the feature map.\n","  \"\"\"\n","  n, c = feature_vector.size()\n","  spatial_size = feature_map.dim() - 2\n","  tiled = feature_vector.view(n, c, *([1] * spatial_size)).expand_as(feature_map)\n","  return tiled\n","\n","def apply_attention(input, attention):\n","  \"\"\" Apply any number of attention maps over the input.\n","    The attention map has to have the same size in all dimensions except dim=1.\n","  \"\"\"\n","  # import pdb\n","  # pdb.set_trace()\n","  n, c = input.size()[:2]\n","  glimpses = attention.size(1)\n","\n","  # flatten the spatial dims into the third dim, since we don't need to care about how they are arranged\n","  input = input.view(n, c, -1)\n","  attention = attention.view(n, glimpses, -1)\n","  s = input.size(2)\n","\n","  # apply a softmax to each attention map separately\n","  # since softmax only takes 2d inputs, we have to collapse the first two dimensions together\n","  # so that each glimpse is normalized separately\n","  attention = attention.view(n * glimpses, -1)\n","  attention = F.softmax(attention)\n","\n","  # apply the weighting by creating a new dim to tile both tensors over\n","  target_size = [n, glimpses, c, s]\n","  input = input.view(n, 1, c, s).expand(*target_size)\n","  attention = attention.view(n, glimpses, 1, s).expand(*target_size)\n","  weighted = input * attention\n","  # sum over only the spatial dimension\n","  weighted_mean = weighted.sum(dim=3)\n","  # the shape at this point is (n, glimpses, c, 1)\n","  return weighted_mean.view(n, -1)\n","\n","\n","class BiAttention(nn.Module):\n","    def __init__(self, x_dim, y_dim, z_dim, glimpse, dropout=[.2, .5]):\n","        super(BiAttention, self).__init__()\n","\n","        self.glimpse = glimpse\n","        self.logits = weight_norm(BCNet(x_dim, y_dim, z_dim, glimpse, dropout=dropout, k=3),\n","                                  name='h_mat', dim=None)\n","\n","    def forward(self, v, q, v_mask=True):\n","        \"\"\"\n","        v: [batch, k, vdim]\n","        q: [batch, qdim]\n","        \"\"\"\n","        p, logits = self.forward_all(v, q, v_mask)\n","        return p, logits\n","\n","    def forward_all(self, v, q, v_mask=True):\n","        v_num = v.size(1)\n","        q_num = q.size(1)\n","        logits = self.logits(v, q)  # b x g x v x q\n","\n","        if v_mask:\n","            mask = (0 == v.abs().sum(2)).unsqueeze(1).unsqueeze(3).expand(logits.size())\n","            logits.data.masked_fill_(mask.data, -float('inf'))\n","\n","        p = nn.functional.softmax(logits.view(-1, self.glimpse, v_num * q_num), 2)\n","        return p.view(-1, self.glimpse, v_num, q_num), logits"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"zQojqDH0eWMb","executionInfo":{"status":"ok","timestamp":1769155922664,"user_tz":-420,"elapsed":753,"user":{"displayName":"Toàn Bách Lương","userId":"04806593440338345841"}},"outputId":"6e4eadc8-29e6-4d7d-c462-075e08f2a6f0"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Overwriting /content/drive/MyDrive/VQA/code/model/attention.py\n"]}]},{"cell_type":"markdown","source":["###### language_model.py"],"metadata":{"id":"pqTx0OfLEN9U"}},{"cell_type":"code","source":["%%writefile /content/drive/MyDrive/VQA/code/model/language_model.py\n","import torch\n","import torch.nn as nn\n","from torch.autograd import Variable\n","import numpy as np\n","from torch.nn.utils.rnn import pack_padded_sequence\n","import torch.nn.init as init\n","import pdb\n","\n","\n","class WordEmbedding(nn.Module):\n","    \"\"\"Word Embedding\n","\n","    The ntoken-th dim is used for padding_idx, which agrees *implicitly*\n","    with the definition in Dictionary.\n","    \"\"\"\n","\n","    def __init__(self, ntoken, emb_dim, dropout=0):\n","        super(WordEmbedding, self).__init__()\n","        self.emb = nn.Embedding(ntoken + 1, emb_dim, padding_idx=ntoken)\n","        self.dropout = nn.Dropout(dropout)\n","        self.ntoken = ntoken\n","        self.emb_dim = emb_dim\n","\n","    def init_embedding(self, np_file):\n","        # weight_init = torch.from_numpy(np.load(np_file))\n","        weight_init = np_file\n","        assert weight_init.shape == (self.ntoken, self.emb_dim)\n","        self.emb.weight.data[:self.ntoken] = weight_init\n","\n","    def forward(self, x):\n","        emb = self.emb(x)\n","        emb = self.dropout(emb)\n","        return emb\n","\n","\n","class UpDnQuestionEmbedding(nn.Module):\n","    def __init__(self, in_dim, num_hid, nlayers, bidirect, dropout=0, rnn_type='GRU'):\n","        \"\"\"Module for question embedding\n","        \"\"\"\n","        super(UpDnQuestionEmbedding, self).__init__()\n","        assert rnn_type == 'LSTM' or rnn_type == 'GRU'\n","        rnn_cls = nn.LSTM if rnn_type == 'LSTM' else nn.GRU\n","\n","        self.rnn = rnn_cls(\n","            in_dim, num_hid, nlayers,\n","            bidirectional=bidirect,\n","            dropout=dropout,\n","            batch_first=True)\n","\n","        self.in_dim = in_dim\n","        self.num_hid = num_hid\n","        self.nlayers = nlayers\n","        self.rnn_type = rnn_type\n","        self.ndirections = 1 + int(bidirect)\n","\n","    def init_hidden(self, batch):\n","        # just to get the type of tensor\n","        weight = next(self.parameters()).data\n","        hid_shape = (self.nlayers * self.ndirections, batch, self.num_hid)\n","        if self.rnn_type == 'LSTM':\n","            return (Variable(weight.new(*hid_shape).zero_()),\n","                    Variable(weight.new(*hid_shape).zero_()))\n","        else:\n","            return Variable(weight.new(*hid_shape).zero_())\n","\n","    def forward(self, x):\n","        # x: [batch, sequence, in_dim]\n","        batch = x.size(0)\n","        hidden = self.init_hidden(batch)\n","        self.rnn.flatten_parameters()\n","        output, hidden = self.rnn(x, hidden)\n","\n","        if self.ndirections == 1:\n","            return output[:, -1]\n","\n","        forward_ = output[:, -1, :self.num_hid]\n","        backward = output[:, 0, self.num_hid:]\n","        return torch.cat((forward_, backward), dim=1)\n","\n","    def forward_all(self, x):\n","        # x: [batch, sequence, in_dim]\n","        batch = x.size(0)\n","        hidden = self.init_hidden(batch)\n","        self.rnn.flatten_parameters()\n","        output, hidden = self.rnn(x, hidden)\n","        return output\n","\n","\n","class QuestionEmbedding(nn.Module):\n","    def __init__(self, in_dim, num_hid, nlayers=1, bidirect=True, dropout=0, rnn_type='GRU', words_dropout=None,\n","                 dropout_before_rnn=None,\n","                 dropout_after_rnn=None):\n","        \"\"\"Module for question embedding\n","        \"\"\"\n","        super(QuestionEmbedding, self).__init__()\n","        assert rnn_type == 'LSTM' or rnn_type == 'GRU'\n","        rnn_cls = nn.LSTM if rnn_type == 'LSTM' else nn.GRU\n","        self.bidirect = bidirect\n","        self.ndirections = 1 + int(bidirect)\n","        if bidirect:\n","            num_hid = int(num_hid / 2)\n","        self.words_dropout = words_dropout\n","        if dropout_before_rnn is not None:\n","            self.dropout_before_rnn = nn.Dropout(p=dropout_before_rnn)\n","        else:\n","            self.dropout_before_rnn = None\n","        self.rnn = rnn_cls(\n","            in_dim, num_hid, nlayers,\n","            bidirectional=bidirect,\n","            dropout=dropout,\n","            batch_first=True)\n","        if dropout_after_rnn is not None:\n","            self.dropout_after_rnn = nn.Dropout(p=dropout_after_rnn)\n","        else:\n","            self.dropout_after_rnn = None\n","\n","        self.in_dim = in_dim\n","        self.num_hid = num_hid\n","        self.nlayers = nlayers\n","        self.rnn_type = rnn_type\n","\n","    def init_hidden(self, batch):\n","        # just to get the type of tensor\n","        weight = next(self.parameters()).data\n","        hid_shape = (self.nlayers * self.ndirections, batch, self.num_hid)\n","        if self.rnn_type == 'LSTM':\n","            return (Variable(weight.new(*hid_shape).zero_()),\n","                    Variable(weight.new(*hid_shape).zero_()))\n","        else:\n","            return Variable(weight.new(*hid_shape).zero_())\n","\n","    def forward(self, x, qlen=None):\n","        # x: [batch, sequence, in_dim]\n","        batch = x.size(0)\n","        num_tokens = x.size(1)\n","        if self.words_dropout is not None and self.words_dropout > 0:\n","            num_dropout = int(self.words_dropout * num_tokens)\n","            rand_ixs = np.random.randint(0, num_tokens, (batch, num_dropout))\n","            for bix, token_ixs in enumerate(rand_ixs):\n","                x[bix, token_ixs] *= 0\n","        hidden = self.init_hidden(batch)\n","        self.rnn.flatten_parameters()\n","        if self.dropout_before_rnn is not None:\n","            x = self.dropout_before_rnn(x)\n","\n","        q_words_emb, hidden = self.rnn(x, hidden)  # q_words_emb: B x num_words x gru_dim, hidden: 1 x B x gru_dim\n","\n","        out = None\n","        if self.bidirect:\n","            forward_ = q_words_emb[:, -1, :self.num_hid]\n","            backward = q_words_emb[:, 0, self.num_hid:]\n","            hid = torch.cat((forward_, backward), dim=1)\n","            out = hid\n","            # return q_words_emb, hid\n","        else:\n","            out = q_words_emb[:, -1]\n","            # return q_words_emb, q_words_emb[:, -1]\n","\n","        if self.dropout_after_rnn is not None:\n","            out = self.dropout_after_rnn(out)\n","        return out\n","\n","class Seq2SeqRNN(nn.Module):\n","  def __init__(self, input_features, rnn_features, num_layers=1, drop=0.0,\n","               rnn_type='LSTM', rnn_bidirectional=False):\n","    super(Seq2SeqRNN, self).__init__()\n","    self.bidirectional = rnn_bidirectional\n","\n","    if rnn_type == 'LSTM':\n","      self.rnn = nn.LSTM(input_size=input_features,\n","                hidden_size=rnn_features, dropout=drop,\n","                num_layers=num_layers, batch_first=True,\n","                bidirectional=rnn_bidirectional)\n","    elif rnn_type == 'GRU':\n","      self.rnn = nn.GRU(input_size=input_features,\n","                hidden_size=rnn_features, dropout=drop,\n","                num_layers=num_layers, batch_first=True,\n","                bidirectional=rnn_bidirectional)\n","    else:\n","      raise ValueError('Unsupported Type')\n","\n","    self.init_weight(rnn_bidirectional, rnn_type)\n","\n","  def init_weight(self, bidirectional, rnn_type):\n","    self._init_rnn(self.rnn.weight_ih_l0, rnn_type)\n","    self._init_rnn(self.rnn.weight_hh_l0, rnn_type)\n","    self.rnn.bias_ih_l0.data.zero_()\n","    self.rnn.bias_hh_l0.data.zero_()\n","\n","    if bidirectional:\n","      self._init_rnn(self.rnn.weight_ih_l0_reverse, rnn_type)\n","      self._init_rnn(self.rnn.weight_hh_l0_reverse, rnn_type)\n","      self.rnn.bias_ih_l0_reverse.data.zero_()\n","      self.rnn.bias_hh_l0_reverse.data.zero_()\n","\n","  def _init_rnn(self, weight, rnn_type):\n","    chunk_size = 4 if rnn_type == 'LSTM' else 3\n","    for w in weight.chunk(chunk_size, 0):\n","      init.xavier_uniform(w)\n","\n","  def forward(self, q_emb, q_len):\n","    lengths = torch.LongTensor(q_len)\n","    lens, indices = torch.sort(lengths, 0, True)\n","\n","    packed = pack_padded_sequence(q_emb[indices.cuda()], lens.tolist(), batch_first=True)\n","    if isinstance(self.rnn, nn.LSTM):\n","        # pdb.set_trace()\n","        _, ( outputs, _ ) = self.rnn(packed)\n","    elif isinstance(self.rnn, nn.GRU):\n","        _, outputs = self.rnn(packed)\n","\n","    if self.bidirectional:\n","      outputs = torch.cat([ outputs[0, :, :], outputs[1, :, :] ], dim=1)\n","    else:\n","      outputs = outputs.squeeze(0)\n","\n","    _, _indices = torch.sort(indices, 0)\n","    outputs = outputs[_indices.cuda()]\n","\n","    return outputs"],"metadata":{"id":"44ugMuVeeQN_","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1769155924794,"user_tz":-420,"elapsed":473,"user":{"displayName":"Toàn Bách Lương","userId":"04806593440338345841"}},"outputId":"16676b0d-d033-4321-8e1d-380c8411a9d1"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Overwriting /content/drive/MyDrive/VQA/code/model/language_model.py\n"]}]},{"cell_type":"markdown","source":["#### Fusion Network\n"],"metadata":{"id":"90sOllcu8X-Z"}},{"cell_type":"markdown","source":["###### mlp.py"],"metadata":{"id":"pSFyIrdRCFts"}},{"cell_type":"code","source":["%%writefile /content/drive/MyDrive/VQA/code/model/fusion_net/mlp.py\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torch.nn.init as init\n","from torch.autograd import Variable\n","\n","# from .fc import GroupMLP\n","# from .language_model import WordEmbedding\n","from model import GroupMLP\n","from model import WordEmbedding\n","from utils import freeze_layer\n","\n","\n","class MLP(nn.Module):\n","    #args, self.train_loader.dataset, self.question_word2vec\n","    # def __init__(self, args, dataset, question_word2vec):\n","    def __init__(self, args, dataset, embedding_weights=None, rnn_bidirectional=True):\n","        super(MLP, self).__init__()\n","        embedding_requires_grad = not args.freeze_w2v  # freeze 则不需要grad\n","        question_features = 300\n","        vision_features = args.output_features  # 图片的\n","\n","        # self.text = BagOfWordsMLPProcessor(\n","        self.text = BagOfWordsProcessor(\n","            embedding_tokens=embedding_weights.size(0) if embedding_weights is not None else dataset.num_tokens,\n","            embedding_weights=embedding_weights,\n","            embedding_features=300,\n","            embedding_requires_grad=embedding_requires_grad,\n","        )\n","        self.mlp = GroupMLP(\n","            in_features=vision_features + question_features,\n","            mid_features= 4 * args.hidden_size,\n","            out_features=args.embedding_size,\n","            drop=0.5,\n","            groups=64,\n","        )\n","\n","        for m in self.modules():\n","            if isinstance(m, nn.Linear) or isinstance(m, nn.Conv2d):\n","                init.xavier_uniform(m.weight)\n","                if m.bias is not None:\n","                    m.bias.data.zero_()\n","\n","    def forward(self, v, b, q, q_len):\n","        q = F.normalize(self.text(q, list(q_len.data)), p=2, dim=1)  # 问题向量求平均值\n","        v = F.normalize(F.avg_pool2d(v, (v.size(2), v.size(3))).squeeze(), p=2, dim=1)\n","\n","        combined = torch.cat([v, q], dim=1)\n","        embedding = self.mlp(combined)\n","        return embedding\n","\n","\n","class BagOfWordsProcessor(nn.Module):\n","    def __init__(self, embedding_tokens, embedding_features,\n","                 embedding_weights, embedding_requires_grad):\n","        super(BagOfWordsProcessor, self).__init__()\n","        self.embedding = nn.Embedding(embedding_tokens, embedding_features, padding_idx=0)\n","        self.embedding.weight.data = embedding_weights\n","        self.embedding.weight.requires_grad = embedding_requires_grad\n","\n","    def forward(self, q, q_len):\n","        embedded = self.embedding(q)\n","        q_len = Variable(torch.Tensor(q_len).view(-1, 1) + 1e-12, requires_grad=False).cuda()\n","\n","        return torch.div(torch.sum(embedded, 1), q_len)"],"metadata":{"id":"-mkrfzpwCKuO","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1769155927590,"user_tz":-420,"elapsed":17,"user":{"displayName":"Toàn Bách Lương","userId":"04806593440338345841"}},"outputId":"8342db41-9176-4ef8-aa71-578594a2e7d7"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Writing /content/drive/MyDrive/VQA/code/model/fusion_net/mlp.py\n"]}]},{"cell_type":"markdown","source":["###### ban.py"],"metadata":{"id":"mhHcAwnFB3hV"}},{"cell_type":"code","source":["%%writefile /content/drive/MyDrive/VQA/code/model/fusion_net/ban.py\n","\"\"\"\n","Bilinear Attention Networks\n","Jin-Hwa Kim, Jaehyun Jun, Byoung-Tak Zhang\n","https://arxiv.org/abs/1805.07932\n","\n","This code is adapted from: https://github.com/jnhwkim/ban-vqa (written by Jin-Hwa Kim)\n","\"\"\"\n","import torch.nn as nn\n","\n","# from .attention import BiAttention\n","# from .classifier import SimpleClassifier\n","# from .counting import Counter\n","# from .fc import FCNet, BCNet\n","# from .language_model import WordEmbedding, UpDnQuestionEmbedding\n","# from utils import freeze_layer\n","\n","from model import BiAttention\n","from model import SimpleClassifier\n","from model import Counter\n","from model import FCNet, BCNet\n","from model import WordEmbedding, UpDnQuestionEmbedding\n","from utils import freeze_layer\n","\n","\n","class BAN(nn.Module):\n","    #args, self.train_loader.dataset, self.question_word2vec\n","    # def __init__(self, args, dataset, question_word2vec):\n","    def __init__(self, args, dataset, question_word2vec):\n","        super(BAN, self).__init__()\n","        self.args = args\n","        self.w_emb = WordEmbedding(question_word2vec.size(0), 300, .0)\n","        if args.freeze_w2v:\n","            self.w_emb.init_embedding(question_word2vec)\n","            freeze_layer(self.w_emb)\n","        self.q_emb = UpDnQuestionEmbedding(300, args.embedding_size, 1, False, .0)\n","        self.v_att = BiAttention(args.v_dim, self.q_emb.num_hid, self.q_emb.num_hid, args.glimpse)\n","        self.b_net = []\n","        self.q_prj = []\n","        self.c_prj = []\n","        self.objects = 10  # minimum number of boxes\n","        for i in range(args.glimpse):\n","            self.b_net.append(BCNet(args.v_dim, self.q_emb.num_hid, self.q_emb.num_hid, None, k=1))\n","            self.q_prj.append(FCNet([self.q_emb.num_hid, self.q_emb.num_hid], '', .2))\n","            self.c_prj.append(FCNet([self.objects + 1, self.q_emb.num_hid], 'ReLU', .0))\n","\n","        self.b_net = nn.ModuleList(self.b_net)\n","        self.q_prj = nn.ModuleList(self.q_prj)\n","        self.c_prj = nn.ModuleList(self.c_prj)\n","        self.counter = Counter(self.objects)\n","        self.drop = nn.Dropout(.5)\n","        self.tanh = nn.Tanh()\n","\n","    def forward(self, v, b, q, q_len):\n","        \"\"\"Forward\n","\n","        v: [batch, num_objs, obj_dim]\n","        b: [batch, num_objs, b_dim]\n","        q: [batch_size, seq_length]\n","\n","        return: logits, not probs\n","        \"\"\"\n","        w_emb = self.w_emb(q)\n","        q_emb = self.q_emb.forward_all(w_emb)  # [batch, q_len, q_dim]\n","        boxes = b[:, :, :4].transpose(1, 2)\n","\n","        b_emb = [0] * self.args.glimpse\n","        att, logits = self.v_att.forward_all(v, q_emb)  # b x g x v x q\n","\n","        for g in range(self.args.glimpse):\n","            b_emb[g] = self.b_net[g].forward_with_weights(v, q_emb, att[:, g, :, :])  # b x l x h\n","\n","            atten, _ = logits[:, g, :, :].max(2)\n","            embed = self.counter(boxes, atten)\n","\n","            q_emb = self.q_prj[g](b_emb[g].unsqueeze(1)) + q_emb\n","            q_emb = q_emb + self.c_prj[g](embed).unsqueeze(1)\n","\n","        return q_emb.sum(1)"],"metadata":{"id":"5TkZ-uc28Xp-","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1769155928186,"user_tz":-420,"elapsed":31,"user":{"displayName":"Toàn Bách Lương","userId":"04806593440338345841"}},"outputId":"cebba3d3-b7c5-4590-b4d8-07073fc50842"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Writing /content/drive/MyDrive/VQA/code/model/fusion_net/ban.py\n"]}]},{"cell_type":"markdown","source":["###### san.py"],"metadata":{"id":"bs1YQHcXB0JV"}},{"cell_type":"code","source":["%%writefile %%writefile /content/drive/MyDrive/VQA/code/model/fusion_net/san.py\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torch.nn.init as init\n","\n","from torch.autograd import Variable\n","\n","# from .attention import SanAttention, apply_attention\n","# from .fc import GroupMLP\n","# from .language_model import Seq2SeqRNN, WordEmbedding\n","from model import SanAttention, apply_attention\n","from model import GroupMLP\n","from model import Seq2SeqRNN, WordEmbedding\n","\n","import pdb\n","from utils import freeze_layer\n","\n","class SAN(nn.Module):\n","    #args, self.train_loader.dataset, self.question_word2vec\n","    #def __init__(self, args, dataset, question_word2vec):\n","    def __init__(self, args, dataset,embedding_weights=None,rnn_bidirectional=True):\n","        super(SAN, self).__init__()\n","        embedding_requires_grad = not args.freeze_w2v\n","        question_features = 1024\n","        rnn_features = int(question_features // 2) if rnn_bidirectional else int(question_features)\n","        vision_features = args.output_features\n","        glimpses = 2\n","\n","        # vocab_size = embedding_weights.size(0)\n","        # vector_dim = embedding_weights.size(1)\n","        # self.embedding = nn.Embedding(vocab_size, vector_dim, padding_idx=0)\n","        # self.embedding.weight.data = embedding_weights\n","        # self.embedding.weight.requires_grad = embedding_requires_grad\n","        self.w_emb = WordEmbedding(embedding_weights.size(0), 300, .0)\n","        if args.freeze_w2v:\n","            self.w_emb.init_embedding(embedding_weights)\n","            freeze_layer(self.w_emb)\n","\n","        self.drop = nn.Dropout(0.5)\n","        self.text = Seq2SeqRNN(\n","            input_features=embedding_weights.size(1),\n","            rnn_features=int(rnn_features),\n","            rnn_type='LSTM',\n","            rnn_bidirectional=rnn_bidirectional,\n","        )\n","        self.attention = SanAttention(\n","            v_features=vision_features,\n","            q_features=question_features,\n","            mid_features=512,\n","            glimpses=2,\n","            drop=0.5,\n","        )\n","        self.mlp = GroupMLP(\n","            in_features=glimpses * vision_features + question_features,\n","            mid_features= 4 * args.hidden_size,\n","            out_features=args.embedding_size,\n","            drop=0.5,\n","            groups=64,\n","        )\n","\n","        for m in self.modules():\n","            if isinstance(m, nn.Linear) or isinstance(m, nn.Conv2d):\n","                init.xavier_uniform(m.weight)\n","                if m.bias is not None:\n","                    m.bias.data.zero_()\n","\n","\n","\n","    def forward(self, v, b, q, q_len):\n","        # pdb.set_trace()\n","        q = self.text(self.drop(self.w_emb(q)), list(q_len.data))\n","        # q = self.text(self.embedding(q), list(q_len.data))\n","\n","        v = F.normalize(v, p=2, dim=1)\n","        a = self.attention(v, q)\n","        v = apply_attention(v, a)\n","\n","        combined = torch.cat([v, q], dim=1)\n","        embedding = self.mlp(combined)\n","        return embedding"],"metadata":{"id":"65hdTizH8hFy","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"error","timestamp":1769155928881,"user_tz":-420,"elapsed":6,"user":{"displayName":"Toàn Bách Lương","userId":"04806593440338345841"}},"outputId":"6a928b68-94f6-4992-e8e8-f39fc14c73e4"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["UsageError: unrecognized arguments: /content/drive/MyDrive/VQA/code/model/fusion_net/san.py\n"]}]},{"cell_type":"markdown","source":["###### updn.py"],"metadata":{"id":"7gYdxg8zDDTq"}},{"cell_type":"code","source":["%%writefile /content/drive/MyDrive/VQA/code/model/fusion_net/updn.py\n","\n","import torch\n","import torch.nn as nn\n","\n","# from .language_model import WordEmbedding, UpDnQuestionEmbedding\n","# from .attention import UpDnAttention\n","# from .classifier import SimpleClassifier\n","# from .fc import FCNet\n","from model import WordEmbedding, UpDnQuestionEmbedding\n","from model import UpDnAttention\n","from model import SimpleClassifier\n","from model import FCNet\n","from utils import freeze_layer\n","\n","class UD(nn.Module):\n","    def __init__(self, args, dataset, question_word2vec):\n","        super(UD, self).__init__()\n","        self.w_emb = WordEmbedding(question_word2vec.size(0), 300, 0.0)\n","        if args.freeze_w2v:\n","            self.w_emb.init_embedding(question_word2vec)\n","            freeze_layer(self.w_emb)\n","            # self.w_emb.weight.requires_grad = False\n","\n","        self.q_emb = UpDnQuestionEmbedding(300, args.embedding_size, 1, False, 0.0)\n","        self.v_att = UpDnAttention(args.v_dim, self.q_emb.num_hid, args.embedding_size)\n","        self.q_net = FCNet([self.q_emb.num_hid, args.embedding_size])\n","        self.v_net = FCNet([args.v_dim, args.embedding_size])\n","        # self.classifier = SimpleClassifier(\n","        #     args.embedding_size, args.embedding_size * 2, args.num_ans_candidates, 0.5)\n","\n","    def forward(self, v, b, q, qlen):\n","        \"\"\"Forward\n","\n","        v: [batch, num_objs, obj_dim]\n","        b: [batch, num_objs, b_dim]\n","        q: [batch_size, seq_length]\n","\n","        return: logits, not probs\n","        \"\"\"\n","        # print(\"q = {}\".format(q))\n","        w_emb = self.w_emb(q)\n","        # print(\"w_emb = {}\".format(w_emb))\n","        q_emb = self.q_emb(w_emb)  # [batch, q_dim]\n","\n","        att = self.v_att(v, q_emb) # [spa, 1]\n","        v_emb = (att * v).sum(1)  # [batch, v_dim]\n","\n","        q_repr = self.q_net(q_emb)\n","        v_repr = self.v_net(v_emb)\n","        joint_repr = q_repr * v_repr\n","       # logits = self.classifier(joint_repr)\n","        return joint_repr"],"metadata":{"id":"pjal-g27DCca","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1769155929693,"user_tz":-420,"elapsed":28,"user":{"displayName":"Toàn Bách Lương","userId":"04806593440338345841"}},"outputId":"d2109490-01c1-4a91-a752-7a428bca221b"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Writing /content/drive/MyDrive/VQA/code/model/fusion_net/updn.py\n"]}]},{"cell_type":"markdown","source":["#### Answer Network"],"metadata":{"id":"S-Qxepl-8kPv"}},{"cell_type":"markdown","source":["###### fc.py"],"metadata":{"id":"XmZjuTqqKqoa"}},{"cell_type":"code","source":["%%writefile /content/drive/MyDrive/VQA/code/model/fc.py\n","from __future__ import print_function\n","import torch.nn as nn\n","from torch.nn.utils.weight_norm import weight_norm\n","import torch\n","\n","\n","class FCNet(nn.Module):\n","    \"\"\"Simple class for non-linear fully connect network\n","    \"\"\"\n","\n","    def __init__(self, dims, act='ReLU', dropout=0):\n","        super(FCNet, self).__init__()\n","\n","        layers = []\n","        for i in range(len(dims) - 2):\n","            in_dim = dims[i]\n","            out_dim = dims[i + 1]\n","            if 0 < dropout:\n","                layers.append(nn.Dropout(dropout))\n","            layers.append(weight_norm(nn.Linear(in_dim, out_dim), dim=None))\n","            if '' != act:\n","                layers.append(getattr(nn, act)())\n","        if 0 < dropout:\n","            layers.append(nn.Dropout(dropout))\n","        layers.append(weight_norm(nn.Linear(dims[-2], dims[-1]), dim=None))\n","        if '' != act:\n","            layers.append(getattr(nn, act)())\n","\n","        self.main = nn.Sequential(*layers)\n","\n","    def forward(self, x):\n","        return self.main(x)\n","\n","\n","class BCNet(nn.Module):\n","    \"\"\"Simple class for non-linear bilinear connect network\n","    \"\"\"\n","\n","    def __init__(self, v_dim, q_dim, h_dim, h_out, act='ReLU', dropout=[.2, .5], k=3):\n","        super(BCNet, self).__init__()\n","\n","        self.c = 32\n","        self.k = k\n","        self.v_dim = v_dim\n","        self.q_dim = q_dim\n","        self.h_dim = h_dim\n","        self.h_out = h_out\n","\n","        self.v_net = FCNet([v_dim, h_dim * self.k], act=act, dropout=dropout[0])\n","        self.q_net = FCNet([q_dim, h_dim * self.k], act=act, dropout=dropout[0])\n","        self.dropout = nn.Dropout(dropout[1])  # attention\n","        if 1 < k:\n","            self.p_net = nn.AvgPool1d(self.k, stride=self.k)\n","\n","        if None == h_out:\n","            pass\n","        elif h_out <= self.c:\n","            self.h_mat = nn.Parameter(torch.Tensor(1, h_out, 1, h_dim * self.k).normal_())\n","            self.h_bias = nn.Parameter(torch.Tensor(1, h_out, 1, 1).normal_())\n","        else:\n","            self.h_net = weight_norm(nn.Linear(h_dim, h_out), dim=None)\n","\n","    def forward(self, v, q):\n","        if None == self.h_out:\n","            v_ = self.v_net(v).transpose(1, 2).unsqueeze(3)\n","            q_ = self.q_net(q).transpose(1, 2).unsqueeze(2)\n","            d_ = torch.matmul(v_, q_)  # b x h_dim x v x q\n","            logits = d_.transpose(1, 2).transpose(2, 3)  # b x v x q x h_dim\n","            return logits\n","\n","        # broadcast Hadamard product, matrix-matrix production\n","        # fast computation but memory inefficient\n","        # epoch 1, time: 157.84\n","        elif self.h_out <= self.c:\n","            v_ = self.dropout(self.v_net(v)).unsqueeze(1)\n","            q_ = self.q_net(q)\n","            h_ = v_ * self.h_mat  # broadcast, b x h_out x v x h_dim\n","            logits = torch.matmul(h_, q_.unsqueeze(1).transpose(2, 3))  # b x h_out x v x q\n","            logits = logits + self.h_bias\n","            return logits  # b x h_out x v x q\n","\n","        # batch outer product, linear projection\n","        # memory efficient but slow computation\n","        # epoch 1, time: 304.87\n","        else:\n","            v_ = self.dropout(self.v_net(v)).transpose(1, 2).unsqueeze(3)\n","            q_ = self.q_net(q).transpose(1, 2).unsqueeze(2)\n","            d_ = torch.matmul(v_, q_)  # b x h_dim x v x q\n","            logits = self.h_net(d_.transpose(1, 2).transpose(2, 3))  # b x v x q x h_out\n","            return logits.transpose(2, 3).transpose(1, 2)  # b x h_out x v x q\n","\n","    def forward_with_weights(self, v, q, w):\n","        v_ = self.v_net(v).transpose(1, 2).unsqueeze(2)  # b x d x 1 x v\n","        q_ = self.q_net(q).transpose(1, 2).unsqueeze(3)  # b x d x q x 1\n","        logits = torch.matmul(torch.matmul(v_, w.unsqueeze(1)), q_)  # b x d x 1 x 1\n","        logits = logits.squeeze(3).squeeze(2)\n","        if 1 < self.k:\n","            logits = logits.unsqueeze(1)  # b x 1 x d\n","            logits = self.p_net(logits).squeeze(1) * self.k  # sum-pooling\n","        return logits\n","\n","\n","class GroupMLP(nn.Module):\n","    def __init__(self, in_features, mid_features, out_features, drop=0.5, groups=1):\n","        super(GroupMLP, self).__init__()\n","\n","        self.conv1 = nn.Conv1d(in_features, mid_features, 1)\n","        self.drop = nn.Dropout(p=drop)\n","        self.relu = nn.LeakyReLU()\n","        self.conv2 = nn.Conv1d(mid_features, out_features, 1, groups=groups)\n","\n","    def forward(self, a):\n","        N, C = a.size()\n","        h = self.relu(self.conv1(a.view(N, C, 1)))\n","        return self.conv2(self.drop(h)).view(N, -1)\n","\n","\n","class GroupMLP_1lay(nn.Module):\n","    def __init__(self, in_features, mid_features, out_features, drop=0.5, groups=1):\n","        super(GroupMLP_1lay, self).__init__()\n","\n","        self.conv1 = nn.Conv1d(in_features, mid_features, 1)\n","        self.batch_norm_fusion = nn.BatchNorm1d(mid_features, affine=False)\n","        self.drop = nn.Dropout(p=drop)\n","        self.relu = nn.LeakyReLU()\n","        self.conv2 = nn.Conv1d(mid_features, out_features, 1, groups=groups)\n","\n","    def forward(self, a):\n","        N, C = a.size()\n","        h = self.conv1(a.view(N, C, 1))\n","        h = self.batch_norm_fusion(h)\n","        h = self.relu(h)\n","        return self.conv2(self.drop(h)).view(N, -1)\n","\n","\n","class GroupMLP_2lay(nn.Module):\n","    def __init__(self, in_features, mid_features, out_features, drop=0.5, groups=1):\n","        super(GroupMLP_2lay, self).__init__()\n","\n","        self.conv1 = nn.Conv1d(in_features, mid_features, 1)\n","        self.batch_norm_fusion = nn.BatchNorm1d(mid_features, affine=False)\n","        self.drop = nn.Dropout(p=drop)\n","        self.relu = nn.LeakyReLU()\n","        self.conv2 = nn.Conv1d(mid_features, mid_features, 1, groups=groups)\n","        self.conv3 = nn.Conv1d(mid_features, out_features, 1, groups=groups)\n","\n","    def forward(self, a):\n","        N, C = a.size()\n","        h = self.conv1(a.view(N, C, 1))\n","        h = self.relu(h)\n","        h = self.conv2(h)\n","        h = self.batch_norm_fusion(h)\n","        h = self.relu(h)\n","        return self.conv3(self.drop(h)).view(N, -1)"],"metadata":{"id":"61KutEkPKu0T","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1769155935213,"user_tz":-420,"elapsed":27,"user":{"displayName":"Toàn Bách Lương","userId":"04806593440338345841"}},"outputId":"af100270-5770-41de-b3f3-bc1ba973e5fc"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Writing /content/drive/MyDrive/VQA/code/model/fc.py\n"]}]},{"cell_type":"markdown","source":["# Github\n"],"metadata":{"id":"oxUz2vaK435k"}},{"cell_type":"code","source":["!more /content/token.txt | gh auth login --with-token"],"metadata":{"id":"EYuX-D_YbJXS","executionInfo":{"status":"ok","timestamp":1769601763081,"user_tz":-420,"elapsed":739,"user":{"displayName":"Toàn Bách Lương","userId":"04806593440338345841"}}},"execution_count":46,"outputs":[]},{"cell_type":"code","source":["%%writefile /content/drive/MyDrive/VQA/.gitignore\n","kg/\n","data/"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"wRZF6EVfZcoN","executionInfo":{"status":"ok","timestamp":1769601590368,"user_tz":-420,"elapsed":35,"user":{"displayName":"Toàn Bách Lương","userId":"04806593440338345841"}},"outputId":"64fa02d6-f75c-4eb5-87a5-31339d7d30e5"},"execution_count":41,"outputs":[{"output_type":"stream","name":"stdout","text":["Overwriting /content/drive/MyDrive/VQA/.gitignore\n"]}]},{"cell_type":"code","source":["!pwd"],"metadata":{"id":"_dBWodvfb8T6","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1769600283649,"user_tz":-420,"elapsed":122,"user":{"displayName":"Toàn Bách Lương","userId":"04806593440338345841"}},"outputId":"f2f1de60-7b5d-4ce8-c224-7f3a39ace0b1"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["/content/drive/MyDrive/VQA\n"]}]},{"cell_type":"code","source":["# !git init"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"0nbB4bgGaglH","executionInfo":{"status":"ok","timestamp":1769600590271,"user_tz":-420,"elapsed":744,"user":{"displayName":"Toàn Bách Lương","userId":"04806593440338345841"}},"outputId":"77e4854e-2c96-4933-f09e-825601dba842"},"execution_count":9,"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[33mhint: Using 'master' as the name for the initial branch. This default branch name\u001b[m\n","\u001b[33mhint: is subject to change. To configure the initial branch name to use in all\u001b[m\n","\u001b[33mhint: of your new repositories, which will suppress this warning, call:\u001b[m\n","\u001b[33mhint: \u001b[m\n","\u001b[33mhint: \tgit config --global init.defaultBranch <name>\u001b[m\n","\u001b[33mhint: \u001b[m\n","\u001b[33mhint: Names commonly chosen instead of 'master' are 'main', 'trunk' and\u001b[m\n","\u001b[33mhint: 'development'. The just-created branch can be renamed via this command:\u001b[m\n","\u001b[33mhint: \u001b[m\n","\u001b[33mhint: \tgit branch -m <name>\u001b[m\n","Initialized empty Git repository in /content/drive/MyDrive/VQA/.git/\n"]}]},{"cell_type":"code","source":["# !git remote add origin https://github.com/LTBach/VQA.git"],"metadata":{"id":"1doq7mxAaK0r","executionInfo":{"status":"ok","timestamp":1769600595363,"user_tz":-420,"elapsed":110,"user":{"displayName":"Toàn Bách Lương","userId":"04806593440338345841"}}},"execution_count":10,"outputs":[]},{"cell_type":"code","source":["# !git branch -M main"],"metadata":{"id":"2w50JxDKajz3","executionInfo":{"status":"ok","timestamp":1769600603180,"user_tz":-420,"elapsed":153,"user":{"displayName":"Toàn Bách Lương","userId":"04806593440338345841"}}},"execution_count":11,"outputs":[]},{"cell_type":"code","source":["!git add ."],"metadata":{"id":"dNviGX0BavF_","executionInfo":{"status":"ok","timestamp":1769600652449,"user_tz":-420,"elapsed":3835,"user":{"displayName":"Toàn Bách Lương","userId":"04806593440338345841"}}},"execution_count":13,"outputs":[]},{"cell_type":"code","source":["!git commit -m \"Add code\""],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"fvxYd1j6aw2b","executionInfo":{"status":"ok","timestamp":1769600670164,"user_tz":-420,"elapsed":213,"user":{"displayName":"Toàn Bách Lương","userId":"04806593440338345841"}},"outputId":"fb10555b-9c05-4560-cac8-a4af8978867b"},"execution_count":14,"outputs":[{"output_type":"stream","name":"stdout","text":["Author identity unknown\n","\n","*** Please tell me who you are.\n","\n","Run\n","\n","  git config --global user.email \"you@example.com\"\n","  git config --global user.name \"Your Name\"\n","\n","to set your account's default identity.\n","Omit --global to set the identity only in this repository.\n","\n","fatal: unable to auto-detect email address (got 'root@07cd57bfa03d.(none)')\n"]}]},{"cell_type":"code","source":["!"],"metadata":{"id":"M6cnwL5ua2hU"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!git push -u origin main"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"vfPUQFMYamYf","executionInfo":{"status":"ok","timestamp":1769600613185,"user_tz":-420,"elapsed":214,"user":{"displayName":"Toàn Bách Lương","userId":"04806593440338345841"}},"outputId":"b055128d-610a-4bd6-9be5-867434fe3f63"},"execution_count":12,"outputs":[{"output_type":"stream","name":"stdout","text":["error: src refspec main does not match any\n","\u001b[31merror: failed to push some refs to 'https://github.com/LTBach/VQA.git'\n","\u001b[m"]}]},{"cell_type":"code","source":["git remote add origin https://github.com/LTBach/VQA.git\n","git branch -M main\n","git push -u origin main"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Lg1FAzAJZXI6","executionInfo":{"status":"ok","timestamp":1769600423207,"user_tz":-420,"elapsed":113,"user":{"displayName":"Toàn Bách Lương","userId":"04806593440338345841"}},"outputId":"b18a2efc-c987-4c4d-d00b-bce8dfc26087"},"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["usage: git [--version] [--help] [-C <path>] [-c <name>=<value>]\n","           [--exec-path[=<path>]] [--html-path] [--man-path] [--info-path]\n","           [-p | --paginate | -P | --no-pager] [--no-replace-objects] [--bare]\n","           [--git-dir=<path>] [--work-tree=<path>] [--namespace=<name>]\n","           [--super-prefix=<path>] [--config-env=<name>=<envvar>]\n","           <command> [<args>]\n","\n","These are common Git commands used in various situations:\n","\n","start a working area (see also: git help tutorial)\n","   clone     Clone a repository into a new directory\n","   init      Create an empty Git repository or reinitialize an existing one\n","\n","work on the current change (see also: git help everyday)\n","   add       Add file contents to the index\n","   mv        Move or rename a file, a directory, or a symlink\n","   restore   Restore working tree files\n","   rm        Remove files from the working tree and from the index\n","\n","examine the history and state (see also: git help revisions)\n","   bisect    Use binary search to find the commit that introduced a bug\n","   diff      Show changes between commits, commit and working tree, etc\n","   grep      Print lines matching a pattern\n","   log       Show commit logs\n","   show      Show various types of objects\n","   status    Show the working tree status\n","\n","grow, mark and tweak your common history\n","   branch    List, create, or delete branches\n","   commit    Record changes to the repository\n","   merge     Join two or more development histories together\n","   rebase    Reapply commits on top of another base tip\n","   reset     Reset current HEAD to the specified state\n","   switch    Switch branches\n","   tag       Create, list, delete or verify a tag object signed with GPG\n","\n","collaborate (see also: git help workflows)\n","   fetch     Download objects and refs from another repository\n","   pull      Fetch from and integrate with another repository or a local branch\n","   push      Update remote refs along with associated objects\n","\n","'git help -a' and 'git help -g' list available subcommands and some\n","concept guides. See 'git help <command>' or 'git help <concept>'\n","to read about a specific subcommand or concept.\n","See 'git help git' for an overview of the system.\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"lYqrrzeNZ5N2"},"execution_count":null,"outputs":[]}]}